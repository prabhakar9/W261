{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    " Name         : Prabhakar Gundugola\n",
    " Email address: prabhakar@berkeley.edu\n",
    " W261-3       : Spring 2016\n",
    " Week 1       : Homework 1\n",
    " Date         : January 19, 2016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW1.0.0\n",
    "#### Define big data. Provide an example of a big data problem in your domain of expertise. \n",
    "\n",
    "Big data is a broad term used to mainly describe the data that have the following characteristics:\n",
    "- 1) Volume - massive volume of generated and stored data that has the potential to be mined for information.\n",
    "- 2) Variety - type and nature of data, which could be structured, semi-structured, and unstructured.\n",
    "- 3) Velocity - speed at which the data is generated and processed.\n",
    "- 4) Veracity - quality of data \n",
    "- 5) Complexity - so large and complex that traditional database and software techniques are not adequate.\n",
    "\n",
    "I work in a Data center company that owns over 145 data centers all over the world. Each and every data center has hundreds of instruments that are IoT enabled. These instruments emit events every minute that we capture and store in a big data lake for data mining and analytics. In addition to that, we also store log files generated by Infrastructure systems. As you see, this problem has all the above big data characteristics and we have to use big data technologies to acquire, store, and process the data as traditional data procecssing techniques cannot scale to meet the needs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW1.0.1\n",
    "#### In 500 words (English or pseudo code or a combination) describe how to estimate the bias, the variance, the irreduciable error for a test dataset T when using polynomial regression models of degree 1, 2,3, 4,5 are considered. How would you select a model?\n",
    "\n",
    "Answer:\n",
    "#####Definitions\n",
    "- Bias and Variance are 2 different sources of reducible errors that affect model accuracy. \n",
    "- Bias is the difference between the expected predicted value and the actual value for any given observation. \n",
    "- Variance is the variability of model prediction for any given observation.\n",
    "- Irreducible error is the noise that cannot fundamentally be reduced by any model. \n",
    "\n",
    "\n",
    "#####Relationship between bias, variance, and model complexity\n",
    "- Dealing with bias and variance is all about dealing with over-fitting and under-fitting. \n",
    "- When the degree of polynomial (model complexity) is increased, it results in over-fitting. This leads to decrease in bias and increase in variance.\n",
    "- Conversely, under-fitting results in increase in bias and decrease in variance\n",
    "\n",
    "#####Estimation\n",
    "Steps to estimate bias, variance, and irreducible error for a test dataset T:\n",
    "- If the size of test dataset T is not large, then apply bootstrapping (process of resampling the dataset with replacement) from T and generate multiple data sets, say 100 datasets.\n",
    "- For each of these 100 datasets, split the dataset into training, validation, and test datasets in the proportion of 50%, 25%, 25%.\n",
    "- Fit each of the 100 training sets with polynomials of degree 1, 2, 3, 4, 5. It will result in 100 models for each degree.\n",
    "- Determine the bias for each observation $x$, which is the difference between the expected predicted value and the actual value. $$Bias = E[y] - f(x)$$\n",
    "- Determinie the variance, which is the squared sum of differences between the predicted value and the expected predicted value . $$Variance = E[(y - E[y])^2]$$\n",
    "- Determine the noise, which is the squared sum of differences between the predicted value and the actual value. $$Noise = E[(y-f(x)^2] = \\sigma^2$$\n",
    "\n",
    "#####Model selection\n",
    "The optimum model is the level of degree at which the increase in bias is equivalent to the reduction in variance. In practice, there is no way to find this equivalence. \n",
    "\n",
    "For model selection, we need to determine the accurate measure of expected prediction error for different degrees and then choose the degree that minimizes the overall error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "###Run control script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting pNaiveBayes.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile pNaiveBayes.sh\n",
    "## pNaiveBayes.sh\n",
    "## Author: Jake Ryland Williams\n",
    "## Usage: pNaiveBayes.sh m wordlist\n",
    "## Input:\n",
    "##       m = number of processes (maps), e.g., 4\n",
    "##       wordlist = a space-separated list of words in quotes, e.g., \"the and of\"\n",
    "##\n",
    "## Instructions: Read this script and its comments closely.\n",
    "##               Do your best to understand the purpose of each command,\n",
    "##               and focus on how arguments are supplied to mapper.py/reducer.py,\n",
    "##               as this will determine how the python scripts take input.\n",
    "##               When you are comfortable with the unix code below,\n",
    "##               answer the questions on the LMS for HW1 about the starter code.\n",
    "\n",
    "## collect user input\n",
    "m=$1 ## the number of parallel processes (maps) to run\n",
    "wordlist=$2 ## if set to \"*\", then all words are used\n",
    "mapper=$3 ## mapper program\n",
    "reducer=$4 ## reducer program\n",
    "\n",
    "## a test set data of 100 messages\n",
    "data=\"enronemail_1h.txt\" \n",
    "\n",
    "## the full set of data (33746 messages)\n",
    "# data=\"enronemail.txt\" \n",
    "\n",
    "## 'wc' determines the number of lines in the data\n",
    "## 'perl -pe' regex strips the piped wc output to a number\n",
    "linesindata=`wc -l $data | perl -pe 's/^.*?(\\d+).*?$/$1/'`\n",
    "\n",
    "## determine the lines per chunk for the desired number of processes\n",
    "linesinchunk=`echo \"$linesindata/$m+1\" | bc`\n",
    "\n",
    "## split the original file into chunks by line\n",
    "split -l $linesinchunk $data $data.chunk.\n",
    "\n",
    "## assign python mappers (mapper.py) to the chunks of data\n",
    "## and emit their output to temporary files\n",
    "for datachunk in $data.chunk.*; do\n",
    "    ## feed word list to the python mapper here and redirect STDOUT to a temporary file on disk\n",
    "    ####\n",
    "    ####\n",
    "    ./$mapper.py $datachunk \"$wordlist\" > $datachunk.counts &\n",
    "    ####\n",
    "    ####\n",
    "done\n",
    "## wait for the mappers to finish their work\n",
    "wait\n",
    "\n",
    "## 'ls' makes a list of the temporary count files\n",
    "## 'perl -pe' regex replaces line breaks with spaces\n",
    "countfiles=`\\ls $data.chunk.*.counts | perl -pe 's/\\n/ /'`\n",
    "\n",
    "## feed the list of countfiles to the python reducer and redirect STDOUT to disk\n",
    "####\n",
    "####\n",
    "./$reducer.py $countfiles > $data.output\n",
    "####\n",
    "####\n",
    "\n",
    "## clean up the data chunks and temporary count files\n",
    "\\rm $data.chunk.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x pNaiveBayes.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data validation and cleansing\n",
    "By exploring the data in the input dataset, 2 problems with 3 records are identified:\n",
    "- There are 2 records with only 3 fields instead of 4 fields.\n",
    "- There is 1 record with extra new line character in the body field\n",
    "\n",
    "#####Data cleansing algorithm\n",
    "\n",
    "- Open enronemail_1new.txt with \"w\" permissions\n",
    "- Initialize prev_line = \"\"\n",
    "- For each line as line in enronemail_1h.txt file\n",
    " - tokenize with delimiter \"\\t\"\n",
    " - If number of tokens >= 3 then \n",
    "     - If number of tokens == 3 then\n",
    "        - Add \"\\t\" as another token between 2nd and 3rd tokens. Now total number of tokens = 4.\n",
    "        - Update line by concatenating all the 4 okens\n",
    "     - If prev_line != \"\" then write prev_line in enronemail_1new.txt\n",
    "     - prev_line = line\n",
    " - If number of tokens == 1 then\n",
    "     - prev_line = prev_line + line\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleanup completed\n"
     ]
    }
   ],
   "source": [
    "# Data cleansing algorithm\n",
    "\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Open enronemail_1new.txt with \"w\" permissions.\n",
    "with open(\"enronemail_1new.txt\", \"w\") as new:\n",
    "    with open(\"enronemail_1h.txt\", \"rU\") as old:\n",
    "        # curr_line is the line to be written to new file. Initially it is set to \"\". \n",
    "        prev_line = \"\"\n",
    "        \n",
    "        # For every line in enronemail_1h.txt file\n",
    "        for line in old:\n",
    "            \n",
    "            line = line.strip()\n",
    "            # Split the line into tokens\n",
    "            tokens = line.split('\\t')\n",
    "            \n",
    "            if len(tokens) >= 3:\n",
    "                \n",
    "                # If subject field is missed out, add blank token and reconstruct the line\n",
    "                if len(tokens) == 3:\n",
    "                    line = tokens[0] + '\\t' + tokens[1] + '\\t' + '' + '\\t' + tokens[2]\n",
    "                \n",
    "                # If len(tokens) == 4 then this line is valid. Keep it in buffer. Now copy the previous line (if not blank). \n",
    "                if prev_line != \"\":\n",
    "                    prev_line += '\\n'\n",
    "                    new.write(prev_line)\n",
    "                prev_line = line\n",
    "            \n",
    "            # If there is only one field, it must be because of an extra new line character in the previous line body field.\n",
    "            if len(tokens) == 1:\n",
    "                # Add this line too to the previous line\n",
    "                prev_line += line\n",
    "        \n",
    "        # Add the last line to the new file\n",
    "        new.write(prev_line)\n",
    "\n",
    "# Now rename enronemail_1new.txt to enronemail_1h.txt\n",
    "os.rename('enronemail_1new.txt', 'enronemail_1h.txt')\n",
    "\n",
    "print \"Cleanup completed\"\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW1.1\n",
    "Read through the provided control script (pNaiveBayes.sh) and all of its comments. When you are comfortable with their purpose and function, respond to the remaining homework questions below. A simple cell in the notebook with a print statmement with  a \"done\" string will suffice here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print \"Done\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW1.2\n",
    "Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh will determine the number of occurrences of a single, user-specified word. Examine the word “assistance” and report your results.\n",
    "\n",
    "Examine the word “assistance” and report your results. \n",
    "To do so, make sure that\n",
    "- mapper.py counts all occurrences of a single word, and\n",
    "- reducer.py collates the counts of the single word.\n",
    "\n",
    "### Mapper\n",
    "**Input**\n",
    "- 2 Input arguments \n",
    "  - dataset file name \n",
    "  - list of words separated by space in double quoted string\n",
    "\n",
    "**Output**\n",
    "- Outputs a tab delimited file with 2 fields:\n",
    "  - word\n",
    "  - count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper12.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper12.py\n",
    "#!/usr/bin/python\n",
    "## mapper12.py\n",
    "## Author: Prabhakar Gundugola\n",
    "## Description: mapper code for HW1.2\n",
    "\n",
    "import sys\n",
    "import re\n",
    "import string\n",
    "\n",
    "## collect user input\n",
    "filename = sys.argv[1]\n",
    "findwords = re.split(\" \",sys.argv[2].lower())\n",
    "\n",
    "with open (filename, \"rU\") as myfile:\n",
    "    for line in myfile:\n",
    "        tokens = line.lower().split('\\t')\n",
    "        word_string = tokens[2] + ' ' + tokens[3].strip()\n",
    "        word_string = word_string.translate(string.maketrans(\"\",\"\"), string.punctuation)\n",
    "        \n",
    "        for word in findwords:\n",
    "            if word in word_string:\n",
    "                print word + '\\t' + str(word_string.count(word))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Reducer\n",
    "**Input**\n",
    "- Mapper output files\n",
    "\n",
    "**Output**\n",
    "- Prints the following output fields separated by '\\t'\n",
    "  - word\n",
    "  - count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing reducer12.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer12.py\n",
    "#!/usr/bin/python\n",
    "## reducer12.py\n",
    "## Author: Prabhakar Gundugola\n",
    "## Description: reducer code for HW1.2-1.4\n",
    "\n",
    "import sys\n",
    "\n",
    "filenames = sys.argv[1:]\n",
    "\n",
    "word_count = {}\n",
    "\n",
    "for filename in filenames:\n",
    "    with open(filename, \"r\") as myfile:\n",
    "        for line in myfile:\n",
    "            word, value = line.split('\\t', 1)\n",
    "            if word not in word_count:\n",
    "                word_count[word] = int(value)\n",
    "            else:\n",
    "                word_count[word] += int(value)\n",
    "\n",
    "for word in word_count:\n",
    "    print word + '\\t' + str(word_count[word])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!chmod a+x mapper12.py\n",
    "!chmod a+x reducer12.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistance\t10\r\n"
     ]
    }
   ],
   "source": [
    "!./pNaiveBayes.sh 4 \"assistance\" \"mapper12\" \"reducer12\"\n",
    "!cat \"enronemail_1h.txt.output\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# HW1.3\n",
    "Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh will classify the email messages by a single, user-specified word using the multinomial Naive Bayes Formulation. Examine the word “assistance” and report your results. To do so, make sure that\n",
    "   \n",
    "   - mapper.py and\n",
    "   - reducer.py \n",
    "   \n",
    "that performs a single word Naive Bayes classification. For multinomial Naive Bayes, the Pr(X=“assistance”|Y=SPAM) is calculated as follows:\n",
    "   the number of times “assistance” occurs in SPAM labeled documents / the number of words in documents labeled SPAM\n",
    "   \n",
    "###Mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper13.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper13.py\n",
    "#!/usr/bin/python\n",
    "## mapper13.py\n",
    "## Author: Prabhakar Gundugola\n",
    "## Description: mapper code for HW1.3\n",
    "\n",
    "import sys\n",
    "import re\n",
    "import string\n",
    "\n",
    "## collect user input\n",
    "filename = sys.argv[1]\n",
    "findwords = re.split(\" \",sys.argv[2].lower())\n",
    "\n",
    "with open (filename, \"rU\") as myfile:\n",
    "    for line in myfile:\n",
    "        tokens = line.lower().split('\\t')\n",
    "        word_string = tokens[2] + ' ' + tokens[3].strip()\n",
    "        word_string = word_string.translate(string.maketrans(\"\",\"\"), string.punctuation)\n",
    "        \n",
    "        for word in findwords:\n",
    "            key = tokens[0] + '\\t' + tokens[1] + '\\t' + word + '\\t' + str(len(word_string.split()))\n",
    "            print key + '\\t' + str(word_string.count(word))\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reducer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer13.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer13.py\n",
    "#!/usr/bin/python\n",
    "## reducer13.py\n",
    "## Author: Prabhakar Gundugola\n",
    "## Description: reducer code for HW1.3\n",
    "\n",
    "import sys\n",
    "import math\n",
    "\n",
    "def isspam(true_class):\n",
    "    if true_class == 1:\n",
    "        return 'SPAM'\n",
    "    else:\n",
    "        return 'HAM'\n",
    "\n",
    "filenames = sys.argv[1:]\n",
    "\n",
    "spam_email_count = 0\n",
    "ham_email_count = 0\n",
    "\n",
    "spam_word_count = 0\n",
    "ham_word_count = 0\n",
    "\n",
    "spam_findword_count = 0\n",
    "ham_findword_count = 0\n",
    "\n",
    "total_cases = 0\n",
    "correct_cases = 0\n",
    "\n",
    "for filename in filenames:\n",
    "    with open(filename, \"r\") as myfile:\n",
    "        for line in myfile:\n",
    "            tokens = line.split('\\t')\n",
    "            doc_id = tokens[0]\n",
    "            true_class = int(tokens[1])\n",
    "            findword = tokens[2]\n",
    "            findword_count = int(tokens[4])\n",
    "            word_count = int(tokens[3])\n",
    "            \n",
    "            if true_class == 1:\n",
    "                spam_email_count += 1\n",
    "                spam_word_count += word_count\n",
    "                spam_findword_count += findword_count\n",
    "            else:\n",
    "                ham_email_count += 1\n",
    "                ham_word_count += word_count\n",
    "                ham_findword_count += findword_count\n",
    "\n",
    "spam_prior = math.log((1.0*spam_email_count)/(spam_email_count + ham_email_count))\n",
    "ham_prior = math.log((1.0*ham_email_count)/(ham_email_count + spam_email_count))\n",
    "\n",
    "spam_findword_prob = math.log((1.0*spam_findword_count/spam_word_count))\n",
    "ham_findword_prob = math.log((1.0*ham_findword_count/ham_word_count))\n",
    "\n",
    "# Naive Bayes classification\n",
    "for filename in filenames:\n",
    "    with open(filename, \"r\") as myfile:\n",
    "        for line in myfile:\n",
    "            total_cases += 1\n",
    "            tokens = line.split('\\t')\n",
    "            doc_id = tokens[0]\n",
    "            true_class = int(tokens[1])\n",
    "            findword_count = int(tokens[4])\n",
    "            \n",
    "            spam_doc_prob = spam_prior + spam_findword_prob*findword_count\n",
    "            ham_doc_prob = ham_prior + ham_findword_prob*findword_count\n",
    "            \n",
    "            result = doc_id.ljust(30) + '\\t\\t' + isspam(true_class) + '\\t\\t'\n",
    "            if spam_doc_prob > ham_doc_prob:\n",
    "                predicted = 1\n",
    "            else:\n",
    "                predicted = 0\n",
    "            result += isspam(predicted)\n",
    "            print result\n",
    "            \n",
    "            if true_class == predicted:\n",
    "                correct_cases += 1\n",
    "\n",
    "accuracy = 1.0*correct_cases/total_cases\n",
    "print \"-----------------------\"\n",
    "print \"Accuracy: \" + str(accuracy*100) + '%'\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0001.1999-12-10.farmer        \t\tHAM\t\tHAM\r\n",
      "0001.1999-12-10.kaminski      \t\tHAM\t\tHAM\r\n",
      "0001.2000-01-17.beck          \t\tHAM\t\tHAM\r\n",
      "0001.2000-06-06.lokay         \t\tHAM\t\tHAM\r\n",
      "0001.2001-02-07.kitchen       \t\tHAM\t\tHAM\r\n",
      "0001.2001-04-02.williams      \t\tHAM\t\tHAM\r\n",
      "0002.1999-12-13.farmer        \t\tHAM\t\tHAM\r\n",
      "0002.2001-02-07.kitchen       \t\tHAM\t\tHAM\r\n",
      "0002.2001-05-25.sa_and_hp     \t\tSPAM\t\tHAM\r\n",
      "0002.2003-12-18.gp            \t\tSPAM\t\tHAM\r\n",
      "0002.2004-08-01.bg            \t\tSPAM\t\tSPAM\r\n",
      "0003.1999-12-10.kaminski      \t\tHAM\t\tHAM\r\n",
      "0003.1999-12-14.farmer        \t\tHAM\t\tHAM\r\n",
      "0003.2000-01-17.beck          \t\tHAM\t\tHAM\r\n",
      "0003.2001-02-08.kitchen       \t\tHAM\t\tHAM\r\n",
      "0003.2003-12-18.gp            \t\tSPAM\t\tHAM\r\n",
      "0003.2004-08-01.bg            \t\tSPAM\t\tHAM\r\n",
      "0004.1999-12-10.kaminski      \t\tHAM\t\tSPAM\r\n",
      "0004.1999-12-14.farmer        \t\tHAM\t\tHAM\r\n",
      "0004.2001-04-02.williams      \t\tHAM\t\tHAM\r\n",
      "0004.2001-06-12.sa_and_hp     \t\tSPAM\t\tHAM\r\n",
      "0004.2004-08-01.bg            \t\tSPAM\t\tHAM\r\n",
      "0005.1999-12-12.kaminski      \t\tHAM\t\tSPAM\r\n",
      "0005.1999-12-14.farmer        \t\tHAM\t\tHAM\r\n",
      "0005.2000-06-06.lokay         \t\tHAM\t\tHAM\r\n",
      "0005.2001-02-08.kitchen       \t\tHAM\t\tHAM\r\n",
      "0005.2001-06-23.sa_and_hp     \t\tSPAM\t\tHAM\r\n",
      "0005.2003-12-18.gp            \t\tSPAM\t\tHAM\r\n",
      "0006.1999-12-13.kaminski      \t\tHAM\t\tHAM\r\n",
      "0006.2001-02-08.kitchen       \t\tHAM\t\tHAM\r\n",
      "0006.2001-04-03.williams      \t\tHAM\t\tHAM\r\n",
      "0006.2001-06-25.sa_and_hp     \t\tSPAM\t\tHAM\r\n",
      "0006.2003-12-18.gp            \t\tSPAM\t\tHAM\r\n",
      "0006.2004-08-01.bg            \t\tSPAM\t\tHAM\r\n",
      "0007.1999-12-13.kaminski      \t\tHAM\t\tHAM\r\n",
      "0007.1999-12-14.farmer        \t\tHAM\t\tHAM\r\n",
      "0007.2000-01-17.beck          \t\tHAM\t\tHAM\r\n",
      "0007.2001-02-09.kitchen       \t\tHAM\t\tHAM\r\n",
      "0007.2003-12-18.gp            \t\tSPAM\t\tHAM\r\n",
      "0007.2004-08-01.bg            \t\tSPAM\t\tHAM\r\n",
      "0008.2001-02-09.kitchen       \t\tHAM\t\tHAM\r\n",
      "0008.2001-06-12.sa_and_hp     \t\tSPAM\t\tHAM\r\n",
      "0008.2001-06-25.sa_and_hp     \t\tSPAM\t\tHAM\r\n",
      "0008.2003-12-18.gp            \t\tSPAM\t\tHAM\r\n",
      "0008.2004-08-01.bg            \t\tSPAM\t\tHAM\r\n",
      "0009.1999-12-13.kaminski      \t\tHAM\t\tHAM\r\n",
      "0009.1999-12-14.farmer        \t\tHAM\t\tHAM\r\n",
      "0009.2000-06-07.lokay         \t\tHAM\t\tHAM\r\n",
      "0009.2001-02-09.kitchen       \t\tHAM\t\tHAM\r\n",
      "0009.2001-06-26.sa_and_hp     \t\tSPAM\t\tHAM\r\n",
      "0009.2003-12-18.gp            \t\tSPAM\t\tHAM\r\n",
      "0010.1999-12-14.farmer        \t\tHAM\t\tHAM\r\n",
      "0010.1999-12-14.kaminski      \t\tHAM\t\tHAM\r\n",
      "0010.2001-02-09.kitchen       \t\tHAM\t\tHAM\r\n",
      "0010.2001-06-28.sa_and_hp     \t\tSPAM\t\tSPAM\r\n",
      "0010.2003-12-18.gp            \t\tSPAM\t\tHAM\r\n",
      "0010.2004-08-01.bg            \t\tSPAM\t\tHAM\r\n",
      "0011.1999-12-14.farmer        \t\tHAM\t\tHAM\r\n",
      "0011.2001-06-28.sa_and_hp     \t\tSPAM\t\tSPAM\r\n",
      "0011.2001-06-29.sa_and_hp     \t\tSPAM\t\tHAM\r\n",
      "0011.2003-12-18.gp            \t\tSPAM\t\tHAM\r\n",
      "0011.2004-08-01.bg            \t\tSPAM\t\tHAM\r\n",
      "0012.1999-12-14.farmer        \t\tHAM\t\tHAM\r\n",
      "0012.1999-12-14.kaminski      \t\tHAM\t\tHAM\r\n",
      "0012.2000-01-17.beck          \t\tHAM\t\tHAM\r\n",
      "0012.2000-06-08.lokay         \t\tHAM\t\tHAM\r\n",
      "0012.2001-02-09.kitchen       \t\tHAM\t\tHAM\r\n",
      "0012.2003-12-19.gp            \t\tSPAM\t\tHAM\r\n",
      "0013.1999-12-14.farmer        \t\tHAM\t\tHAM\r\n",
      "0013.1999-12-14.kaminski      \t\tHAM\t\tHAM\r\n",
      "0013.2001-04-03.williams      \t\tHAM\t\tHAM\r\n",
      "0013.2001-06-30.sa_and_hp     \t\tSPAM\t\tHAM\r\n",
      "0013.2004-08-01.bg            \t\tSPAM\t\tSPAM\r\n",
      "0014.1999-12-14.kaminski      \t\tHAM\t\tHAM\r\n",
      "0014.1999-12-15.farmer        \t\tHAM\t\tHAM\r\n",
      "0014.2001-02-12.kitchen       \t\tHAM\t\tHAM\r\n",
      "0014.2001-07-04.sa_and_hp     \t\tSPAM\t\tHAM\r\n",
      "0014.2003-12-19.gp            \t\tSPAM\t\tHAM\r\n",
      "0014.2004-08-01.bg            \t\tSPAM\t\tHAM\r\n",
      "0015.1999-12-14.kaminski      \t\tHAM\t\tHAM\r\n",
      "0015.1999-12-15.farmer        \t\tHAM\t\tHAM\r\n",
      "0015.2000-06-09.lokay         \t\tHAM\t\tHAM\r\n",
      "0015.2001-02-12.kitchen       \t\tHAM\t\tHAM\r\n",
      "0015.2001-07-05.sa_and_hp     \t\tSPAM\t\tHAM\r\n",
      "0015.2003-12-19.gp            \t\tSPAM\t\tHAM\r\n",
      "0016.1999-12-15.farmer        \t\tHAM\t\tHAM\r\n",
      "0016.2001-02-12.kitchen       \t\tHAM\t\tHAM\r\n",
      "0016.2001-07-05.sa_and_hp     \t\tSPAM\t\tHAM\r\n",
      "0016.2001-07-06.sa_and_hp     \t\tSPAM\t\tHAM\r\n",
      "0016.2003-12-19.gp            \t\tSPAM\t\tHAM\r\n",
      "0016.2004-08-01.bg            \t\tSPAM\t\tHAM\r\n",
      "0017.1999-12-14.kaminski      \t\tHAM\t\tHAM\r\n",
      "0017.2000-01-17.beck          \t\tHAM\t\tHAM\r\n",
      "0017.2001-04-03.williams      \t\tHAM\t\tHAM\r\n",
      "0017.2003-12-18.gp            \t\tSPAM\t\tHAM\r\n",
      "0017.2004-08-01.bg            \t\tSPAM\t\tHAM\r\n",
      "0017.2004-08-02.bg            \t\tSPAM\t\tHAM\r\n",
      "0018.1999-12-14.kaminski      \t\tHAM\t\tHAM\r\n",
      "0018.2001-07-13.sa_and_hp     \t\tSPAM\t\tSPAM\r\n",
      "0018.2003-12-18.gp            \t\tSPAM\t\tSPAM\r\n",
      "-----------------------\r\n",
      "Accuracy: 60.0%\r\n"
     ]
    }
   ],
   "source": [
    "!chmod a+x mapper13.py\n",
    "!chmod a+x reducer13.py\n",
    "\n",
    "!./pNaiveBayes.sh 4 \"assistance\" \"mapper13\" \"reducer13\"\n",
    "!cat \"enronemail_1h.txt.output\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW1.4\n",
    "Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh will classify the email messages by a list of one or more user-specified words. Examine the words “assistance”, “valium”, and “enlargementWithATypo” and report your results.\n",
    "   To do so, make sure that\n",
    "\n",
    "   - mapper.py counts all occurrences of a list of words, and\n",
    "   - reducer.py performs the multiple-word multinomial Naive Bayes classification via the chosen list.\n",
    "\n",
    "No smoothing is needed in this HW.\n",
    "\n",
    "### Mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper14.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper14.py\n",
    "#!/usr/bin/python\n",
    "## mapper14.py\n",
    "## Author: Prabhakar Gundugola\n",
    "## Description: mapper code for HW1.4\n",
    "\n",
    "import sys\n",
    "import re\n",
    "import string\n",
    "\n",
    "## collect user input\n",
    "filename = sys.argv[1]\n",
    "findwords = re.split(\" \",sys.argv[2].lower())\n",
    "\n",
    "with open (filename, \"r\") as myfile:\n",
    "    for line in myfile:\n",
    "        tokens = line.lower().split('\\t')\n",
    "        word_string = tokens[2] + ' ' + tokens[3].strip()\n",
    "        word_string = word_string.translate(string.maketrans(\"\",\"\"), string.punctuation)\n",
    "        \n",
    "        key = tokens[0] + '\\t' + tokens[1] + '\\t' + str(len(word_string.split()))\n",
    "        for word in findwords:\n",
    "            key += '\\t' + word + '\\t' + str(word_string.count(word))\n",
    "        print key\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reducer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer14.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer14.py\n",
    "#!/usr/bin/python\n",
    "## reducer13.py\n",
    "## Author: Prabhakar Gundugola\n",
    "## Description: reducer code for HW1.4\n",
    "\n",
    "import sys\n",
    "import math\n",
    "\n",
    "def isspam(true_class):\n",
    "    if true_class == 1:\n",
    "        return 'SPAM'\n",
    "    else:\n",
    "        return 'HAM'\n",
    "\n",
    "filenames = sys.argv[1:]\n",
    "\n",
    "spam_email_count = 0\n",
    "ham_email_count = 0\n",
    "\n",
    "spam_word_count = 0\n",
    "ham_word_count = 0\n",
    "\n",
    "spam_findword = {}\n",
    "ham_findword = {}\n",
    "\n",
    "total_cases = 0\n",
    "correct_cases = 0\n",
    "\n",
    "for filename in filenames:\n",
    "    with open(filename, \"r\") as myfile:\n",
    "        for line in myfile:\n",
    "            tokens = line.split('\\t')\n",
    "            doc_id = tokens[0]\n",
    "            true_class = int(tokens[1])\n",
    "            #findword = tokens[3]\n",
    "            #findword_count = int(tokens[4])\n",
    "            word_count = int(tokens[2])\n",
    "            \n",
    "            if true_class == 1:\n",
    "                spam_email_count += 1\n",
    "                spam_word_count += word_count\n",
    "            else:\n",
    "                ham_email_count += 1\n",
    "                ham_word_count += word_count\n",
    "            \n",
    "            if len(tokens) > 3:\n",
    "                for i in range(3, len(tokens), 2):\n",
    "                    findword = tokens[i]\n",
    "                    findword_count = int(tokens[i+1])\n",
    "                    \n",
    "                    if true_class == 1:\n",
    "                        if findword not in spam_findword:\n",
    "                            spam_findword[findword] = findword_count\n",
    "                        else:\n",
    "                            spam_findword[findword] += findword_count\n",
    "                    else:\n",
    "                        if findword not in ham_findword:\n",
    "                            ham_findword[findword] = findword_count\n",
    "                        else:\n",
    "                            ham_findword[findword] += findword_count\n",
    "\n",
    "spam_prior = math.log((1.0*spam_email_count)/(spam_email_count + ham_email_count))\n",
    "ham_prior = math.log((1.0*ham_email_count)/(ham_email_count + spam_email_count))\n",
    "spam_findword_prob = {}\n",
    "ham_findword_prob = {}\n",
    "\n",
    "for word in spam_findword:\n",
    "    if spam_findword[word] > 0:\n",
    "        spam_findword_prob[word] = math.log((1.0*spam_findword[word]/spam_word_count))\n",
    "    else:\n",
    "        spam_findword_prob[word] = float('-inf')\n",
    "for word in ham_findword:\n",
    "    if ham_findword[word] > 0:\n",
    "        ham_findword_prob[word] = math.log((1.0*ham_findword[word]/ham_word_count))\n",
    "    else:\n",
    "        ham_findword_prob[word] = float('-inf')\n",
    "\n",
    "# Naive Bayes classification\n",
    "for filename in filenames:\n",
    "    with open(filename, \"r\") as myfile:\n",
    "        for line in myfile:\n",
    "            total_cases += 1\n",
    "            tokens = line.split('\\t')\n",
    "            doc_id = tokens[0]\n",
    "            true_class = int(tokens[1])\n",
    "            vocab = {}\n",
    "            if len(tokens) > 3:\n",
    "                for i in range(3, len(tokens), 2):\n",
    "                    findword = tokens[i]\n",
    "                    findword_count = int(tokens[i+1])\n",
    "                    vocab[findword] = findword_count\n",
    "            \n",
    "            spam_doc_prob, ham_doc_prob = 0.0, 0.0\n",
    "            for key, value in vocab.iteritems():\n",
    "                if spam_findword_prob[key] == float('-inf'):\n",
    "                    if value == 0:\n",
    "                        spam_doc_prob += 0\n",
    "                    else:\n",
    "                        spam_doc_prob += float('-inf')\n",
    "                else:\n",
    "                    spam_doc_prob += spam_findword_prob[key]*value\n",
    "\n",
    "            for key, value in vocab.iteritems():\n",
    "                if ham_findword_prob[key] == float('-inf'):\n",
    "                    if value == 0:\n",
    "                        ham_doc_prob += 0\n",
    "                    else:\n",
    "                        ham_doc_prob += float('-inf')\n",
    "                else:\n",
    "                    ham_doc_prob += ham_findword_prob[key]*value\n",
    "                    \n",
    "            spam_doc_prob += spam_prior\n",
    "            ham_doc_prob += ham_prior\n",
    "            \n",
    "            result = doc_id.ljust(30) + '\\t\\t' + isspam(true_class) + '\\t\\t'\n",
    "            if spam_doc_prob > ham_doc_prob:\n",
    "                predicted = 1\n",
    "            else:\n",
    "                predicted = 0\n",
    "            result += isspam(predicted)\n",
    "            print result\n",
    "\n",
    "            if true_class == predicted:\n",
    "                correct_cases += 1\n",
    "\n",
    "accuracy = 100.0*correct_cases/total_cases\n",
    "print \"-----------------------\"\n",
    "print \"Accuracy: \" + str(accuracy) + '%'\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spam_email_count:  44\r\n",
      "ham_email_count:  56\r\n",
      "spam_word_count:  18283\r\n",
      "ham_word_count:  13205\r\n",
      "spam_prior:  -0.82098055207\r\n",
      "ham_prior:  -0.579818495253\r\n",
      "0001.1999-12-10.farmer        \t\tHAM\t\tHAM\r\n",
      "0001.1999-12-10.kaminski      \t\tHAM\t\tHAM\r\n",
      "0001.2000-01-17.beck          \t\tHAM\t\tHAM\r\n",
      "0001.2000-06-06.lokay         \t\tHAM\t\tHAM\r\n",
      "0001.2001-02-07.kitchen       \t\tHAM\t\tHAM\r\n",
      "0001.2001-04-02.williams      \t\tHAM\t\tHAM\r\n",
      "0002.1999-12-13.farmer        \t\tHAM\t\tHAM\r\n",
      "0002.2001-02-07.kitchen       \t\tHAM\t\tHAM\r\n",
      "0002.2001-05-25.sa_and_hp     \t\tSPAM\t\tHAM\r\n",
      "0002.2003-12-18.gp            \t\tSPAM\t\tHAM\r\n",
      "0002.2004-08-01.bg            \t\tSPAM\t\tSPAM\r\n",
      "0003.1999-12-10.kaminski      \t\tHAM\t\tHAM\r\n",
      "0003.1999-12-14.farmer        \t\tHAM\t\tHAM\r\n",
      "0003.2000-01-17.beck          \t\tHAM\t\tHAM\r\n",
      "0003.2001-02-08.kitchen       \t\tHAM\t\tHAM\r\n",
      "0003.2003-12-18.gp            \t\tSPAM\t\tHAM\r\n",
      "0003.2004-08-01.bg            \t\tSPAM\t\tHAM\r\n",
      "0004.1999-12-10.kaminski      \t\tHAM\t\tSPAM\r\n",
      "0004.1999-12-14.farmer        \t\tHAM\t\tHAM\r\n",
      "0004.2001-04-02.williams      \t\tHAM\t\tHAM\r\n",
      "0004.2001-06-12.sa_and_hp     \t\tSPAM\t\tHAM\r\n",
      "0004.2004-08-01.bg            \t\tSPAM\t\tHAM\r\n",
      "0005.1999-12-12.kaminski      \t\tHAM\t\tSPAM\r\n",
      "0005.1999-12-14.farmer        \t\tHAM\t\tHAM\r\n",
      "0005.2000-06-06.lokay         \t\tHAM\t\tHAM\r\n",
      "0005.2001-02-08.kitchen       \t\tHAM\t\tHAM\r\n",
      "0005.2001-06-23.sa_and_hp     \t\tSPAM\t\tHAM\r\n",
      "0005.2003-12-18.gp            \t\tSPAM\t\tHAM\r\n",
      "0006.1999-12-13.kaminski      \t\tHAM\t\tHAM\r\n",
      "0006.2001-02-08.kitchen       \t\tHAM\t\tHAM\r\n",
      "0006.2001-04-03.williams      \t\tHAM\t\tHAM\r\n",
      "0006.2001-06-25.sa_and_hp     \t\tSPAM\t\tHAM\r\n",
      "0006.2003-12-18.gp            \t\tSPAM\t\tHAM\r\n",
      "0006.2004-08-01.bg            \t\tSPAM\t\tHAM\r\n",
      "0007.1999-12-13.kaminski      \t\tHAM\t\tHAM\r\n",
      "0007.1999-12-14.farmer        \t\tHAM\t\tHAM\r\n",
      "0007.2000-01-17.beck          \t\tHAM\t\tHAM\r\n",
      "0007.2001-02-09.kitchen       \t\tHAM\t\tHAM\r\n",
      "0007.2003-12-18.gp            \t\tSPAM\t\tHAM\r\n",
      "0007.2004-08-01.bg            \t\tSPAM\t\tHAM\r\n",
      "0008.2001-02-09.kitchen       \t\tHAM\t\tHAM\r\n",
      "0008.2001-06-12.sa_and_hp     \t\tSPAM\t\tHAM\r\n",
      "0008.2001-06-25.sa_and_hp     \t\tSPAM\t\tHAM\r\n",
      "0008.2003-12-18.gp            \t\tSPAM\t\tHAM\r\n",
      "0008.2004-08-01.bg            \t\tSPAM\t\tHAM\r\n",
      "0009.1999-12-13.kaminski      \t\tHAM\t\tHAM\r\n",
      "0009.1999-12-14.farmer        \t\tHAM\t\tHAM\r\n",
      "0009.2000-06-07.lokay         \t\tHAM\t\tHAM\r\n",
      "0009.2001-02-09.kitchen       \t\tHAM\t\tHAM\r\n",
      "0009.2001-06-26.sa_and_hp     \t\tSPAM\t\tHAM\r\n",
      "0009.2003-12-18.gp            \t\tSPAM\t\tSPAM\r\n",
      "0010.1999-12-14.farmer        \t\tHAM\t\tHAM\r\n",
      "0010.1999-12-14.kaminski      \t\tHAM\t\tHAM\r\n",
      "0010.2001-02-09.kitchen       \t\tHAM\t\tHAM\r\n",
      "0010.2001-06-28.sa_and_hp     \t\tSPAM\t\tSPAM\r\n",
      "0010.2003-12-18.gp            \t\tSPAM\t\tHAM\r\n",
      "0010.2004-08-01.bg            \t\tSPAM\t\tHAM\r\n",
      "0011.1999-12-14.farmer        \t\tHAM\t\tHAM\r\n",
      "0011.2001-06-28.sa_and_hp     \t\tSPAM\t\tSPAM\r\n",
      "0011.2001-06-29.sa_and_hp     \t\tSPAM\t\tHAM\r\n",
      "0011.2003-12-18.gp            \t\tSPAM\t\tHAM\r\n",
      "0011.2004-08-01.bg            \t\tSPAM\t\tHAM\r\n",
      "0012.1999-12-14.farmer        \t\tHAM\t\tHAM\r\n",
      "0012.1999-12-14.kaminski      \t\tHAM\t\tHAM\r\n",
      "0012.2000-01-17.beck          \t\tHAM\t\tHAM\r\n",
      "0012.2000-06-08.lokay         \t\tHAM\t\tHAM\r\n",
      "0012.2001-02-09.kitchen       \t\tHAM\t\tHAM\r\n",
      "0012.2003-12-19.gp            \t\tSPAM\t\tHAM\r\n",
      "0013.1999-12-14.farmer        \t\tHAM\t\tHAM\r\n",
      "0013.1999-12-14.kaminski      \t\tHAM\t\tHAM\r\n",
      "0013.2001-04-03.williams      \t\tHAM\t\tHAM\r\n",
      "0013.2001-06-30.sa_and_hp     \t\tSPAM\t\tHAM\r\n",
      "0013.2004-08-01.bg            \t\tSPAM\t\tSPAM\r\n",
      "0014.1999-12-14.kaminski      \t\tHAM\t\tHAM\r\n",
      "0014.1999-12-15.farmer        \t\tHAM\t\tHAM\r\n",
      "0014.2001-02-12.kitchen       \t\tHAM\t\tHAM\r\n",
      "0014.2001-07-04.sa_and_hp     \t\tSPAM\t\tHAM\r\n",
      "0014.2003-12-19.gp            \t\tSPAM\t\tHAM\r\n",
      "0014.2004-08-01.bg            \t\tSPAM\t\tHAM\r\n",
      "0015.1999-12-14.kaminski      \t\tHAM\t\tHAM\r\n",
      "0015.1999-12-15.farmer        \t\tHAM\t\tHAM\r\n",
      "0015.2000-06-09.lokay         \t\tHAM\t\tHAM\r\n",
      "0015.2001-02-12.kitchen       \t\tHAM\t\tHAM\r\n",
      "0015.2001-07-05.sa_and_hp     \t\tSPAM\t\tHAM\r\n",
      "0015.2003-12-19.gp            \t\tSPAM\t\tHAM\r\n",
      "0016.1999-12-15.farmer        \t\tHAM\t\tHAM\r\n",
      "0016.2001-02-12.kitchen       \t\tHAM\t\tHAM\r\n",
      "0016.2001-07-05.sa_and_hp     \t\tSPAM\t\tHAM\r\n",
      "0016.2001-07-06.sa_and_hp     \t\tSPAM\t\tHAM\r\n",
      "0016.2003-12-19.gp            \t\tSPAM\t\tSPAM\r\n",
      "0016.2004-08-01.bg            \t\tSPAM\t\tHAM\r\n",
      "0017.1999-12-14.kaminski      \t\tHAM\t\tHAM\r\n",
      "0017.2000-01-17.beck          \t\tHAM\t\tHAM\r\n",
      "0017.2001-04-03.williams      \t\tHAM\t\tHAM\r\n",
      "0017.2003-12-18.gp            \t\tSPAM\t\tHAM\r\n",
      "0017.2004-08-01.bg            \t\tSPAM\t\tSPAM\r\n",
      "0017.2004-08-02.bg            \t\tSPAM\t\tHAM\r\n",
      "0018.1999-12-14.kaminski      \t\tHAM\t\tHAM\r\n",
      "0018.2001-07-13.sa_and_hp     \t\tSPAM\t\tSPAM\r\n",
      "0018.2003-12-18.gp            \t\tSPAM\t\tSPAM\r\n",
      "-----------------------\r\n",
      "Accuracy: 63.0%\r\n"
     ]
    }
   ],
   "source": [
    "!chmod a+x mapper14.py\n",
    "!chmod a+x reducer14.py\n",
    "\n",
    "!./pNaiveBayes.sh 1 \"assistance valium enlargementWithATypo\" \"mapper14\" \"reducer14\"\n",
    "!cat \"enronemail_1h.txt.output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

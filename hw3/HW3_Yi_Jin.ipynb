{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATSCIW261 Assignment 3\n",
    "\n",
    "**Name: Yi Jin**\n",
    "\n",
    "**Email: yjin@ischool.berkeley.edu**\n",
    "\n",
    "**Session: W261-3**\n",
    "\n",
    "**Week 3**\n",
    "\n",
    "**Date: Jan 29, 2016**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW3.0. ###\n",
    "\n",
    "**What is a merge sort? Where is it used in Hadoop? **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mergesort is an efficient sorting algorithm. It divides the unsorted list into n  sublists, each containing 1 element. Then it repeatedly merge sublists to produce new sorted sublists until only one sorted list remaining. Instead of sorting the complete list, sorting partially sorted lists is much more efficient. It generally runs in time O(n log n), given n sorted sublists.\n",
    "\n",
    "Mergesort is used in hadoop shuffle between map output and reduce input. It merges the output from multiple mappers to a sorted list of key/value pairs for reducer. It is one of the important keys in Hadoop MapReduce."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** How is  a combiner function in the context of Hadoop? ** \n",
    "\n",
    "** Give an example where it can be used and justify why it should be used in the context of this problem. **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A combiner is an optional program in MapReduce. It summarize the map output records with the same key. The output(key-value collection) of the combiner will be sent over the network to the actual Reducer task as input. \n",
    "\n",
    "The Combiner is used in between the Mapper and the Reducer to reduce the volume of data transfer between Map and Reduce. Usually, the output of the map task is large and the data transferred to the reduce task is high.\n",
    "\n",
    "There are three ways to implement a combiner: as part of a mapper, a separate combiner program, and as part of a reducer.\n",
    "\n",
    "WordCount program is an example. The mapper emits a record for each word, so numerous records are created in the map phase. These records need to be sorted and transferred to the reducer. A combiner can be added to aggregate the counts by words to improve the sorting the transfer efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** What is the Hadoop shuffle? **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Hadoop shuffle is the heart of MapReduce. It is the process that the system performs the sort, and transfers the map outputs to the reducers as inputs. It includes three phases: Partition, Sort, and Combine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This script is Deprecated. Instead use start-dfs.sh and start-yarn.sh\n",
      "16/01/29 23:04:20 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Starting namenodes on [localhost]\n",
      "localhost: starting namenode, logging to /usr/local/hadoop/logs/hadoop-ubuntu-namenode-ip-172-31-25-140.out\n",
      "localhost: starting datanode, logging to /usr/local/hadoop/logs/hadoop-ubuntu-datanode-ip-172-31-25-140.out\n",
      "Starting secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: starting secondarynamenode, logging to /usr/local/hadoop/logs/hadoop-ubuntu-secondarynamenode-ip-172-31-25-140.out\n",
      "16/01/29 23:04:35 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "starting yarn daemons\n",
      "starting resourcemanager, logging to /usr/local/hadoop/logs/yarn-ubuntu-resourcemanager-ip-172-31-25-140.out\n",
      "localhost: starting nodemanager, logging to /usr/local/hadoop/logs/yarn-ubuntu-nodemanager-ip-172-31-25-140.out\n"
     ]
    }
   ],
   "source": [
    "# start hadoop\n",
    "!/usr/local/hadoop/sbin/start-all.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW3.1 Use Counters to do EDA (exploratory data analysis and to monitor progress) ###\n",
    "\n",
    "Now, letâ€™s use Hadoop Counters to identify the number of complaints pertaining to debt collection, mortgage and other categories (all other categories get lumped into this one) in the consumer complaints dataset. Basically produce the distribution of the Product column in this dataset using counters (limited to 3 counters here).\n",
    "\n",
    "Hadoop offers Job Tracker, an UI tool to determine the status and statistics of all jobs. Using the job tracker UI, developers can view the Counters that have been created. Screenshot your  job tracker UI as your job completes and include it here. Make sure that your user defined counters are visible. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/31 06:00:37 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/31 06:00:39 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "put: `/hw3/Consumer_Complaints.csv': File exists\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -mkdir -p /hw3\n",
    "!hdfs dfs -put Consumer_Complaints.csv /hw3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting 3_1_countMapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile 3_1_countMapper.py\n",
    "#!/usr/bin/python\n",
    "#HW 3.1 - Mapper with category count\n",
    "\n",
    "import sys\n",
    "\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    fields = line.split(',')\n",
    "\n",
    "    product = fields[1]\n",
    "    if product == \"Debt collection\":\n",
    "        sys.stderr.write(\"reporter:counter:Product,Debt,1\\n\")\n",
    "        print \"Debt_collection\\t1\"\n",
    "    elif product == \"Mortgage\":\n",
    "        sys.stderr.write(\"reporter:counter:Product,Mortgage,1\\n\")        \n",
    "        print \"Mortgage\\t1\"\n",
    "    else:\n",
    "        print \"Other_categories\\t1\"\n",
    "        sys.stderr.write(\"reporter:counter:Product,Other,1\\n\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting 3_1_reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile 3_1_reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "\n",
    "current_word = None\n",
    "current_count = 0\n",
    "word = None\n",
    "\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    word, count = line.split('\\t', 1)\n",
    "\n",
    "    # convert count (currently a string) to int\n",
    "    try:\n",
    "        count = int(count)\n",
    "    except ValueError:\n",
    "        # count was not a number, so silently ignore/discard this line\n",
    "        continue\n",
    "\n",
    "    if current_word == word:\n",
    "        current_count += count\n",
    "    else:\n",
    "        if current_word:\n",
    "            # write result to STDOUT\n",
    "            print '%s\\t%s' % (current_word, current_count)\n",
    "        current_count = count\n",
    "        current_word = word\n",
    "\n",
    "# output the last word\n",
    "if current_word == word:\n",
    "    print '%s\\t%s' % (current_word, current_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/04 00:51:24 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/04 00:51:25 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /hw3/3_1_out\n",
      "16/02/04 00:51:26 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/04 00:51:26 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/02/04 00:51:26 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/02/04 00:51:26 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/02/04 00:51:26 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/02/04 00:51:26 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/02/04 00:51:27 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local298997729_0001\n",
      "16/02/04 00:51:27 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/02/04 00:51:27 INFO mapreduce.Job: Running job: job_local298997729_0001\n",
      "16/02/04 00:51:27 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/02/04 00:51:27 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/02/04 00:51:27 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/02/04 00:51:27 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/02/04 00:51:27 INFO mapred.LocalJobRunner: Starting task: attempt_local298997729_0001_m_000000_0\n",
      "16/02/04 00:51:27 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/02/04 00:51:27 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "16/02/04 00:51:27 INFO mapred.MapTask: Processing split: hdfs://localhost:54310/hw3/Consumer_Complaints.csv:0+50906486\n",
      "16/02/04 00:51:27 INFO mapred.MapTask: numReduceTasks: 1\n",
      "16/02/04 00:51:27 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/02/04 00:51:27 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/02/04 00:51:27 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/02/04 00:51:27 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/02/04 00:51:27 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/02/04 00:51:27 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/02/04 00:51:27 INFO streaming.PipeMapRed: PipeMapRed exec [/home/ubuntu/HW3/./3_1_countMapper.py]\n",
      "16/02/04 00:51:27 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/02/04 00:51:27 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/02/04 00:51:27 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/02/04 00:51:27 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/02/04 00:51:27 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/02/04 00:51:27 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/02/04 00:51:27 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/02/04 00:51:27 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/02/04 00:51:27 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/02/04 00:51:27 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/02/04 00:51:27 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/02/04 00:51:27 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/02/04 00:51:27 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 00:51:27 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 00:51:27 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 00:51:27 INFO streaming.PipeMapRed: Records R/W=784/1\n",
      "16/02/04 00:51:27 INFO streaming.PipeMapRed: R/W/S=1000/244/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 00:51:27 INFO streaming.PipeMapRed: R/W/S=10000/9381/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 00:51:28 INFO mapreduce.Job: Job job_local298997729_0001 running in uber mode : false\n",
      "16/02/04 00:51:28 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/02/04 00:51:28 INFO streaming.PipeMapRed: R/W/S=100000/99354/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 00:51:28 INFO streaming.PipeMapRed: R/W/S=200000/198921/0 in:200000=200000/1 [rec/s] out:198921=198921/1 [rec/s]\n",
      "16/02/04 00:51:29 INFO streaming.PipeMapRed: R/W/S=300000/299310/0 in:300000=300000/1 [rec/s] out:299310=299310/1 [rec/s]\n",
      "16/02/04 00:51:29 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/04 00:51:29 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/04 00:51:29 INFO mapred.LocalJobRunner: \n",
      "16/02/04 00:51:29 INFO mapred.MapTask: Starting flush of map output\n",
      "16/02/04 00:51:29 INFO mapred.MapTask: Spilling map output\n",
      "16/02/04 00:51:29 INFO mapred.MapTask: bufstart = 0; bufend = 4894959; bufvoid = 104857600\n",
      "16/02/04 00:51:29 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 24962748(99850992); length = 1251649/6553600\n",
      "16/02/04 00:51:29 INFO mapred.MapTask: Finished spill 0\n",
      "16/02/04 00:51:29 INFO mapred.Task: Task:attempt_local298997729_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/02/04 00:51:29 INFO mapred.LocalJobRunner: Records R/W=784/1\n",
      "16/02/04 00:51:29 INFO mapred.Task: Task 'attempt_local298997729_0001_m_000000_0' done.\n",
      "16/02/04 00:51:29 INFO mapred.LocalJobRunner: Finishing task: attempt_local298997729_0001_m_000000_0\n",
      "16/02/04 00:51:29 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/02/04 00:51:29 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/02/04 00:51:29 INFO mapred.LocalJobRunner: Starting task: attempt_local298997729_0001_r_000000_0\n",
      "16/02/04 00:51:29 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/02/04 00:51:29 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "16/02/04 00:51:29 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@39741f43\n",
      "16/02/04 00:51:29 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=333971456, maxSingleShuffleLimit=83492864, mergeThreshold=220421168, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/04 00:51:29 INFO reduce.EventFetcher: attempt_local298997729_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/04 00:51:29 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local298997729_0001_m_000000_0 decomp: 5520787 len: 5520791 to MEMORY\n",
      "16/02/04 00:51:29 INFO reduce.InMemoryMapOutput: Read 5520787 bytes from map-output for attempt_local298997729_0001_m_000000_0\n",
      "16/02/04 00:51:29 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 5520787, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->5520787\n",
      "16/02/04 00:51:29 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/04 00:51:29 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/04 00:51:29 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/04 00:51:29 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/04 00:51:29 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 5520769 bytes\n",
      "16/02/04 00:51:29 INFO reduce.MergeManagerImpl: Merged 1 segments, 5520787 bytes to disk to satisfy reduce memory limit\n",
      "16/02/04 00:51:29 INFO reduce.MergeManagerImpl: Merging 1 files, 5520791 bytes from disk\n",
      "16/02/04 00:51:29 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/04 00:51:29 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/04 00:51:29 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 5520769 bytes\n",
      "16/02/04 00:51:29 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/04 00:51:29 INFO streaming.PipeMapRed: PipeMapRed exec [/home/ubuntu/HW3/./3_1_reducer.py]\n",
      "16/02/04 00:51:29 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/02/04 00:51:29 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/02/04 00:51:29 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 00:51:29 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 00:51:29 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 00:51:29 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 00:51:29 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 00:51:30 INFO streaming.PipeMapRed: R/W/S=100000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 00:51:30 INFO streaming.PipeMapRed: R/W/S=200000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 00:51:30 INFO streaming.PipeMapRed: R/W/S=300000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 00:51:30 INFO streaming.PipeMapRed: Records R/W=312913/1\n",
      "16/02/04 00:51:30 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/04 00:51:30 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/04 00:51:30 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/02/04 00:51:30 INFO mapred.Task: Task:attempt_local298997729_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/02/04 00:51:30 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/04 00:51:30 INFO mapred.Task: Task attempt_local298997729_0001_r_000000_0 is allowed to commit now\n",
      "16/02/04 00:51:30 INFO output.FileOutputCommitter: Saved output of task 'attempt_local298997729_0001_r_000000_0' to hdfs://localhost:54310/hw3/3_1_out/_temporary/0/task_local298997729_0001_r_000000\n",
      "16/02/04 00:51:30 INFO mapred.LocalJobRunner: Records R/W=312913/1 > reduce\n",
      "16/02/04 00:51:30 INFO mapred.Task: Task 'attempt_local298997729_0001_r_000000_0' done.\n",
      "16/02/04 00:51:30 INFO mapred.LocalJobRunner: Finishing task: attempt_local298997729_0001_r_000000_0\n",
      "16/02/04 00:51:30 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/02/04 00:51:31 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/02/04 00:51:31 INFO mapreduce.Job: Job job_local298997729_0001 completed successfully\n",
      "16/02/04 00:51:31 INFO mapreduce.Job: Counters: 38\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=11253750\n",
      "\t\tFILE: Number of bytes written=17331233\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=101812972\n",
      "\t\tHDFS: Number of bytes written=62\n",
      "\t\tHDFS: Number of read operations=13\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=312913\n",
      "\t\tMap output records=312913\n",
      "\t\tMap output bytes=4894959\n",
      "\t\tMap output materialized bytes=5520791\n",
      "\t\tInput split bytes=102\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=3\n",
      "\t\tReduce shuffle bytes=5520791\n",
      "\t\tReduce input records=312913\n",
      "\t\tReduce output records=3\n",
      "\t\tSpilled Records=625826\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=20\n",
      "\t\tTotal committed heap usage (bytes)=666894336\n",
      "\tProduct\n",
      "\t\tDebt=44372\n",
      "\t\tMortgage=125752\n",
      "\t\tOther=142789\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=50906486\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=62\n",
      "16/02/04 00:51:31 INFO streaming.StreamJob: Output directory: /hw3/3_1_out\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r /hw3/3_1_out\n",
    "\n",
    "!hadoop jar /usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar \\\n",
    "-mapper 3_1_countMapper.py -reducer 3_1_reducer.py -input /hw3/Consumer_Complaints.csv -output /hw3/3_1_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/31 03:22:58 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Debt_collection\t44372\n",
      "Mortgage\t125752\n",
      "Other_categories\t142789\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /hw3/3_1_out/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Counter output screenshot**\n",
    "\n",
    "<img src=\"/home/ubuntu/HW3/3_1_Counter.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW 3.2 Analyze the performance of your Mappers, Combiners and Reducers using Counters ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For this brief study the Input file will be one record (the next line only): **\n",
    "\n",
    "foo foo quux labs foo bar quux\n",
    "\n",
    "**Perform a word count analysis of this single record dataset using a Mapper and Reducer based WordCount (i.e., no combiners are used here) using user defined Counters to count up how many time the mapper and reducer are called. What is the value of your user defined Mapper Counter, and Reducer Counter after completing this word count job. The answer  should be 1 and 4 respectively. Please explain. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 3_2_wordcount.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile 3_2_wordcount.txt\n",
    "foo foo quux labs foo bar quux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/31 04:06:24 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -put 3_2_wordcount.txt /hw3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting 3_2_mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile 3_2_mapper.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "import sys\n",
    "# increment the counter once mapper is called\n",
    "sys.stderr.write(\"reporter:counter:Calls,mapper_calls,1\\n\")\n",
    "\n",
    "for line in sys.stdin:\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "    words = line.split()\n",
    "    for word in words:\n",
    "        \n",
    "        print '%s\\t%s' % (word, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting 3_2_reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile 3_2_reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "\n",
    "sys.stderr.write(\"reporter:counter:Calls,reducer_calls,1\\n\")\n",
    "current_word = None\n",
    "current_count = 0\n",
    "word = None\n",
    "\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    word, count = line.split('\\t', 1)\n",
    "\n",
    "    # convert count (currently a string) to int\n",
    "    try:\n",
    "        count = int(count)\n",
    "    except ValueError:\n",
    "        # count was not a number, so silently ignore/discard this line\n",
    "        continue\n",
    "\n",
    "    if current_word == word:\n",
    "        current_count += count\n",
    "    else:\n",
    "        if current_word:\n",
    "            # write result to STDOUT\n",
    "            print '%s\\t%s' % (current_word, current_count)\n",
    "        current_count = count\n",
    "        current_word = word\n",
    "\n",
    "# output the last word\n",
    "if current_word == word:\n",
    "    print '%s\\t%s' % (current_word, current_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/31 04:23:57 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/31 04:23:58 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /hw3/3_2_out\n",
      "16/01/31 04:23:59 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/31 04:24:00 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/01/31 04:24:00 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/01/31 04:24:00 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/01/31 04:24:00 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/01/31 04:24:00 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/01/31 04:24:00 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local396141848_0001\n",
      "16/01/31 04:24:00 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/01/31 04:24:00 INFO mapreduce.Job: Running job: job_local396141848_0001\n",
      "16/01/31 04:24:00 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/01/31 04:24:00 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/01/31 04:24:00 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/31 04:24:00 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/01/31 04:24:00 INFO mapred.LocalJobRunner: Starting task: attempt_local396141848_0001_m_000000_0\n",
      "16/01/31 04:24:00 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/31 04:24:00 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "16/01/31 04:24:00 INFO mapred.MapTask: Processing split: hdfs://localhost:54310/hw3/3_2_wordcount.txt:0+30\n",
      "16/01/31 04:24:00 INFO mapred.MapTask: numReduceTasks: 4\n",
      "16/01/31 04:24:00 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/01/31 04:24:00 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/01/31 04:24:00 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/01/31 04:24:00 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/01/31 04:24:00 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/01/31 04:24:00 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/01/31 04:24:01 INFO streaming.PipeMapRed: PipeMapRed exec [/home/ubuntu/HW3/./3_2_mapper.py]\n",
      "16/01/31 04:24:01 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/01/31 04:24:01 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/01/31 04:24:01 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/01/31 04:24:01 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/01/31 04:24:01 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/01/31 04:24:01 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/01/31 04:24:01 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/01/31 04:24:01 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/01/31 04:24:01 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/01/31 04:24:01 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/01/31 04:24:01 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/01/31 04:24:01 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/01/31 04:24:01 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 04:24:01 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/31 04:24:01 INFO streaming.PipeMapRed: Records R/W=1/1\n",
      "16/01/31 04:24:01 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/31 04:24:01 INFO mapred.LocalJobRunner: \n",
      "16/01/31 04:24:01 INFO mapred.MapTask: Starting flush of map output\n",
      "16/01/31 04:24:01 INFO mapred.MapTask: Spilling map output\n",
      "16/01/31 04:24:01 INFO mapred.MapTask: bufstart = 0; bufend = 45; bufvoid = 104857600\n",
      "16/01/31 04:24:01 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214372(104857488); length = 25/6553600\n",
      "16/01/31 04:24:01 INFO mapred.MapTask: Finished spill 0\n",
      "16/01/31 04:24:01 INFO mapred.Task: Task:attempt_local396141848_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/01/31 04:24:01 INFO mapred.LocalJobRunner: Records R/W=1/1\n",
      "16/01/31 04:24:01 INFO mapred.Task: Task 'attempt_local396141848_0001_m_000000_0' done.\n",
      "16/01/31 04:24:01 INFO mapred.LocalJobRunner: Finishing task: attempt_local396141848_0001_m_000000_0\n",
      "16/01/31 04:24:01 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/01/31 04:24:01 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/01/31 04:24:01 INFO mapred.LocalJobRunner: Starting task: attempt_local396141848_0001_r_000000_0\n",
      "16/01/31 04:24:01 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/31 04:24:01 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "16/01/31 04:24:01 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@3fcf041f\n",
      "16/01/31 04:24:01 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=333971456, maxSingleShuffleLimit=83492864, mergeThreshold=220421168, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/01/31 04:24:01 INFO reduce.EventFetcher: attempt_local396141848_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/01/31 04:24:01 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local396141848_0001_m_000000_0 decomp: 20 len: 24 to MEMORY\n",
      "16/01/31 04:24:01 INFO reduce.InMemoryMapOutput: Read 20 bytes from map-output for attempt_local396141848_0001_m_000000_0\n",
      "16/01/31 04:24:01 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 20, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->20\n",
      "16/01/31 04:24:01 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/01/31 04:24:01 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/31 04:24:01 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/01/31 04:24:01 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/31 04:24:01 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 13 bytes\n",
      "16/01/31 04:24:01 INFO reduce.MergeManagerImpl: Merged 1 segments, 20 bytes to disk to satisfy reduce memory limit\n",
      "16/01/31 04:24:01 INFO reduce.MergeManagerImpl: Merging 1 files, 24 bytes from disk\n",
      "16/01/31 04:24:01 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/01/31 04:24:01 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/31 04:24:01 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 13 bytes\n",
      "16/01/31 04:24:01 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/31 04:24:01 INFO streaming.PipeMapRed: PipeMapRed exec [/home/ubuntu/HW3/./3_2_reducer.py]\n",
      "16/01/31 04:24:01 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/01/31 04:24:01 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/01/31 04:24:01 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 04:24:01 INFO streaming.PipeMapRed: Records R/W=2/1\n",
      "16/01/31 04:24:01 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/31 04:24:01 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/31 04:24:01 INFO mapred.Task: Task:attempt_local396141848_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/01/31 04:24:01 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/31 04:24:01 INFO mapred.Task: Task attempt_local396141848_0001_r_000000_0 is allowed to commit now\n",
      "16/01/31 04:24:01 INFO output.FileOutputCommitter: Saved output of task 'attempt_local396141848_0001_r_000000_0' to hdfs://localhost:54310/hw3/3_2_out/_temporary/0/task_local396141848_0001_r_000000\n",
      "16/01/31 04:24:01 INFO mapred.LocalJobRunner: Records R/W=2/1 > reduce\n",
      "16/01/31 04:24:01 INFO mapred.Task: Task 'attempt_local396141848_0001_r_000000_0' done.\n",
      "16/01/31 04:24:01 INFO mapred.LocalJobRunner: Finishing task: attempt_local396141848_0001_r_000000_0\n",
      "16/01/31 04:24:01 INFO mapred.LocalJobRunner: Starting task: attempt_local396141848_0001_r_000001_0\n",
      "16/01/31 04:24:01 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/31 04:24:01 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "16/01/31 04:24:01 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@7047125\n",
      "16/01/31 04:24:01 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=333971456, maxSingleShuffleLimit=83492864, mergeThreshold=220421168, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/01/31 04:24:01 INFO reduce.EventFetcher: attempt_local396141848_0001_r_000001_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/01/31 04:24:01 INFO reduce.LocalFetcher: localfetcher#2 about to shuffle output of map attempt_local396141848_0001_m_000000_0 decomp: 26 len: 30 to MEMORY\n",
      "16/01/31 04:24:01 INFO reduce.InMemoryMapOutput: Read 26 bytes from map-output for attempt_local396141848_0001_m_000000_0\n",
      "16/01/31 04:24:01 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 26, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->26\n",
      "16/01/31 04:24:01 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/01/31 04:24:01 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/31 04:24:01 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/01/31 04:24:01 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/31 04:24:01 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 20 bytes\n",
      "16/01/31 04:24:01 INFO reduce.MergeManagerImpl: Merged 1 segments, 26 bytes to disk to satisfy reduce memory limit\n",
      "16/01/31 04:24:01 INFO reduce.MergeManagerImpl: Merging 1 files, 30 bytes from disk\n",
      "16/01/31 04:24:01 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/01/31 04:24:01 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/31 04:24:01 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 20 bytes\n",
      "16/01/31 04:24:01 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/31 04:24:01 INFO streaming.PipeMapRed: PipeMapRed exec [/home/ubuntu/HW3/./3_2_reducer.py]\n",
      "16/01/31 04:24:01 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/01/31 04:24:01 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 04:24:01 INFO streaming.PipeMapRed: Records R/W=3/1\n",
      "16/01/31 04:24:01 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/31 04:24:01 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/31 04:24:01 INFO mapred.Task: Task:attempt_local396141848_0001_r_000001_0 is done. And is in the process of committing\n",
      "16/01/31 04:24:01 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/31 04:24:01 INFO mapred.Task: Task attempt_local396141848_0001_r_000001_0 is allowed to commit now\n",
      "16/01/31 04:24:01 INFO output.FileOutputCommitter: Saved output of task 'attempt_local396141848_0001_r_000001_0' to hdfs://localhost:54310/hw3/3_2_out/_temporary/0/task_local396141848_0001_r_000001\n",
      "16/01/31 04:24:01 INFO mapred.LocalJobRunner: Records R/W=3/1 > reduce\n",
      "16/01/31 04:24:01 INFO mapred.Task: Task 'attempt_local396141848_0001_r_000001_0' done.\n",
      "16/01/31 04:24:01 INFO mapred.LocalJobRunner: Finishing task: attempt_local396141848_0001_r_000001_0\n",
      "16/01/31 04:24:01 INFO mapred.LocalJobRunner: Starting task: attempt_local396141848_0001_r_000002_0\n",
      "16/01/31 04:24:01 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/31 04:24:01 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "16/01/31 04:24:01 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@37900029\n",
      "16/01/31 04:24:01 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=333971456, maxSingleShuffleLimit=83492864, mergeThreshold=220421168, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/01/31 04:24:01 INFO reduce.EventFetcher: attempt_local396141848_0001_r_000002_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/01/31 04:24:01 INFO reduce.LocalFetcher: localfetcher#3 about to shuffle output of map attempt_local396141848_0001_m_000000_0 decomp: 10 len: 14 to MEMORY\n",
      "16/01/31 04:24:01 INFO reduce.InMemoryMapOutput: Read 10 bytes from map-output for attempt_local396141848_0001_m_000000_0\n",
      "16/01/31 04:24:01 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 10, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->10\n",
      "16/01/31 04:24:01 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/01/31 04:24:01 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/31 04:24:01 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/01/31 04:24:01 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/31 04:24:01 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 4 bytes\n",
      "16/01/31 04:24:01 INFO reduce.MergeManagerImpl: Merged 1 segments, 10 bytes to disk to satisfy reduce memory limit\n",
      "16/01/31 04:24:01 INFO reduce.MergeManagerImpl: Merging 1 files, 14 bytes from disk\n",
      "16/01/31 04:24:01 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/01/31 04:24:01 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/31 04:24:01 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 4 bytes\n",
      "16/01/31 04:24:01 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/31 04:24:01 INFO streaming.PipeMapRed: PipeMapRed exec [/home/ubuntu/HW3/./3_2_reducer.py]\n",
      "16/01/31 04:24:01 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 04:24:01 INFO streaming.PipeMapRed: Records R/W=1/1\n",
      "16/01/31 04:24:01 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/31 04:24:01 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/31 04:24:01 INFO mapred.Task: Task:attempt_local396141848_0001_r_000002_0 is done. And is in the process of committing\n",
      "16/01/31 04:24:01 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/31 04:24:01 INFO mapred.Task: Task attempt_local396141848_0001_r_000002_0 is allowed to commit now\n",
      "16/01/31 04:24:01 INFO output.FileOutputCommitter: Saved output of task 'attempt_local396141848_0001_r_000002_0' to hdfs://localhost:54310/hw3/3_2_out/_temporary/0/task_local396141848_0001_r_000002\n",
      "16/01/31 04:24:01 INFO mapred.LocalJobRunner: Records R/W=1/1 > reduce\n",
      "16/01/31 04:24:01 INFO mapred.Task: Task 'attempt_local396141848_0001_r_000002_0' done.\n",
      "16/01/31 04:24:01 INFO mapred.LocalJobRunner: Finishing task: attempt_local396141848_0001_r_000002_0\n",
      "16/01/31 04:24:01 INFO mapred.LocalJobRunner: Starting task: attempt_local396141848_0001_r_000003_0\n",
      "16/01/31 04:24:01 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/31 04:24:01 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "16/01/31 04:24:01 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@1ae78d87\n",
      "16/01/31 04:24:01 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=333971456, maxSingleShuffleLimit=83492864, mergeThreshold=220421168, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/01/31 04:24:01 INFO reduce.EventFetcher: attempt_local396141848_0001_r_000003_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/01/31 04:24:01 INFO reduce.LocalFetcher: localfetcher#4 about to shuffle output of map attempt_local396141848_0001_m_000000_0 decomp: 11 len: 15 to MEMORY\n",
      "16/01/31 04:24:01 INFO reduce.InMemoryMapOutput: Read 11 bytes from map-output for attempt_local396141848_0001_m_000000_0\n",
      "16/01/31 04:24:01 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 11, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->11\n",
      "16/01/31 04:24:01 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/01/31 04:24:01 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/31 04:24:01 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/01/31 04:24:01 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/31 04:24:01 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 4 bytes\n",
      "16/01/31 04:24:01 INFO reduce.MergeManagerImpl: Merged 1 segments, 11 bytes to disk to satisfy reduce memory limit\n",
      "16/01/31 04:24:01 INFO reduce.MergeManagerImpl: Merging 1 files, 15 bytes from disk\n",
      "16/01/31 04:24:01 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/01/31 04:24:01 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/31 04:24:01 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 4 bytes\n",
      "16/01/31 04:24:01 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/31 04:24:01 INFO streaming.PipeMapRed: PipeMapRed exec [/home/ubuntu/HW3/./3_2_reducer.py]\n",
      "16/01/31 04:24:01 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 04:24:01 INFO streaming.PipeMapRed: Records R/W=1/1\n",
      "16/01/31 04:24:01 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/31 04:24:01 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/31 04:24:01 INFO mapreduce.Job: Job job_local396141848_0001 running in uber mode : false\n",
      "16/01/31 04:24:01 INFO mapreduce.Job:  map 100% reduce 75%\n",
      "16/01/31 04:24:01 INFO mapred.Task: Task:attempt_local396141848_0001_r_000003_0 is done. And is in the process of committing\n",
      "16/01/31 04:24:01 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/31 04:24:01 INFO mapred.Task: Task attempt_local396141848_0001_r_000003_0 is allowed to commit now\n",
      "16/01/31 04:24:01 INFO output.FileOutputCommitter: Saved output of task 'attempt_local396141848_0001_r_000003_0' to hdfs://localhost:54310/hw3/3_2_out/_temporary/0/task_local396141848_0001_r_000003\n",
      "16/01/31 04:24:01 INFO mapred.LocalJobRunner: Records R/W=1/1 > reduce\n",
      "16/01/31 04:24:01 INFO mapred.Task: Task 'attempt_local396141848_0001_r_000003_0' done.\n",
      "16/01/31 04:24:01 INFO mapred.LocalJobRunner: Finishing task: attempt_local396141848_0001_r_000003_0\n",
      "16/01/31 04:24:01 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/01/31 04:24:02 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/01/31 04:24:02 INFO mapreduce.Job: Job job_local396141848_0001 completed successfully\n",
      "16/01/31 04:24:02 INFO mapreduce.Job: Counters: 37\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=532141\n",
      "\t\tFILE: Number of bytes written=1922964\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=150\n",
      "\t\tHDFS: Number of bytes written=65\n",
      "\t\tHDFS: Number of read operations=55\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=25\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=1\n",
      "\t\tMap output records=7\n",
      "\t\tMap output bytes=45\n",
      "\t\tMap output materialized bytes=83\n",
      "\t\tInput split bytes=96\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=4\n",
      "\t\tReduce shuffle bytes=83\n",
      "\t\tReduce input records=7\n",
      "\t\tReduce output records=4\n",
      "\t\tSpilled Records=14\n",
      "\t\tShuffled Maps =4\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=4\n",
      "\t\tGC time elapsed (ms)=16\n",
      "\t\tTotal committed heap usage (bytes)=1525153792\n",
      "\tCalls\n",
      "\t\tmapper_calls=1\n",
      "\t\treducer_calls=4\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=30\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=26\n",
      "16/01/31 04:24:02 INFO streaming.StreamJob: Output directory: /hw3/3_2_out\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r /hw3/3_2_out\n",
    "\n",
    "!hadoop jar /usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar \\\n",
    "-mapper 3_2_mapper.py -reducer 3_2_reducer.py -input /hw3/3_2_wordcount.txt -output /hw3/3_2_out \\\n",
    "-numReduceTasks 4 #Set the number of reducers to 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the number of reducers to 4 in the command. The mapper is called once due to the small input file size. The reducer is called 4 times.\n",
    "\n",
    "    Calls\n",
    "\t\tmapper_calls=1\n",
    "\t\treducer_calls=4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Please use mulitple mappers and reducers for these jobs (at least 2 mappers and 2 reducers).\n",
    "Perform a word count analysis of the Issue column of the Consumer Complaints  Dataset using a Mapper and Reducer based WordCount (i.e., no combiners used anywhere)  using user defined Counters to count up how many time the mapper and reducer are called. What is the value of your user defined Mapper Counter, and Reducer Counter after completing your word count job.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting 3_2_issue_mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile 3_2_issue_mapper.py\n",
    "#!/usr/bin/python\n",
    "#HW 3.2\n",
    "\n",
    "import sys\n",
    "import re\n",
    "\n",
    "WORD_RE = re.compile(r\"[\\w']+\") #Compile regex to easily parse complete words\n",
    "sys.stderr.write(\"reporter:counter:Calls,mapper_calls,1\\n\")\n",
    "\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    fields = line.split(',')\n",
    "    # extract issue column\n",
    "    issue=fields[3].strip()\n",
    "    words=re.findall(WORD_RE, issue) #create list of words\n",
    "    \n",
    "    # emit the word\n",
    "    for word in words:\n",
    "        print '%s\\t%s' % (word, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/31 05:37:07 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/31 05:37:08 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /hw3/3_2_issue_out\n",
      "16/01/31 05:37:09 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/31 05:37:10 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/01/31 05:37:10 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/01/31 05:37:10 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/01/31 05:37:10 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/01/31 05:37:10 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/01/31 05:37:10 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "16/01/31 05:37:10 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/01/31 05:37:10 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local676675752_0001\n",
      "16/01/31 05:37:10 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/01/31 05:37:10 INFO mapreduce.Job: Running job: job_local676675752_0001\n",
      "16/01/31 05:37:10 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/01/31 05:37:10 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/01/31 05:37:10 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/31 05:37:10 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/01/31 05:37:10 INFO mapred.LocalJobRunner: Starting task: attempt_local676675752_0001_m_000000_0\n",
      "16/01/31 05:37:10 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/31 05:37:10 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "16/01/31 05:37:10 INFO mapred.MapTask: Processing split: hdfs://localhost:54310/hw3/Consumer_Complaints.csv:0+50906486\n",
      "16/01/31 05:37:10 INFO mapred.MapTask: numReduceTasks: 4\n",
      "16/01/31 05:37:10 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/01/31 05:37:10 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/01/31 05:37:10 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/01/31 05:37:10 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/01/31 05:37:10 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/01/31 05:37:10 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/01/31 05:37:10 INFO streaming.PipeMapRed: PipeMapRed exec [/home/ubuntu/HW3/./3_2_issue_mapper.py]\n",
      "16/01/31 05:37:10 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/01/31 05:37:10 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/01/31 05:37:10 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/01/31 05:37:10 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/01/31 05:37:10 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/01/31 05:37:10 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/01/31 05:37:10 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/01/31 05:37:10 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/01/31 05:37:10 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/01/31 05:37:10 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/01/31 05:37:10 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/01/31 05:37:10 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/01/31 05:37:10 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 05:37:10 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 05:37:10 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 05:37:10 INFO streaming.PipeMapRed: Records R/W=779/1\n",
      "16/01/31 05:37:10 INFO streaming.PipeMapRed: R/W/S=1000/329/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 05:37:11 INFO streaming.PipeMapRed: R/W/S=10000/32846/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 05:37:11 INFO mapreduce.Job: Job job_local676675752_0001 running in uber mode : false\n",
      "16/01/31 05:37:11 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/01/31 05:37:11 INFO streaming.PipeMapRed: R/W/S=100000/350944/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 05:37:12 INFO streaming.PipeMapRed: R/W/S=200000/673318/0 in:200000=200000/1 [rec/s] out:673318=673318/1 [rec/s]\n",
      "16/01/31 05:37:12 INFO streaming.PipeMapRed: R/W/S=300000/947128/0 in:150000=300000/2 [rec/s] out:473564=947128/2 [rec/s]\n",
      "16/01/31 05:37:13 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/31 05:37:13 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/31 05:37:13 INFO mapred.LocalJobRunner: \n",
      "16/01/31 05:37:13 INFO mapred.MapTask: Starting flush of map output\n",
      "16/01/31 05:37:13 INFO mapred.MapTask: Spilling map output\n",
      "16/01/31 05:37:13 INFO mapred.MapTask: bufstart = 0; bufend = 9272509; bufvoid = 104857600\n",
      "16/01/31 05:37:13 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 22292468(89169872); length = 3921929/6553600\n",
      "16/01/31 05:37:13 INFO mapred.MapTask: Finished spill 0\n",
      "16/01/31 05:37:13 INFO mapred.Task: Task:attempt_local676675752_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/01/31 05:37:13 INFO mapred.LocalJobRunner: Records R/W=779/1\n",
      "16/01/31 05:37:13 INFO mapred.Task: Task 'attempt_local676675752_0001_m_000000_0' done.\n",
      "16/01/31 05:37:13 INFO mapred.LocalJobRunner: Finishing task: attempt_local676675752_0001_m_000000_0\n",
      "16/01/31 05:37:13 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/01/31 05:37:13 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/01/31 05:37:13 INFO mapred.LocalJobRunner: Starting task: attempt_local676675752_0001_r_000000_0\n",
      "16/01/31 05:37:13 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/31 05:37:13 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "16/01/31 05:37:13 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@5cbe0cd7\n",
      "16/01/31 05:37:14 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=333971456, maxSingleShuffleLimit=83492864, mergeThreshold=220421168, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/01/31 05:37:14 INFO reduce.EventFetcher: attempt_local676675752_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/01/31 05:37:14 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local676675752_0001_m_000000_0 decomp: 2322636 len: 2322640 to MEMORY\n",
      "16/01/31 05:37:14 INFO reduce.InMemoryMapOutput: Read 2322636 bytes from map-output for attempt_local676675752_0001_m_000000_0\n",
      "16/01/31 05:37:14 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 2322636, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->2322636\n",
      "16/01/31 05:37:14 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/01/31 05:37:14 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/31 05:37:14 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/01/31 05:37:14 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/31 05:37:14 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 2322626 bytes\n",
      "16/01/31 05:37:14 INFO reduce.MergeManagerImpl: Merged 1 segments, 2322636 bytes to disk to satisfy reduce memory limit\n",
      "16/01/31 05:37:14 INFO reduce.MergeManagerImpl: Merging 1 files, 2322640 bytes from disk\n",
      "16/01/31 05:37:14 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/01/31 05:37:14 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/31 05:37:14 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 2322626 bytes\n",
      "16/01/31 05:37:14 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/31 05:37:14 INFO streaming.PipeMapRed: PipeMapRed exec [/home/ubuntu/HW3/./3_2_reducer.py]\n",
      "16/01/31 05:37:14 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/01/31 05:37:14 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/01/31 05:37:14 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 05:37:14 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 05:37:14 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 05:37:14 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 05:37:14 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 05:37:14 INFO streaming.PipeMapRed: R/W/S=100000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 05:37:14 INFO streaming.PipeMapRed: R/W/S=200000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 05:37:14 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/31 05:37:14 INFO streaming.PipeMapRed: Records R/W=233473/1\n",
      "16/01/31 05:37:14 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/31 05:37:14 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/01/31 05:37:14 INFO mapred.Task: Task:attempt_local676675752_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/01/31 05:37:14 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/31 05:37:14 INFO mapred.Task: Task attempt_local676675752_0001_r_000000_0 is allowed to commit now\n",
      "16/01/31 05:37:14 INFO output.FileOutputCommitter: Saved output of task 'attempt_local676675752_0001_r_000000_0' to hdfs://localhost:54310/hw3/3_2_issue_out/_temporary/0/task_local676675752_0001_r_000000\n",
      "16/01/31 05:37:14 INFO mapred.LocalJobRunner: Records R/W=233473/1 > reduce\n",
      "16/01/31 05:37:14 INFO mapred.Task: Task 'attempt_local676675752_0001_r_000000_0' done.\n",
      "16/01/31 05:37:14 INFO mapred.LocalJobRunner: Finishing task: attempt_local676675752_0001_r_000000_0\n",
      "16/01/31 05:37:14 INFO mapred.LocalJobRunner: Starting task: attempt_local676675752_0001_r_000001_0\n",
      "16/01/31 05:37:14 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/31 05:37:14 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "16/01/31 05:37:14 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@7da612aa\n",
      "16/01/31 05:37:14 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=333971456, maxSingleShuffleLimit=83492864, mergeThreshold=220421168, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/01/31 05:37:14 INFO reduce.EventFetcher: attempt_local676675752_0001_r_000001_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/01/31 05:37:14 INFO reduce.LocalFetcher: localfetcher#2 about to shuffle output of map attempt_local676675752_0001_m_000000_0 decomp: 4945858 len: 4945862 to MEMORY\n",
      "16/01/31 05:37:14 INFO reduce.InMemoryMapOutput: Read 4945858 bytes from map-output for attempt_local676675752_0001_m_000000_0\n",
      "16/01/31 05:37:14 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 4945858, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->4945858\n",
      "16/01/31 05:37:14 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/01/31 05:37:14 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/31 05:37:14 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/01/31 05:37:14 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/31 05:37:14 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 4945852 bytes\n",
      "16/01/31 05:37:14 INFO reduce.MergeManagerImpl: Merged 1 segments, 4945858 bytes to disk to satisfy reduce memory limit\n",
      "16/01/31 05:37:14 INFO reduce.MergeManagerImpl: Merging 1 files, 4945862 bytes from disk\n",
      "16/01/31 05:37:14 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/01/31 05:37:14 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/31 05:37:14 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 4945852 bytes\n",
      "16/01/31 05:37:14 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/31 05:37:14 INFO streaming.PipeMapRed: PipeMapRed exec [/home/ubuntu/HW3/./3_2_reducer.py]\n",
      "16/01/31 05:37:14 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/01/31 05:37:14 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 05:37:14 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 05:37:14 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 05:37:14 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 05:37:14 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 05:37:15 INFO streaming.PipeMapRed: R/W/S=100000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 05:37:15 INFO streaming.PipeMapRed: R/W/S=200000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 05:37:15 INFO streaming.PipeMapRed: R/W/S=300000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 05:37:15 INFO streaming.PipeMapRed: R/W/S=400000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 05:37:15 INFO streaming.PipeMapRed: Records R/W=409916/1\n",
      "16/01/31 05:37:15 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/31 05:37:15 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/31 05:37:15 INFO mapred.Task: Task:attempt_local676675752_0001_r_000001_0 is done. And is in the process of committing\n",
      "16/01/31 05:37:15 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/31 05:37:15 INFO mapred.Task: Task attempt_local676675752_0001_r_000001_0 is allowed to commit now\n",
      "16/01/31 05:37:15 INFO output.FileOutputCommitter: Saved output of task 'attempt_local676675752_0001_r_000001_0' to hdfs://localhost:54310/hw3/3_2_issue_out/_temporary/0/task_local676675752_0001_r_000001\n",
      "16/01/31 05:37:15 INFO mapred.LocalJobRunner: Records R/W=409916/1 > reduce\n",
      "16/01/31 05:37:15 INFO mapred.Task: Task 'attempt_local676675752_0001_r_000001_0' done.\n",
      "16/01/31 05:37:15 INFO mapred.LocalJobRunner: Finishing task: attempt_local676675752_0001_r_000001_0\n",
      "16/01/31 05:37:15 INFO mapred.LocalJobRunner: Starting task: attempt_local676675752_0001_r_000002_0\n",
      "16/01/31 05:37:15 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/31 05:37:15 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "16/01/31 05:37:15 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@52f35438\n",
      "16/01/31 05:37:15 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=333971456, maxSingleShuffleLimit=83492864, mergeThreshold=220421168, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/01/31 05:37:15 INFO reduce.EventFetcher: attempt_local676675752_0001_r_000002_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/01/31 05:37:15 INFO reduce.LocalFetcher: localfetcher#3 about to shuffle output of map attempt_local676675752_0001_m_000000_0 decomp: 1953944 len: 1953948 to MEMORY\n",
      "16/01/31 05:37:15 INFO reduce.InMemoryMapOutput: Read 1953944 bytes from map-output for attempt_local676675752_0001_m_000000_0\n",
      "16/01/31 05:37:15 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 1953944, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->1953944\n",
      "16/01/31 05:37:15 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/01/31 05:37:15 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/31 05:37:15 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/01/31 05:37:15 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/31 05:37:15 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 1953938 bytes\n",
      "16/01/31 05:37:15 INFO reduce.MergeManagerImpl: Merged 1 segments, 1953944 bytes to disk to satisfy reduce memory limit\n",
      "16/01/31 05:37:15 INFO reduce.MergeManagerImpl: Merging 1 files, 1953948 bytes from disk\n",
      "16/01/31 05:37:15 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/01/31 05:37:15 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/31 05:37:15 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 1953938 bytes\n",
      "16/01/31 05:37:15 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/31 05:37:15 INFO streaming.PipeMapRed: PipeMapRed exec [/home/ubuntu/HW3/./3_2_reducer.py]\n",
      "16/01/31 05:37:15 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 05:37:15 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 05:37:15 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 05:37:15 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 05:37:15 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 05:37:15 INFO streaming.PipeMapRed: R/W/S=100000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 05:37:15 INFO mapreduce.Job:  map 100% reduce 50%\n",
      "16/01/31 05:37:15 INFO streaming.PipeMapRed: Records R/W=174366/1\n",
      "16/01/31 05:37:15 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/31 05:37:15 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/31 05:37:15 INFO mapred.Task: Task:attempt_local676675752_0001_r_000002_0 is done. And is in the process of committing\n",
      "16/01/31 05:37:15 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/31 05:37:15 INFO mapred.Task: Task attempt_local676675752_0001_r_000002_0 is allowed to commit now\n",
      "16/01/31 05:37:15 INFO output.FileOutputCommitter: Saved output of task 'attempt_local676675752_0001_r_000002_0' to hdfs://localhost:54310/hw3/3_2_issue_out/_temporary/0/task_local676675752_0001_r_000002\n",
      "16/01/31 05:37:15 INFO mapred.LocalJobRunner: Records R/W=174366/1 > reduce\n",
      "16/01/31 05:37:15 INFO mapred.Task: Task 'attempt_local676675752_0001_r_000002_0' done.\n",
      "16/01/31 05:37:15 INFO mapred.LocalJobRunner: Finishing task: attempt_local676675752_0001_r_000002_0\n",
      "16/01/31 05:37:15 INFO mapred.LocalJobRunner: Starting task: attempt_local676675752_0001_r_000003_0\n",
      "16/01/31 05:37:15 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/31 05:37:15 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "16/01/31 05:37:15 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@6f8c90fd\n",
      "16/01/31 05:37:15 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=333971456, maxSingleShuffleLimit=83492864, mergeThreshold=220421168, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/01/31 05:37:15 INFO reduce.EventFetcher: attempt_local676675752_0001_r_000003_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/01/31 05:37:15 INFO reduce.LocalFetcher: localfetcher#4 about to shuffle output of map attempt_local676675752_0001_m_000000_0 decomp: 2011045 len: 2011049 to MEMORY\n",
      "16/01/31 05:37:15 INFO reduce.InMemoryMapOutput: Read 2011045 bytes from map-output for attempt_local676675752_0001_m_000000_0\n",
      "16/01/31 05:37:15 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 2011045, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->2011045\n",
      "16/01/31 05:37:15 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/01/31 05:37:15 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/31 05:37:15 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/01/31 05:37:15 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/31 05:37:15 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 2011031 bytes\n",
      "16/01/31 05:37:15 INFO reduce.MergeManagerImpl: Merged 1 segments, 2011045 bytes to disk to satisfy reduce memory limit\n",
      "16/01/31 05:37:15 INFO reduce.MergeManagerImpl: Merging 1 files, 2011049 bytes from disk\n",
      "16/01/31 05:37:15 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/01/31 05:37:15 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/31 05:37:15 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 2011031 bytes\n",
      "16/01/31 05:37:15 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/31 05:37:15 INFO streaming.PipeMapRed: PipeMapRed exec [/home/ubuntu/HW3/./3_2_reducer.py]\n",
      "16/01/31 05:37:15 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 05:37:15 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 05:37:15 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 05:37:15 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 05:37:15 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 05:37:15 INFO streaming.PipeMapRed: R/W/S=100000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 05:37:15 INFO streaming.PipeMapRed: Records R/W=162728/1\n",
      "16/01/31 05:37:15 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/31 05:37:15 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/31 05:37:15 INFO mapred.Task: Task:attempt_local676675752_0001_r_000003_0 is done. And is in the process of committing\n",
      "16/01/31 05:37:15 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/31 05:37:15 INFO mapred.Task: Task attempt_local676675752_0001_r_000003_0 is allowed to commit now\n",
      "16/01/31 05:37:15 INFO output.FileOutputCommitter: Saved output of task 'attempt_local676675752_0001_r_000003_0' to hdfs://localhost:54310/hw3/3_2_issue_out/_temporary/0/task_local676675752_0001_r_000003\n",
      "16/01/31 05:37:15 INFO mapred.LocalJobRunner: Records R/W=162728/1 > reduce\n",
      "16/01/31 05:37:15 INFO mapred.Task: Task 'attempt_local676675752_0001_r_000003_0' done.\n",
      "16/01/31 05:37:15 INFO mapred.LocalJobRunner: Finishing task: attempt_local676675752_0001_r_000003_0\n",
      "16/01/31 05:37:15 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/01/31 05:37:16 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/01/31 05:37:16 INFO mapreduce.Job: Job job_local676675752_0001 completed successfully\n",
      "16/01/31 05:37:16 INFO mapreduce.Job: Counters: 37\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=60662390\n",
      "\t\tFILE: Number of bytes written=88138196\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=254532430\n",
      "\t\tHDFS: Number of bytes written=5534\n",
      "\t\tHDFS: Number of read operations=55\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=25\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=312913\n",
      "\t\tMap output records=980483\n",
      "\t\tMap output bytes=9272509\n",
      "\t\tMap output materialized bytes=11233499\n",
      "\t\tInput split bytes=102\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=181\n",
      "\t\tReduce shuffle bytes=11233499\n",
      "\t\tReduce input records=980483\n",
      "\t\tReduce output records=181\n",
      "\t\tSpilled Records=1960966\n",
      "\t\tShuffled Maps =4\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=4\n",
      "\t\tGC time elapsed (ms)=8\n",
      "\t\tTotal committed heap usage (bytes)=1600651264\n",
      "\tCalls\n",
      "\t\tmapper_calls=1\n",
      "\t\treducer_calls=4\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=50906486\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=2229\n",
      "16/01/31 05:37:16 INFO streaming.StreamJob: Output directory: /hw3/3_2_issue_out\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r /hw3/3_2_issue_out\n",
    "\n",
    "!hadoop jar /usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar \\\n",
    "-D mapred.map.tasks=4 \\\n",
    "-D mapred.reduce.tasks=4 \\\n",
    "-mapper 3_2_issue_mapper.py -reducer 3_2_reducer.py -input /hw3/Consumer_Complaints.csv -output /hw3/3_2_issue_out \\\n",
    "#-numReduceTasks 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the hadoop streaming command, set the number of mappers and reducers to 4. When the task is completed, the mapper is called once, and the reducer is called four times. The input file is split according to the block size, which overridden the suggested mapper numbers in the command."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Perform a word count analysis of the Issue column of the Consumer Complaints  Dataset using a Mapper, Reducer, and standalone combiner (i.e., not an in-memory combiner) based WordCount using user defined Counters to count up how many time the mapper, combiner, reducer are called. What is the value of your user defined Mapper Counter, and Reducer Counter after completing your word count job.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 3_2_combiner.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile 3_2_combiner.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import ast\n",
    "\n",
    "# key changes and new counts\n",
    "prev_key = None\n",
    "total_count = 0\n",
    "\n",
    "sys.stderr.write(\"reporter:counter:Calls,combiner_calls,1\\n\")\n",
    "# Sum counts and print it\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    key, count = line.split('\\t', 1)\n",
    "    \n",
    "    try:\n",
    "        count = int(count)\n",
    "    except ValueError:\n",
    "        # count was not a number, so silently ignore/discard this line\n",
    "        continue\n",
    "\n",
    "    if key == prev_key:\n",
    "        total_count += count\n",
    "    else:\n",
    "        # Key changed, time to output the previous one.\n",
    "        if prev_key:\n",
    "            print '%s\\t%s' % (prev_key, total_count)\n",
    "        total_count = count\n",
    "        prev_key = key\n",
    "\n",
    "# Don't forget to output the last key that we just counted.\n",
    "if prev_key == key:\n",
    "    print '%s\\t%s' % (key, total_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/31 06:01:35 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "rm: `/hw3/3_2_combiner_out': No such file or directory\n",
      "16/01/31 06:01:37 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/31 06:01:37 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/01/31 06:01:37 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/01/31 06:01:37 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/01/31 06:01:38 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/01/31 06:01:38 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/01/31 06:01:38 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "16/01/31 06:01:38 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/01/31 06:01:38 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1108734189_0001\n",
      "16/01/31 06:01:38 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/01/31 06:01:38 INFO mapreduce.Job: Running job: job_local1108734189_0001\n",
      "16/01/31 06:01:38 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/01/31 06:01:38 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/01/31 06:01:38 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/31 06:01:38 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/01/31 06:01:38 INFO mapred.LocalJobRunner: Starting task: attempt_local1108734189_0001_m_000000_0\n",
      "16/01/31 06:01:38 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/31 06:01:38 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "16/01/31 06:01:38 INFO mapred.MapTask: Processing split: hdfs://localhost:54310/hw3/Consumer_Complaints.csv:0+50906486\n",
      "16/01/31 06:01:38 INFO mapred.MapTask: numReduceTasks: 4\n",
      "16/01/31 06:01:38 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/01/31 06:01:38 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/01/31 06:01:38 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/01/31 06:01:38 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/01/31 06:01:38 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/01/31 06:01:38 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/01/31 06:01:38 INFO streaming.PipeMapRed: PipeMapRed exec [/home/ubuntu/HW3/./3_2_issue_mapper.py]\n",
      "16/01/31 06:01:38 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/01/31 06:01:38 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/01/31 06:01:38 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/01/31 06:01:38 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/01/31 06:01:38 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/01/31 06:01:38 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/01/31 06:01:38 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/01/31 06:01:38 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/01/31 06:01:38 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/01/31 06:01:38 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/01/31 06:01:38 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/01/31 06:01:38 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/01/31 06:01:38 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 06:01:38 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 06:01:38 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 06:01:38 INFO streaming.PipeMapRed: Records R/W=779/1\n",
      "16/01/31 06:01:38 INFO streaming.PipeMapRed: R/W/S=1000/558/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 06:01:38 INFO streaming.PipeMapRed: R/W/S=10000/32409/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 06:01:39 INFO mapreduce.Job: Job job_local1108734189_0001 running in uber mode : false\n",
      "16/01/31 06:01:39 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/01/31 06:01:39 INFO streaming.PipeMapRed: R/W/S=100000/350620/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 06:01:40 INFO streaming.PipeMapRed: R/W/S=200000/673331/0 in:200000=200000/1 [rec/s] out:673331=673331/1 [rec/s]\n",
      "16/01/31 06:01:40 INFO streaming.PipeMapRed: R/W/S=300000/947128/0 in:300000=300000/1 [rec/s] out:947128=947128/1 [rec/s]\n",
      "16/01/31 06:01:40 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/31 06:01:40 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/31 06:01:40 INFO mapred.LocalJobRunner: \n",
      "16/01/31 06:01:40 INFO mapred.MapTask: Starting flush of map output\n",
      "16/01/31 06:01:40 INFO mapred.MapTask: Spilling map output\n",
      "16/01/31 06:01:40 INFO mapred.MapTask: bufstart = 0; bufend = 9272509; bufvoid = 104857600\n",
      "16/01/31 06:01:40 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 22292468(89169872); length = 3921929/6553600\n",
      "16/01/31 06:01:41 INFO streaming.PipeMapRed: PipeMapRed exec [/home/ubuntu/HW3/./3_2_combiner.py]\n",
      "16/01/31 06:01:41 INFO Configuration.deprecation: mapred.skip.map.auto.incr.proc.count is deprecated. Instead, use mapreduce.map.skip.proc-count.auto-incr\n",
      "16/01/31 06:01:41 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 06:01:41 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 06:01:41 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 06:01:41 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 06:01:41 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 06:01:41 INFO streaming.PipeMapRed: R/W/S=100000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 06:01:41 INFO streaming.PipeMapRed: R/W/S=200000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 06:01:41 INFO streaming.PipeMapRed: Records R/W=233473/1\n",
      "16/01/31 06:01:41 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/31 06:01:41 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/31 06:01:41 INFO streaming.PipeMapRed: PipeMapRed exec [/home/ubuntu/HW3/./3_2_combiner.py]\n",
      "16/01/31 06:01:41 INFO Configuration.deprecation: mapred.skip.reduce.auto.incr.proc.count is deprecated. Instead, use mapreduce.reduce.skip.proc-count.auto-incr\n",
      "16/01/31 06:01:41 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 06:01:41 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 06:01:41 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 06:01:41 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 06:01:41 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 06:01:41 INFO streaming.PipeMapRed: R/W/S=100000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 06:01:41 INFO streaming.PipeMapRed: R/W/S=200000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 06:01:41 INFO streaming.PipeMapRed: R/W/S=300000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 06:01:41 INFO streaming.PipeMapRed: R/W/S=400000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 06:01:41 INFO streaming.PipeMapRed: Records R/W=409916/1\n",
      "16/01/31 06:01:41 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/31 06:01:41 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/31 06:01:41 INFO streaming.PipeMapRed: PipeMapRed exec [/home/ubuntu/HW3/./3_2_combiner.py]\n",
      "16/01/31 06:01:41 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 06:01:41 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 06:01:41 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 06:01:41 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 06:01:41 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 06:01:42 INFO streaming.PipeMapRed: R/W/S=100000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 06:01:42 INFO streaming.PipeMapRed: Records R/W=174366/1\n",
      "16/01/31 06:01:42 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/31 06:01:42 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/31 06:01:42 INFO streaming.PipeMapRed: PipeMapRed exec [/home/ubuntu/HW3/./3_2_combiner.py]\n",
      "16/01/31 06:01:42 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 06:01:42 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 06:01:42 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 06:01:42 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 06:01:42 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 06:01:42 INFO streaming.PipeMapRed: R/W/S=100000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 06:01:42 INFO streaming.PipeMapRed: Records R/W=162728/1\n",
      "16/01/31 06:01:42 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/31 06:01:42 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/31 06:01:42 INFO mapred.MapTask: Finished spill 0\n",
      "16/01/31 06:01:42 INFO mapred.Task: Task:attempt_local1108734189_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/01/31 06:01:42 INFO mapred.LocalJobRunner: Records R/W=162728/1\n",
      "16/01/31 06:01:42 INFO mapred.Task: Task 'attempt_local1108734189_0001_m_000000_0' done.\n",
      "16/01/31 06:01:42 INFO mapred.LocalJobRunner: Finishing task: attempt_local1108734189_0001_m_000000_0\n",
      "16/01/31 06:01:42 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/01/31 06:01:42 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/01/31 06:01:42 INFO mapred.LocalJobRunner: Starting task: attempt_local1108734189_0001_r_000000_0\n",
      "16/01/31 06:01:42 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/31 06:01:42 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "16/01/31 06:01:42 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@76ff2a4d\n",
      "16/01/31 06:01:42 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=333971456, maxSingleShuffleLimit=83492864, mergeThreshold=220421168, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/01/31 06:01:42 INFO reduce.EventFetcher: attempt_local1108734189_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/01/31 06:01:42 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1108734189_0001_m_000000_0 decomp: 595 len: 599 to MEMORY\n",
      "16/01/31 06:01:42 INFO reduce.InMemoryMapOutput: Read 595 bytes from map-output for attempt_local1108734189_0001_m_000000_0\n",
      "16/01/31 06:01:42 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 595, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->595\n",
      "16/01/31 06:01:42 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/01/31 06:01:42 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/31 06:01:42 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/01/31 06:01:42 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/31 06:01:42 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 585 bytes\n",
      "16/01/31 06:01:42 INFO reduce.MergeManagerImpl: Merged 1 segments, 595 bytes to disk to satisfy reduce memory limit\n",
      "16/01/31 06:01:42 INFO reduce.MergeManagerImpl: Merging 1 files, 599 bytes from disk\n",
      "16/01/31 06:01:42 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/01/31 06:01:42 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/31 06:01:42 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 585 bytes\n",
      "16/01/31 06:01:42 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/31 06:01:42 INFO streaming.PipeMapRed: PipeMapRed exec [/home/ubuntu/HW3/./3_2_reducer.py]\n",
      "16/01/31 06:01:42 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/01/31 06:01:42 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/01/31 06:01:42 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 06:01:42 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 06:01:42 INFO streaming.PipeMapRed: Records R/W=45/1\n",
      "16/01/31 06:01:42 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/31 06:01:42 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/31 06:01:42 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/01/31 06:01:42 INFO mapred.Task: Task:attempt_local1108734189_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/01/31 06:01:42 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/31 06:01:42 INFO mapred.Task: Task attempt_local1108734189_0001_r_000000_0 is allowed to commit now\n",
      "16/01/31 06:01:42 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1108734189_0001_r_000000_0' to hdfs://localhost:54310/hw3/3_2_combiner_out/_temporary/0/task_local1108734189_0001_r_000000\n",
      "16/01/31 06:01:42 INFO mapred.LocalJobRunner: Records R/W=45/1 > reduce\n",
      "16/01/31 06:01:42 INFO mapred.Task: Task 'attempt_local1108734189_0001_r_000000_0' done.\n",
      "16/01/31 06:01:42 INFO mapred.LocalJobRunner: Finishing task: attempt_local1108734189_0001_r_000000_0\n",
      "16/01/31 06:01:42 INFO mapred.LocalJobRunner: Starting task: attempt_local1108734189_0001_r_000001_0\n",
      "16/01/31 06:01:42 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/31 06:01:42 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "16/01/31 06:01:42 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@d5cbf25\n",
      "16/01/31 06:01:42 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=333971456, maxSingleShuffleLimit=83492864, mergeThreshold=220421168, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/01/31 06:01:42 INFO reduce.EventFetcher: attempt_local1108734189_0001_r_000001_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/01/31 06:01:42 INFO reduce.LocalFetcher: localfetcher#2 about to shuffle output of map attempt_local1108734189_0001_m_000000_0 decomp: 727 len: 731 to MEMORY\n",
      "16/01/31 06:01:42 INFO reduce.InMemoryMapOutput: Read 727 bytes from map-output for attempt_local1108734189_0001_m_000000_0\n",
      "16/01/31 06:01:42 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 727, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->727\n",
      "16/01/31 06:01:42 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/01/31 06:01:42 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/31 06:01:42 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/01/31 06:01:42 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/31 06:01:42 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 721 bytes\n",
      "16/01/31 06:01:42 INFO reduce.MergeManagerImpl: Merged 1 segments, 727 bytes to disk to satisfy reduce memory limit\n",
      "16/01/31 06:01:42 INFO reduce.MergeManagerImpl: Merging 1 files, 731 bytes from disk\n",
      "16/01/31 06:01:42 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/01/31 06:01:42 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/31 06:01:42 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 721 bytes\n",
      "16/01/31 06:01:42 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/31 06:01:42 INFO streaming.PipeMapRed: PipeMapRed exec [/home/ubuntu/HW3/./3_2_reducer.py]\n",
      "16/01/31 06:01:42 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/01/31 06:01:42 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 06:01:42 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 06:01:42 INFO streaming.PipeMapRed: Records R/W=49/1\n",
      "16/01/31 06:01:42 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/31 06:01:42 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/31 06:01:42 INFO mapred.Task: Task:attempt_local1108734189_0001_r_000001_0 is done. And is in the process of committing\n",
      "16/01/31 06:01:42 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/31 06:01:42 INFO mapred.Task: Task attempt_local1108734189_0001_r_000001_0 is allowed to commit now\n",
      "16/01/31 06:01:42 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1108734189_0001_r_000001_0' to hdfs://localhost:54310/hw3/3_2_combiner_out/_temporary/0/task_local1108734189_0001_r_000001\n",
      "16/01/31 06:01:42 INFO mapred.LocalJobRunner: Records R/W=49/1 > reduce\n",
      "16/01/31 06:01:42 INFO mapred.Task: Task 'attempt_local1108734189_0001_r_000001_0' done.\n",
      "16/01/31 06:01:42 INFO mapred.LocalJobRunner: Finishing task: attempt_local1108734189_0001_r_000001_0\n",
      "16/01/31 06:01:42 INFO mapred.LocalJobRunner: Starting task: attempt_local1108734189_0001_r_000002_0\n",
      "16/01/31 06:01:42 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/31 06:01:42 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "16/01/31 06:01:42 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@30f148b\n",
      "16/01/31 06:01:42 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=333971456, maxSingleShuffleLimit=83492864, mergeThreshold=220421168, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/01/31 06:01:42 INFO reduce.EventFetcher: attempt_local1108734189_0001_r_000002_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/01/31 06:01:42 INFO reduce.LocalFetcher: localfetcher#3 about to shuffle output of map attempt_local1108734189_0001_m_000000_0 decomp: 628 len: 632 to MEMORY\n",
      "16/01/31 06:01:42 INFO reduce.InMemoryMapOutput: Read 628 bytes from map-output for attempt_local1108734189_0001_m_000000_0\n",
      "16/01/31 06:01:42 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 628, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->628\n",
      "16/01/31 06:01:42 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/01/31 06:01:42 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/31 06:01:42 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/01/31 06:01:42 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/31 06:01:42 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 622 bytes\n",
      "16/01/31 06:01:42 INFO reduce.MergeManagerImpl: Merged 1 segments, 628 bytes to disk to satisfy reduce memory limit\n",
      "16/01/31 06:01:42 INFO reduce.MergeManagerImpl: Merging 1 files, 632 bytes from disk\n",
      "16/01/31 06:01:42 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/01/31 06:01:42 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/31 06:01:42 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 622 bytes\n",
      "16/01/31 06:01:42 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/31 06:01:42 INFO streaming.PipeMapRed: PipeMapRed exec [/home/ubuntu/HW3/./3_2_reducer.py]\n",
      "16/01/31 06:01:42 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 06:01:42 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 06:01:42 INFO streaming.PipeMapRed: Records R/W=42/1\n",
      "16/01/31 06:01:42 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/31 06:01:42 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/31 06:01:42 INFO mapred.Task: Task:attempt_local1108734189_0001_r_000002_0 is done. And is in the process of committing\n",
      "16/01/31 06:01:42 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/31 06:01:42 INFO mapred.Task: Task attempt_local1108734189_0001_r_000002_0 is allowed to commit now\n",
      "16/01/31 06:01:42 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1108734189_0001_r_000002_0' to hdfs://localhost:54310/hw3/3_2_combiner_out/_temporary/0/task_local1108734189_0001_r_000002\n",
      "16/01/31 06:01:42 INFO mapred.LocalJobRunner: Records R/W=42/1 > reduce\n",
      "16/01/31 06:01:42 INFO mapred.Task: Task 'attempt_local1108734189_0001_r_000002_0' done.\n",
      "16/01/31 06:01:42 INFO mapred.LocalJobRunner: Finishing task: attempt_local1108734189_0001_r_000002_0\n",
      "16/01/31 06:01:42 INFO mapred.LocalJobRunner: Starting task: attempt_local1108734189_0001_r_000003_0\n",
      "16/01/31 06:01:42 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/31 06:01:42 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "16/01/31 06:01:42 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@1af70aa3\n",
      "16/01/31 06:01:42 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=333971456, maxSingleShuffleLimit=83492864, mergeThreshold=220421168, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/01/31 06:01:42 INFO reduce.EventFetcher: attempt_local1108734189_0001_r_000003_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/01/31 06:01:42 INFO reduce.LocalFetcher: localfetcher#4 about to shuffle output of map attempt_local1108734189_0001_m_000000_0 decomp: 649 len: 653 to MEMORY\n",
      "16/01/31 06:01:42 INFO reduce.InMemoryMapOutput: Read 649 bytes from map-output for attempt_local1108734189_0001_m_000000_0\n",
      "16/01/31 06:01:42 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 649, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->649\n",
      "16/01/31 06:01:42 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/01/31 06:01:42 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/31 06:01:42 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/01/31 06:01:42 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/31 06:01:42 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 635 bytes\n",
      "16/01/31 06:01:42 INFO reduce.MergeManagerImpl: Merged 1 segments, 649 bytes to disk to satisfy reduce memory limit\n",
      "16/01/31 06:01:42 INFO reduce.MergeManagerImpl: Merging 1 files, 653 bytes from disk\n",
      "16/01/31 06:01:42 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/01/31 06:01:42 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/31 06:01:42 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 635 bytes\n",
      "16/01/31 06:01:42 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/31 06:01:42 INFO streaming.PipeMapRed: PipeMapRed exec [/home/ubuntu/HW3/./3_2_reducer.py]\n",
      "16/01/31 06:01:42 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 06:01:42 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 06:01:42 INFO streaming.PipeMapRed: Records R/W=45/1\n",
      "16/01/31 06:01:42 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/31 06:01:42 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/31 06:01:42 INFO mapred.Task: Task:attempt_local1108734189_0001_r_000003_0 is done. And is in the process of committing\n",
      "16/01/31 06:01:42 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/31 06:01:42 INFO mapred.Task: Task attempt_local1108734189_0001_r_000003_0 is allowed to commit now\n",
      "16/01/31 06:01:42 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1108734189_0001_r_000003_0' to hdfs://localhost:54310/hw3/3_2_combiner_out/_temporary/0/task_local1108734189_0001_r_000003\n",
      "16/01/31 06:01:42 INFO mapred.LocalJobRunner: Records R/W=45/1 > reduce\n",
      "16/01/31 06:01:42 INFO mapred.Task: Task 'attempt_local1108734189_0001_r_000003_0' done.\n",
      "16/01/31 06:01:42 INFO mapred.LocalJobRunner: Finishing task: attempt_local1108734189_0001_r_000003_0\n",
      "16/01/31 06:01:42 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/01/31 06:01:43 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/01/31 06:01:43 INFO mapreduce.Job: Job job_local1108734189_0001 completed successfully\n",
      "16/01/31 06:01:43 INFO mapreduce.Job: Counters: 38\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=557617\n",
      "\t\tFILE: Number of bytes written=1954571\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=254532430\n",
      "\t\tHDFS: Number of bytes written=5534\n",
      "\t\tHDFS: Number of read operations=55\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=25\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=312913\n",
      "\t\tMap output records=980483\n",
      "\t\tMap output bytes=9272509\n",
      "\t\tMap output materialized bytes=2615\n",
      "\t\tInput split bytes=102\n",
      "\t\tCombine input records=980483\n",
      "\t\tCombine output records=181\n",
      "\t\tReduce input groups=181\n",
      "\t\tReduce shuffle bytes=2615\n",
      "\t\tReduce input records=181\n",
      "\t\tReduce output records=181\n",
      "\t\tSpilled Records=362\n",
      "\t\tShuffled Maps =4\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=4\n",
      "\t\tGC time elapsed (ms)=13\n",
      "\t\tTotal committed heap usage (bytes)=1664614400\n",
      "\tCalls\n",
      "\t\tcombiner_calls=4\n",
      "\t\tmapper_calls=1\n",
      "\t\treducer_calls=4\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=50906486\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=2229\n",
      "16/01/31 06:01:43 INFO streaming.StreamJob: Output directory: /hw3/3_2_combiner_out\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r /hw3/3_2_combiner_out\n",
    "\n",
    "!hadoop jar /usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar \\\n",
    "-D mapred.map.tasks=4 \\\n",
    "-D mapred.reduce.tasks=4 \\\n",
    "-mapper 3_2_issue_mapper.py -reducer 3_2_reducer.py -combiner 3_2_combiner.py -input /hw3/Consumer_Complaints.csv -output /hw3/3_2_combiner_out \\\n",
    "#-numReduceTasks 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the job is completed, the mapper is called once, the combiner is called 4 times, and the reducer is called 4 times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using a single reducer: What are the top 50 most frequent terms in your word count analysis? Present the top 50 terms and their frequency and their relative frequency. If there are ties please sort the tokens in alphanumeric/string order. Present bottom 10 tokens (least frequent items). **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting 3_2_top_mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile 3_2_top_mapper.py\n",
    "#!/usr/bin/python\n",
    "#HW 3.2\n",
    "\n",
    "import sys\n",
    "import re\n",
    "\n",
    "WORD_RE = re.compile(r\"[\\w']+\") #Compile regex to easily parse complete words\n",
    "\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    fields = line.split(',')\n",
    "    # extract issue column\n",
    "    issue=fields[3].strip()\n",
    "    words=re.findall(WORD_RE, issue) #create list of words\n",
    "    \n",
    "    # count the words\n",
    "    for word in words:\n",
    "        print '%s\\t%s' % (word, 1) \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting 3_2_top_reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile 3_2_top_reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "word_count = {}\n",
    "total_count = 0\n",
    "\n",
    "sys.stderr.write(\"reporter:counter:Calls,reducer_calls,1\\n\")\n",
    "current_key = None\n",
    "current_count = 0\n",
    "key = None\n",
    "\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    key, count = line.split(\"\\t\", 1)\n",
    "    count = int(count)\n",
    "    \n",
    "    if current_key == key:\n",
    "        current_count += count\n",
    "    else:\n",
    "        if current_key:\n",
    "            # write result to STDOUT\n",
    "            word_count[current_key] = current_count\n",
    "        current_count = count\n",
    "        current_key = key\n",
    "\n",
    "# output the last word\n",
    "if current_key == key:\n",
    "    word_count[current_key] = current_count\n",
    "\n",
    "total_count = sum(word_count.values())\n",
    "#word_count = sorted(word_count.items(),key=lambda t: t[1], reverse=True)\n",
    "\n",
    "for key, value in word_count.items():\n",
    "    print \"%s\\t%.0f\\t%.4f\" % (key, value, value*1.0/total_count)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/03 01:44:43 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/03 01:44:44 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /hw3/3_2_top_out\n",
      "16/02/03 01:44:45 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/03 01:44:46 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/02/03 01:44:46 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/02/03 01:44:46 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/02/03 01:44:46 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/02/03 01:44:46 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/02/03 01:44:46 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1892644608_0001\n",
      "16/02/03 01:44:46 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/02/03 01:44:46 INFO mapreduce.Job: Running job: job_local1892644608_0001\n",
      "16/02/03 01:44:46 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/02/03 01:44:46 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/02/03 01:44:46 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/02/03 01:44:46 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/02/03 01:44:46 INFO mapred.LocalJobRunner: Starting task: attempt_local1892644608_0001_m_000000_0\n",
      "16/02/03 01:44:46 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/02/03 01:44:46 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "16/02/03 01:44:46 INFO mapred.MapTask: Processing split: hdfs://localhost:54310/hw3/Consumer_Complaints.csv:0+50906486\n",
      "16/02/03 01:44:46 INFO mapred.MapTask: numReduceTasks: 1\n",
      "16/02/03 01:44:46 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/02/03 01:44:46 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/02/03 01:44:46 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/02/03 01:44:46 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/02/03 01:44:46 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/02/03 01:44:46 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/02/03 01:44:46 INFO streaming.PipeMapRed: PipeMapRed exec [/home/ubuntu/HW3/./3_2_top_mapper.py]\n",
      "16/02/03 01:44:46 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/02/03 01:44:46 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/02/03 01:44:46 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/02/03 01:44:46 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/02/03 01:44:46 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/02/03 01:44:46 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/02/03 01:44:46 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/02/03 01:44:46 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/02/03 01:44:46 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/02/03 01:44:46 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/02/03 01:44:46 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/02/03 01:44:46 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/02/03 01:44:46 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/03 01:44:46 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/03 01:44:46 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/03 01:44:46 INFO streaming.PipeMapRed: Records R/W=779/1\n",
      "16/02/03 01:44:46 INFO streaming.PipeMapRed: R/W/S=1000/376/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/03 01:44:47 INFO streaming.PipeMapRed: R/W/S=10000/32409/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/03 01:44:47 INFO mapreduce.Job: Job job_local1892644608_0001 running in uber mode : false\n",
      "16/02/03 01:44:47 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/02/03 01:44:47 INFO streaming.PipeMapRed: R/W/S=100000/350620/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/03 01:44:48 INFO streaming.PipeMapRed: R/W/S=200000/673338/0 in:200000=200000/1 [rec/s] out:673338=673338/1 [rec/s]\n",
      "16/02/03 01:44:48 INFO streaming.PipeMapRed: R/W/S=300000/947128/0 in:150000=300000/2 [rec/s] out:473564=947128/2 [rec/s]\n",
      "16/02/03 01:44:48 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/03 01:44:48 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/03 01:44:48 INFO mapred.LocalJobRunner: \n",
      "16/02/03 01:44:48 INFO mapred.MapTask: Starting flush of map output\n",
      "16/02/03 01:44:48 INFO mapred.MapTask: Spilling map output\n",
      "16/02/03 01:44:48 INFO mapred.MapTask: bufstart = 0; bufend = 9272509; bufvoid = 104857600\n",
      "16/02/03 01:44:48 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 22292468(89169872); length = 3921929/6553600\n",
      "16/02/03 01:44:49 INFO mapred.MapTask: Finished spill 0\n",
      "16/02/03 01:44:49 INFO mapred.Task: Task:attempt_local1892644608_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/02/03 01:44:49 INFO mapred.LocalJobRunner: Records R/W=779/1\n",
      "16/02/03 01:44:49 INFO mapred.Task: Task 'attempt_local1892644608_0001_m_000000_0' done.\n",
      "16/02/03 01:44:49 INFO mapred.LocalJobRunner: Finishing task: attempt_local1892644608_0001_m_000000_0\n",
      "16/02/03 01:44:49 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/02/03 01:44:49 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/02/03 01:44:49 INFO mapred.LocalJobRunner: Starting task: attempt_local1892644608_0001_r_000000_0\n",
      "16/02/03 01:44:49 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/02/03 01:44:49 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "16/02/03 01:44:49 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@4bc2ebc7\n",
      "16/02/03 01:44:49 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=333971456, maxSingleShuffleLimit=83492864, mergeThreshold=220421168, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/03 01:44:49 INFO reduce.EventFetcher: attempt_local1892644608_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/03 01:44:49 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1892644608_0001_m_000000_0 decomp: 11233477 len: 11233481 to MEMORY\n",
      "16/02/03 01:44:50 INFO reduce.InMemoryMapOutput: Read 11233477 bytes from map-output for attempt_local1892644608_0001_m_000000_0\n",
      "16/02/03 01:44:50 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 11233477, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->11233477\n",
      "16/02/03 01:44:50 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/03 01:44:50 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/03 01:44:50 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/03 01:44:50 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/03 01:44:50 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 11233471 bytes\n",
      "16/02/03 01:44:50 INFO reduce.MergeManagerImpl: Merged 1 segments, 11233477 bytes to disk to satisfy reduce memory limit\n",
      "16/02/03 01:44:50 INFO reduce.MergeManagerImpl: Merging 1 files, 11233481 bytes from disk\n",
      "16/02/03 01:44:50 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/03 01:44:50 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/03 01:44:50 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 11233471 bytes\n",
      "16/02/03 01:44:50 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/03 01:44:50 INFO streaming.PipeMapRed: PipeMapRed exec [/home/ubuntu/HW3/./3_2_top_reducer.py]\n",
      "16/02/03 01:44:50 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/02/03 01:44:50 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/02/03 01:44:50 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/03 01:44:50 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/03 01:44:50 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/03 01:44:50 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/03 01:44:50 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/02/03 01:44:50 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/03 01:44:50 INFO streaming.PipeMapRed: R/W/S=100000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/03 01:44:50 INFO streaming.PipeMapRed: R/W/S=200000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/03 01:44:50 INFO streaming.PipeMapRed: R/W/S=300000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/03 01:44:51 INFO streaming.PipeMapRed: R/W/S=400000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/03 01:44:51 INFO streaming.PipeMapRed: R/W/S=500000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/03 01:44:51 INFO streaming.PipeMapRed: R/W/S=600000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/03 01:44:51 INFO streaming.PipeMapRed: R/W/S=700000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/03 01:44:51 INFO streaming.PipeMapRed: R/W/S=800000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/03 01:44:51 INFO streaming.PipeMapRed: R/W/S=900000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/03 01:44:51 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/03 01:44:51 INFO streaming.PipeMapRed: Records R/W=980483/1\n",
      "16/02/03 01:44:51 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/03 01:44:51 INFO mapred.Task: Task:attempt_local1892644608_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/02/03 01:44:51 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/03 01:44:51 INFO mapred.Task: Task attempt_local1892644608_0001_r_000000_0 is allowed to commit now\n",
      "16/02/03 01:44:51 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1892644608_0001_r_000000_0' to hdfs://localhost:54310/hw3/3_2_top_out/_temporary/0/task_local1892644608_0001_r_000000\n",
      "16/02/03 01:44:51 INFO mapred.LocalJobRunner: Records R/W=980483/1 > reduce\n",
      "16/02/03 01:44:51 INFO mapred.Task: Task 'attempt_local1892644608_0001_r_000000_0' done.\n",
      "16/02/03 01:44:51 INFO mapred.LocalJobRunner: Finishing task: attempt_local1892644608_0001_r_000000_0\n",
      "16/02/03 01:44:51 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/02/03 01:44:52 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/02/03 01:44:52 INFO mapreduce.Job: Job job_local1892644608_0001 completed successfully\n",
      "16/02/03 01:44:52 INFO mapreduce.Job: Counters: 36\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=22679130\n",
      "\t\tFILE: Number of bytes written=34472339\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=101812972\n",
      "\t\tHDFS: Number of bytes written=3496\n",
      "\t\tHDFS: Number of read operations=13\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=312913\n",
      "\t\tMap output records=980483\n",
      "\t\tMap output bytes=9272509\n",
      "\t\tMap output materialized bytes=11233481\n",
      "\t\tInput split bytes=102\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=181\n",
      "\t\tReduce shuffle bytes=11233481\n",
      "\t\tReduce input records=980483\n",
      "\t\tReduce output records=181\n",
      "\t\tSpilled Records=1960966\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=8\n",
      "\t\tTotal committed heap usage (bytes)=597688320\n",
      "\tCalls\n",
      "\t\treducer_calls=1\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=50906486\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=3496\n",
      "16/02/03 01:44:52 INFO streaming.StreamJob: Output directory: /hw3/3_2_top_out\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r /hw3/3_2_top_out\n",
    "\n",
    "!hadoop jar /usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar \\\n",
    "-mapper 3_2_top_mapper.py -reducer 3_2_top_reducer.py -input /hw3/Consumer_Complaints.csv -output /hw3/3_2_top_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/03 01:44:58 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Arbitration\t168\t0.0002\n",
      "being\t5663\t0.0058\n",
      "caused\t5663\t0.0058\n",
      "report\t34903\t0.0356\n",
      "attempts\t11848\t0.0121\n",
      "Unable\t4357\t0.0044\n",
      "issues\t538\t0.0005\n",
      "unable\t3821\t0.0039\n",
      "Workout\t350\t0.0004\n",
      "credited\t92\t0.0001\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /hw3/3_2_top_out/part-00000 |head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/03 01:45:53 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/03 01:45:53 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /hw3/3_2_top_out_sorted\n",
      "16/02/03 01:45:54 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/03 01:45:55 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/02/03 01:45:55 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/02/03 01:45:55 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/02/03 01:45:55 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/02/03 01:45:55 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/02/03 01:45:56 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local361650863_0001\n",
      "16/02/03 01:45:56 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/02/03 01:45:56 INFO mapreduce.Job: Running job: job_local361650863_0001\n",
      "16/02/03 01:45:56 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/02/03 01:45:56 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/02/03 01:45:56 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/02/03 01:45:56 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/02/03 01:45:56 INFO mapred.LocalJobRunner: Starting task: attempt_local361650863_0001_m_000000_0\n",
      "16/02/03 01:45:56 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/02/03 01:45:56 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "16/02/03 01:45:56 INFO mapred.MapTask: Processing split: hdfs://localhost:54310/hw3/3_2_top_out/part-00000:0+3496\n",
      "16/02/03 01:45:56 INFO mapred.MapTask: numReduceTasks: 1\n",
      "16/02/03 01:45:56 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/02/03 01:45:56 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/02/03 01:45:56 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/02/03 01:45:56 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/02/03 01:45:56 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/02/03 01:45:56 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/02/03 01:45:56 INFO streaming.PipeMapRed: PipeMapRed exec [/home/ubuntu/HW3/./3_4_mapper_sort.py]\n",
      "16/02/03 01:45:56 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/02/03 01:45:56 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/02/03 01:45:56 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/02/03 01:45:56 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/02/03 01:45:56 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/02/03 01:45:56 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/02/03 01:45:56 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/02/03 01:45:56 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/02/03 01:45:56 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/02/03 01:45:56 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/02/03 01:45:56 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/02/03 01:45:56 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/02/03 01:45:56 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/03 01:45:56 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/03 01:45:56 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/03 01:45:56 INFO streaming.PipeMapRed: Records R/W=181/1\n",
      "16/02/03 01:45:56 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/03 01:45:56 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/03 01:45:56 INFO mapred.LocalJobRunner: \n",
      "16/02/03 01:45:56 INFO mapred.MapTask: Starting flush of map output\n",
      "16/02/03 01:45:56 INFO mapred.MapTask: Spilling map output\n",
      "16/02/03 01:45:56 INFO mapred.MapTask: bufstart = 0; bufend = 4039; bufvoid = 104857600\n",
      "16/02/03 01:45:56 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26212952(104851808); length = 1445/6553600\n",
      "16/02/03 01:45:56 INFO mapred.MapTask: Finished spill 0\n",
      "16/02/03 01:45:56 INFO mapred.Task: Task:attempt_local361650863_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/02/03 01:45:56 INFO mapred.LocalJobRunner: Records R/W=181/1\n",
      "16/02/03 01:45:56 INFO mapred.Task: Task 'attempt_local361650863_0001_m_000000_0' done.\n",
      "16/02/03 01:45:56 INFO mapred.LocalJobRunner: Finishing task: attempt_local361650863_0001_m_000000_0\n",
      "16/02/03 01:45:56 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/02/03 01:45:56 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/02/03 01:45:56 INFO mapred.LocalJobRunner: Starting task: attempt_local361650863_0001_r_000000_0\n",
      "16/02/03 01:45:56 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/02/03 01:45:56 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "16/02/03 01:45:56 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@6efffd35\n",
      "16/02/03 01:45:56 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=333971456, maxSingleShuffleLimit=83492864, mergeThreshold=220421168, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/03 01:45:56 INFO reduce.EventFetcher: attempt_local361650863_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/03 01:45:56 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local361650863_0001_m_000000_0 decomp: 4765 len: 4769 to MEMORY\n",
      "16/02/03 01:45:56 INFO reduce.InMemoryMapOutput: Read 4765 bytes from map-output for attempt_local361650863_0001_m_000000_0\n",
      "16/02/03 01:45:56 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 4765, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->4765\n",
      "16/02/03 01:45:56 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/03 01:45:56 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/03 01:45:56 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/03 01:45:56 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/03 01:45:56 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 4744 bytes\n",
      "16/02/03 01:45:56 INFO reduce.MergeManagerImpl: Merged 1 segments, 4765 bytes to disk to satisfy reduce memory limit\n",
      "16/02/03 01:45:56 INFO reduce.MergeManagerImpl: Merging 1 files, 4769 bytes from disk\n",
      "16/02/03 01:45:56 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/03 01:45:56 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/03 01:45:56 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 4744 bytes\n",
      "16/02/03 01:45:56 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/03 01:45:56 INFO streaming.PipeMapRed: PipeMapRed exec [/home/ubuntu/HW3/./3_4_reducer_sort.py]\n",
      "16/02/03 01:45:56 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/02/03 01:45:56 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/02/03 01:45:56 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/03 01:45:56 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/03 01:45:56 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/03 01:45:56 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/03 01:45:56 INFO streaming.PipeMapRed: Records R/W=362/1\n",
      "16/02/03 01:45:56 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/03 01:45:56 INFO mapred.Task: Task:attempt_local361650863_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/02/03 01:45:56 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/03 01:45:56 INFO mapred.Task: Task attempt_local361650863_0001_r_000000_0 is allowed to commit now\n",
      "16/02/03 01:45:56 INFO output.FileOutputCommitter: Saved output of task 'attempt_local361650863_0001_r_000000_0' to hdfs://localhost:54310/hw3/3_2_top_out_sorted/_temporary/0/task_local361650863_0001_r_000000\n",
      "16/02/03 01:45:56 INFO mapred.LocalJobRunner: Records R/W=362/1 > reduce\n",
      "16/02/03 01:45:56 INFO mapred.Task: Task 'attempt_local361650863_0001_r_000000_0' done.\n",
      "16/02/03 01:45:56 INFO mapred.LocalJobRunner: Finishing task: attempt_local361650863_0001_r_000000_0\n",
      "16/02/03 01:45:56 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/02/03 01:45:57 INFO mapreduce.Job: Job job_local361650863_0001 running in uber mode : false\n",
      "16/02/03 01:45:57 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/02/03 01:45:57 INFO mapreduce.Job: Job job_local361650863_0001 completed successfully\n",
      "16/02/03 01:45:57 INFO mapreduce.Job: Counters: 35\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=221700\n",
      "\t\tFILE: Number of bytes written=785591\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=6992\n",
      "\t\tHDFS: Number of bytes written=1148\n",
      "\t\tHDFS: Number of read operations=13\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=181\n",
      "\t\tMap output records=362\n",
      "\t\tMap output bytes=4039\n",
      "\t\tMap output materialized bytes=4769\n",
      "\t\tInput split bytes=101\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=182\n",
      "\t\tReduce shuffle bytes=4769\n",
      "\t\tReduce input records=362\n",
      "\t\tReduce output records=100\n",
      "\t\tSpilled Records=724\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=0\n",
      "\t\tTotal committed heap usage (bytes)=529530880\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=3496\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=1148\n",
      "16/02/03 01:45:57 INFO streaming.StreamJob: Output directory: /hw3/3_2_top_out_sorted\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r /hw3/3_2_top_out_sorted\n",
    "\n",
    "!hadoop jar /usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar \\\n",
    "-D stream.map.output.field.separator=\"\\t\" \\\n",
    "-D mapreduce.job.output.key.comparator.class=\\\n",
    "org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    " -D mapreduce.partition.keycomparator.options=\"-k2,2nr -k1,1\" \\\n",
    "-mapper 3_4_mapper_sort.py -reducer 3_4_reducer_sort.py \\\n",
    "-input /hw3/3_2_top_out -output /hw3/3_2_top_out_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/03 01:47:03 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Loan\t107254\t0.1094\t\n",
      "\t\n",
      "modification\t70487\t0.0719\t\n",
      "\t\n",
      "credit\t40483\t0.0413\t\n",
      "\t\n",
      "servicing\t36767\t0.0375\t\n",
      "\t\n",
      "report\t34903\t0.0356\t\n",
      "\t\n",
      "Incorrect\t29133\t0.0297\t\n",
      "\t\n",
      "information\t29069\t0.0296\t\n",
      "\t\n",
      "on\t29069\t0.0296\t\n",
      "\t\n",
      "or\t22533\t0.0230\t\n",
      "\t\n",
      "debt\t17966\t0.0183\t\n",
      "\t\n",
      "Account\t16555\t0.0169\t\n",
      "\t\n",
      "and\t16448\t0.0168\t\n",
      "\t\n",
      "opening\t16205\t0.0165\t\n",
      "\t\n",
      "Credit\t14768\t0.0151\t\n",
      "\t\n",
      "club\t12545\t0.0128\t\n",
      "\t\n",
      "health\t12545\t0.0128\t\n",
      "\t\n",
      "loan\t12376\t0.0126\t\n",
      "\t\n",
      "not\t12353\t0.0126\t\n",
      "\t\n",
      "Cont'd\t11848\t0.0121\t\n",
      "\t\n",
      "attempts\t11848\t0.0121\t\n",
      "\t\n",
      "collect\t11848\t0.0121\t\n",
      "\t\n",
      "owed\t11848\t0.0121\t\n",
      "\t\n",
      "of\t10885\t0.0111\t\n",
      "\t\n",
      "my\t10731\t0.0109\t\n",
      "\t\n",
      "Deposits\t10555\t0.0108\t\n",
      "\t\n",
      "withdrawals\t10555\t0.0108\t\n",
      "\t\n",
      "Problems\t9484\t0.0097\t\n",
      "\t\n",
      "Application\t8868\t0.0090\t\n",
      "\t\n",
      "to\t8401\t0.0086\t\n",
      "\t\n",
      "Billing\t8158\t0.0083\t\n",
      "\t\n",
      "Other\t7886\t0.0080\t\n",
      "\t\n",
      "disputes\t6938\t0.0071\t\n",
      "\t\n",
      "Communication\t6920\t0.0071\t\n",
      "\t\n",
      "tactics\t6920\t0.0071\t\n",
      "\t\n",
      "reporting\t6559\t0.0067\t\n",
      "\t\n",
      "lease\t6337\t0.0065\t\n",
      "\t\n",
      "the\t6248\t0.0064\t\n",
      "\t\n",
      "being\t5663\t0.0058\t\n",
      "\t\n",
      "by\t5663\t0.0058\t\n",
      "\t\n",
      "caused\t5663\t0.0058\t\n",
      "\t\n",
      "funds\t5663\t0.0058\t\n",
      "\t\n",
      "low\t5663\t0.0058\t\n",
      "\t\n",
      "process\t5505\t0.0056\t\n",
      "\t\n",
      "Disclosure\t5214\t0.0053\t\n",
      "\t\n",
      "verification\t5214\t0.0053\t\n",
      "\t\n",
      "Managing\t5006\t0.0051\t\n",
      "\t\n",
      "company's\t4858\t0.0050\t\n",
      "\t\n",
      "investigation\t4858\t0.0050\t\n",
      "\t\n",
      "card\t4405\t0.0045\t\n",
      "\t\n",
      "Unable\t4357\t0.0044\t\n",
      "\t\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /hw3/3_2_top_out_sorted/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW3.3. Shopping Cart Analysis ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/01 22:49:14 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -put ProductPurchaseData.txt /hw3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting 3_3_mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile 3_3_mapper.py\n",
    "#!/usr/bin/python\n",
    "#HW 3.3\n",
    "\n",
    "import sys\n",
    "\n",
    "max_basket = 0\n",
    "total_count = 0\n",
    "\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    products = line.split(\" \")\n",
    "    if len(products) > max_basket:\n",
    "        max_basket = len(products)\n",
    "    \n",
    "    # count the product\n",
    "    for item in products:\n",
    "        print item+\"\\t1\"\n",
    "        total_count += 1\n",
    "\n",
    "print \"max_basket\\t\"+str(max_basket)\n",
    "print \"total_count\\t\"+str(total_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting 3_3_reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile 3_3_reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "from operator import itemgetter\n",
    "\n",
    "total_count = 0\n",
    "max_basket = 0\n",
    "topN = 0\n",
    "products = {}\n",
    "\n",
    "sys.stderr.write(\"reporter:counter:Calls,reducer_calls,1\\n\")\n",
    "current_key = None\n",
    "current_count = 0\n",
    "key = None\n",
    "\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    key, count = line.split(\"\\t\", 1)\n",
    "    count = int(count)\n",
    "    \n",
    "    if key == \"total_count\":\n",
    "        total_count += count\n",
    "    elif key == \"max_basket\":\n",
    "        max_basket = max(max_basket, count)\n",
    "    else:\n",
    "\n",
    "        if current_key == key:\n",
    "            current_count += count\n",
    "        else:\n",
    "            if current_key:\n",
    "                # write result to STDOUT\n",
    "                products[current_key] = current_count\n",
    "            current_count = count\n",
    "            current_key = key\n",
    "\n",
    "# output the last word\n",
    "if current_key == key:\n",
    "    products[current_key] = current_count\n",
    "#print \"The number of unique items: \", len(products)\n",
    "#print \"The largest basket size: \", max_basket\n",
    "\n",
    "#products = sorted(products.items(),key=lambda t: t[1], reverse=True)\n",
    "\n",
    "#print \"Product\\tFrequency\\tRelative frequency\"    \n",
    "for key, value in products.items():\n",
    "    print \"%s\\t%.0f\\t%.4f\" % (key, value, value*1.0/total_count)\n",
    "print \"max_basket\\t\"+str(max_basket)\n",
    "print \"unique_items\\t\"+str(len(products))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/03 02:11:58 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/03 02:11:59 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /hw3/3_3_out\n",
      "16/02/03 02:12:00 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/03 02:12:00 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/02/03 02:12:00 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/02/03 02:12:00 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/02/03 02:12:00 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/02/03 02:12:01 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/02/03 02:12:01 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1403091349_0001\n",
      "16/02/03 02:12:01 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/02/03 02:12:01 INFO mapreduce.Job: Running job: job_local1403091349_0001\n",
      "16/02/03 02:12:01 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/02/03 02:12:01 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/02/03 02:12:01 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/02/03 02:12:01 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/02/03 02:12:01 INFO mapred.LocalJobRunner: Starting task: attempt_local1403091349_0001_m_000000_0\n",
      "16/02/03 02:12:01 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/02/03 02:12:01 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "16/02/03 02:12:01 INFO mapred.MapTask: Processing split: hdfs://localhost:54310/hw3/ProductPurchaseData.txt:0+3458517\n",
      "16/02/03 02:12:01 INFO mapred.MapTask: numReduceTasks: 1\n",
      "16/02/03 02:12:01 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/02/03 02:12:01 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/02/03 02:12:01 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/02/03 02:12:01 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/02/03 02:12:01 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/02/03 02:12:01 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/02/03 02:12:01 INFO streaming.PipeMapRed: PipeMapRed exec [/home/ubuntu/HW3/./3_3_mapper.py]\n",
      "16/02/03 02:12:01 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/02/03 02:12:01 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/02/03 02:12:01 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/02/03 02:12:01 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/02/03 02:12:01 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/02/03 02:12:01 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/02/03 02:12:01 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/02/03 02:12:01 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/02/03 02:12:01 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/02/03 02:12:01 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/02/03 02:12:01 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/02/03 02:12:01 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/02/03 02:12:01 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/03 02:12:01 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/03 02:12:01 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/03 02:12:01 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/03 02:12:01 INFO streaming.PipeMapRed: Records R/W=1216/1\n",
      "16/02/03 02:12:01 INFO streaming.PipeMapRed: R/W/S=10000/121390/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/03 02:12:02 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/03 02:12:02 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/03 02:12:02 INFO mapred.LocalJobRunner: \n",
      "16/02/03 02:12:02 INFO mapred.MapTask: Starting flush of map output\n",
      "16/02/03 02:12:02 INFO mapred.MapTask: Spilling map output\n",
      "16/02/03 02:12:02 INFO mapred.MapTask: bufstart = 0; bufend = 4569923; bufvoid = 104857600\n",
      "16/02/03 02:12:02 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 24691096(98764384); length = 1523301/6553600\n",
      "16/02/03 02:12:02 INFO mapreduce.Job: Job job_local1403091349_0001 running in uber mode : false\n",
      "16/02/03 02:12:02 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/02/03 02:12:03 INFO mapred.MapTask: Finished spill 0\n",
      "16/02/03 02:12:03 INFO mapred.Task: Task:attempt_local1403091349_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/02/03 02:12:03 INFO mapred.LocalJobRunner: Records R/W=1216/1\n",
      "16/02/03 02:12:03 INFO mapred.Task: Task 'attempt_local1403091349_0001_m_000000_0' done.\n",
      "16/02/03 02:12:03 INFO mapred.LocalJobRunner: Finishing task: attempt_local1403091349_0001_m_000000_0\n",
      "16/02/03 02:12:03 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/02/03 02:12:03 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/02/03 02:12:03 INFO mapred.LocalJobRunner: Starting task: attempt_local1403091349_0001_r_000000_0\n",
      "16/02/03 02:12:03 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/02/03 02:12:03 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "16/02/03 02:12:03 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@617df4d3\n",
      "16/02/03 02:12:03 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=333971456, maxSingleShuffleLimit=83492864, mergeThreshold=220421168, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/03 02:12:03 INFO reduce.EventFetcher: attempt_local1403091349_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/03 02:12:03 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1403091349_0001_m_000000_0 decomp: 5331577 len: 5331581 to MEMORY\n",
      "16/02/03 02:12:03 INFO reduce.InMemoryMapOutput: Read 5331577 bytes from map-output for attempt_local1403091349_0001_m_000000_0\n",
      "16/02/03 02:12:03 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 5331577, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->5331577\n",
      "16/02/03 02:12:03 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/03 02:12:03 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/03 02:12:03 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/03 02:12:03 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/03 02:12:03 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 5331564 bytes\n",
      "16/02/03 02:12:03 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/02/03 02:12:03 INFO reduce.MergeManagerImpl: Merged 1 segments, 5331577 bytes to disk to satisfy reduce memory limit\n",
      "16/02/03 02:12:03 INFO reduce.MergeManagerImpl: Merging 1 files, 5331581 bytes from disk\n",
      "16/02/03 02:12:03 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/03 02:12:03 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/03 02:12:03 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 5331564 bytes\n",
      "16/02/03 02:12:03 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/03 02:12:03 INFO streaming.PipeMapRed: PipeMapRed exec [/home/ubuntu/HW3/./3_3_reducer.py]\n",
      "16/02/03 02:12:03 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/02/03 02:12:03 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/02/03 02:12:03 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/03 02:12:03 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/03 02:12:03 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/03 02:12:03 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/03 02:12:03 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/03 02:12:03 INFO streaming.PipeMapRed: R/W/S=100000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/03 02:12:03 INFO streaming.PipeMapRed: R/W/S=200000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/03 02:12:03 INFO streaming.PipeMapRed: R/W/S=300000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/03 02:12:04 INFO streaming.PipeMapRed: Records R/W=380826/1\n",
      "16/02/03 02:12:04 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/03 02:12:04 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/03 02:12:04 INFO mapred.Task: Task:attempt_local1403091349_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/02/03 02:12:04 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/03 02:12:04 INFO mapred.Task: Task attempt_local1403091349_0001_r_000000_0 is allowed to commit now\n",
      "16/02/03 02:12:04 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1403091349_0001_r_000000_0' to hdfs://localhost:54310/hw3/3_3_out/_temporary/0/task_local1403091349_0001_r_000000\n",
      "16/02/03 02:12:04 INFO mapred.LocalJobRunner: Records R/W=380826/1 > reduce\n",
      "16/02/03 02:12:04 INFO mapred.Task: Task 'attempt_local1403091349_0001_r_000000_0' done.\n",
      "16/02/03 02:12:04 INFO mapred.LocalJobRunner: Finishing task: attempt_local1403091349_0001_r_000000_0\n",
      "16/02/03 02:12:04 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/02/03 02:12:04 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/02/03 02:12:04 INFO mapreduce.Job: Job job_local1403091349_0001 completed successfully\n",
      "16/02/03 02:12:04 INFO mapreduce.Job: Counters: 36\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=10875328\n",
      "\t\tFILE: Number of bytes written=16768983\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=6917034\n",
      "\t\tHDFS: Number of bytes written=230817\n",
      "\t\tHDFS: Number of read operations=13\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=31101\n",
      "\t\tMap output records=380826\n",
      "\t\tMap output bytes=4569923\n",
      "\t\tMap output materialized bytes=5331581\n",
      "\t\tInput split bytes=102\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=12594\n",
      "\t\tReduce shuffle bytes=5331581\n",
      "\t\tReduce input records=380826\n",
      "\t\tReduce output records=12593\n",
      "\t\tSpilled Records=761652\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=28\n",
      "\t\tTotal committed heap usage (bytes)=713031680\n",
      "\tCalls\n",
      "\t\treducer_calls=1\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=3458517\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=230817\n",
      "16/02/03 02:12:04 INFO streaming.StreamJob: Output directory: /hw3/3_3_out\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r /hw3/3_3_out\n",
    "\n",
    "!hadoop jar /usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar \\\n",
    "-D stream.map.output.field.separator=\"\\t\" \\\n",
    "-D mapreduce.job.output.key.comparator.class=\\\n",
    "org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "-D mapreduce.partition.keycomparator.options=\"-k1,1\" \\\n",
    "-mapper 3_3_mapper.py -reducer 3_3_reducer.py \\\n",
    "-input /hw3/ProductPurchaseData.txt -output /hw3/3_3_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/03 02:12:44 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "FRO33948\t9\t0.0000\n",
      "GRO18058\t1\t0.0000\n",
      "GRO55789\t1\t0.0000\n",
      "GRO73026\t7\t0.0000\n",
      "FRO75698\t1\t0.0000\n",
      "GRO43334\t1\t0.0000\n",
      "SNA87642\t1\t0.0000\n",
      "SNA33045\t3\t0.0000\n",
      "max_basket\t37\n",
      "unique_items\t12591\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /hw3/3_3_out/part-00000 | tail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting 3_3_reducer_sort.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile 3_3_reducer_sort.py\n",
    "#!/usr/bin/python\n",
    "#HW 3.3\n",
    "\n",
    "import sys\n",
    "topN = 0\n",
    "max_basket = 0\n",
    "unique_items = 0\n",
    "\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    fields = line.split(\"\\t\", 1)\n",
    "\n",
    "    if fields[0] == \"max_basket\":\n",
    "        max_basket = int(fields[1])\n",
    "    elif fields[0] == \"unique_items\":\n",
    "        unique_items = int(fields[1])\n",
    "    elif topN < 50:\n",
    "        print line\n",
    "        topN += 1\n",
    "\n",
    "print \"The number of unique items: \", unique_items\n",
    "print \"The largest basket size: \", max_basket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/03 02:30:54 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "rm: `/hw3/3_3_out_sorted': No such file or directory\n",
      "16/02/03 02:30:55 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/03 02:30:56 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/02/03 02:30:56 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/02/03 02:30:56 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/02/03 02:30:56 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/02/03 02:30:56 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/02/03 02:30:56 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local850505182_0001\n",
      "16/02/03 02:30:57 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/02/03 02:30:57 INFO mapreduce.Job: Running job: job_local850505182_0001\n",
      "16/02/03 02:30:57 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/02/03 02:30:57 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/02/03 02:30:57 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/02/03 02:30:57 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/02/03 02:30:57 INFO mapred.LocalJobRunner: Starting task: attempt_local850505182_0001_m_000000_0\n",
      "16/02/03 02:30:57 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/02/03 02:30:57 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "16/02/03 02:30:57 INFO mapred.MapTask: Processing split: hdfs://localhost:54310/hw3/3_3_out/part-00000:0+230817\n",
      "16/02/03 02:30:57 INFO mapred.MapTask: numReduceTasks: 1\n",
      "16/02/03 02:30:57 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/02/03 02:30:57 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/02/03 02:30:57 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/02/03 02:30:57 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/02/03 02:30:57 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/02/03 02:30:57 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/02/03 02:30:57 INFO streaming.PipeMapRed: PipeMapRed exec [/home/ubuntu/HW3/./3_4_mapper_sort.py]\n",
      "16/02/03 02:30:57 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/02/03 02:30:57 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/02/03 02:30:57 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/02/03 02:30:57 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/02/03 02:30:57 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/02/03 02:30:57 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/02/03 02:30:57 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/02/03 02:30:57 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/02/03 02:30:57 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/02/03 02:30:57 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/02/03 02:30:57 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/02/03 02:30:57 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/02/03 02:30:57 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/03 02:30:57 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/03 02:30:57 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/03 02:30:57 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/03 02:30:57 INFO streaming.PipeMapRed: Records R/W=7151/1\n",
      "16/02/03 02:30:57 INFO streaming.PipeMapRed: R/W/S=10000/763/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/03 02:30:57 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/03 02:30:57 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/03 02:30:57 INFO mapred.LocalJobRunner: \n",
      "16/02/03 02:30:57 INFO mapred.MapTask: Starting flush of map output\n",
      "16/02/03 02:30:57 INFO mapred.MapTask: Spilling map output\n",
      "16/02/03 02:30:57 INFO mapred.MapTask: bufstart = 0; bufend = 268596; bufvoid = 104857600\n",
      "16/02/03 02:30:57 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26113656(104454624); length = 100741/6553600\n",
      "16/02/03 02:30:57 INFO mapred.MapTask: Finished spill 0\n",
      "16/02/03 02:30:57 INFO mapred.Task: Task:attempt_local850505182_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/02/03 02:30:57 INFO mapred.LocalJobRunner: Records R/W=7151/1\n",
      "16/02/03 02:30:57 INFO mapred.Task: Task 'attempt_local850505182_0001_m_000000_0' done.\n",
      "16/02/03 02:30:57 INFO mapred.LocalJobRunner: Finishing task: attempt_local850505182_0001_m_000000_0\n",
      "16/02/03 02:30:57 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/02/03 02:30:57 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/02/03 02:30:57 INFO mapred.LocalJobRunner: Starting task: attempt_local850505182_0001_r_000000_0\n",
      "16/02/03 02:30:57 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/02/03 02:30:57 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "16/02/03 02:30:57 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@2efe33c7\n",
      "16/02/03 02:30:57 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=333971456, maxSingleShuffleLimit=83492864, mergeThreshold=220421168, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/03 02:30:57 INFO reduce.EventFetcher: attempt_local850505182_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/03 02:30:57 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local850505182_0001_m_000000_0 decomp: 318970 len: 318974 to MEMORY\n",
      "16/02/03 02:30:57 INFO reduce.InMemoryMapOutput: Read 318970 bytes from map-output for attempt_local850505182_0001_m_000000_0\n",
      "16/02/03 02:30:57 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 318970, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->318970\n",
      "16/02/03 02:30:57 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/03 02:30:57 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/03 02:30:57 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/03 02:30:57 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/03 02:30:57 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 318949 bytes\n",
      "16/02/03 02:30:57 INFO reduce.MergeManagerImpl: Merged 1 segments, 318970 bytes to disk to satisfy reduce memory limit\n",
      "16/02/03 02:30:57 INFO reduce.MergeManagerImpl: Merging 1 files, 318974 bytes from disk\n",
      "16/02/03 02:30:57 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/03 02:30:57 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/03 02:30:57 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 318949 bytes\n",
      "16/02/03 02:30:57 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/03 02:30:57 INFO streaming.PipeMapRed: PipeMapRed exec [/home/ubuntu/HW3/./3_3_reducer_sort.py]\n",
      "16/02/03 02:30:57 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/02/03 02:30:57 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/02/03 02:30:58 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/03 02:30:58 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/03 02:30:58 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/03 02:30:58 INFO mapreduce.Job: Job job_local850505182_0001 running in uber mode : false\n",
      "16/02/03 02:30:58 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/02/03 02:30:58 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/03 02:30:58 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/03 02:30:58 INFO streaming.PipeMapRed: Records R/W=25186/1\n",
      "16/02/03 02:30:58 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/03 02:30:58 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/03 02:30:58 INFO mapred.Task: Task:attempt_local850505182_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/02/03 02:30:58 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/03 02:30:58 INFO mapred.Task: Task attempt_local850505182_0001_r_000000_0 is allowed to commit now\n",
      "16/02/03 02:30:58 INFO output.FileOutputCommitter: Saved output of task 'attempt_local850505182_0001_r_000000_0' to hdfs://localhost:54310/hw3/3_3_out_sorted/_temporary/0/task_local850505182_0001_r_000000\n",
      "16/02/03 02:30:58 INFO mapred.LocalJobRunner: Records R/W=25186/1 > reduce\n",
      "16/02/03 02:30:58 INFO mapred.Task: Task 'attempt_local850505182_0001_r_000000_0' done.\n",
      "16/02/03 02:30:58 INFO mapred.LocalJobRunner: Finishing task: attempt_local850505182_0001_r_000000_0\n",
      "16/02/03 02:30:58 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/02/03 02:30:59 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/02/03 02:30:59 INFO mapreduce.Job: Job job_local850505182_0001 completed successfully\n",
      "16/02/03 02:30:59 INFO mapreduce.Job: Counters: 35\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=850104\n",
      "\t\tFILE: Number of bytes written=1728168\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=461634\n",
      "\t\tHDFS: Number of bytes written=1116\n",
      "\t\tHDFS: Number of read operations=13\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=12593\n",
      "\t\tMap output records=25186\n",
      "\t\tMap output bytes=268596\n",
      "\t\tMap output materialized bytes=318974\n",
      "\t\tInput split bytes=97\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=12594\n",
      "\t\tReduce shuffle bytes=318974\n",
      "\t\tReduce input records=25186\n",
      "\t\tReduce output records=52\n",
      "\t\tSpilled Records=50372\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=15\n",
      "\t\tTotal committed heap usage (bytes)=665845760\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=230817\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=1116\n",
      "16/02/03 02:30:59 INFO streaming.StreamJob: Output directory: /hw3/3_3_out_sorted\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r /hw3/3_3_out_sorted\n",
    "\n",
    "!hadoop jar /usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar \\\n",
    "-D stream.map.output.field.separator=\"\\t\" \\\n",
    "-D mapreduce.job.output.key.comparator.class=\\\n",
    "org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "-D mapreduce.partition.keycomparator.options=\"-k2,2nr -k1,1\" \\\n",
    "-mapper 3_4_mapper_sort.py -reducer 3_3_reducer_sort.py \\\n",
    "-input /hw3/3_3_out -output /hw3/3_3_out_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/03 02:31:40 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "DAI62779\t6667\t0.0175\n",
      "FRO40251\t3881\t0.0102\n",
      "ELE17451\t3875\t0.0102\n",
      "GRO73461\t3602\t0.0095\n",
      "SNA80324\t3044\t0.0080\n",
      "ELE32164\t2851\t0.0075\n",
      "DAI75645\t2736\t0.0072\n",
      "SNA45677\t2455\t0.0064\n",
      "FRO31317\t2330\t0.0061\n",
      "DAI85309\t2293\t0.0060\n",
      "ELE26917\t2292\t0.0060\n",
      "FRO80039\t2233\t0.0059\n",
      "GRO21487\t2115\t0.0056\n",
      "SNA99873\t2083\t0.0055\n",
      "GRO59710\t2004\t0.0053\n",
      "GRO71621\t1920\t0.0050\n",
      "FRO85978\t1918\t0.0050\n",
      "GRO30386\t1840\t0.0048\n",
      "ELE74009\t1816\t0.0048\n",
      "GRO56726\t1784\t0.0047\n",
      "DAI63921\t1773\t0.0047\n",
      "GRO46854\t1756\t0.0046\n",
      "ELE66600\t1713\t0.0045\n",
      "DAI83733\t1712\t0.0045\n",
      "FRO32293\t1702\t0.0045\n",
      "ELE66810\t1697\t0.0045\n",
      "SNA55762\t1646\t0.0043\n",
      "DAI22177\t1627\t0.0043\n",
      "FRO78087\t1531\t0.0040\n",
      "ELE99737\t1516\t0.0040\n",
      "ELE34057\t1489\t0.0039\n",
      "GRO94758\t1489\t0.0039\n",
      "FRO35904\t1436\t0.0038\n",
      "FRO53271\t1420\t0.0037\n",
      "SNA93860\t1407\t0.0037\n",
      "SNA90094\t1390\t0.0036\n",
      "GRO38814\t1352\t0.0036\n",
      "ELE56788\t1345\t0.0035\n",
      "GRO61133\t1321\t0.0035\n",
      "DAI88807\t1316\t0.0035\n",
      "ELE74482\t1316\t0.0035\n",
      "ELE59935\t1311\t0.0034\n",
      "SNA96271\t1295\t0.0034\n",
      "DAI43223\t1290\t0.0034\n",
      "ELE91337\t1289\t0.0034\n",
      "GRO15017\t1275\t0.0033\n",
      "DAI31081\t1261\t0.0033\n",
      "GRO81087\t1220\t0.0032\n",
      "DAI22896\t1219\t0.0032\n",
      "GRO85051\t1214\t0.0032\n",
      "The number of unique items:  12591\t\n",
      "The largest basket size:  37\t\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /hw3/3_3_out_sorted/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### HW3.4. (Computationally prohibitive but then again Hadoop can handle this) Pairs ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting 3_4_mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile 3_4_mapper.py\n",
    "#!/usr/bin/python\n",
    "#HW 3.4\n",
    "\n",
    "import sys\n",
    "sys.stderr.write(\"reporter:counter:Calls,mapper_calls,1\\n\")\n",
    "linecount = 0\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    products = line.split(\" \")\n",
    "    products = sorted(products)\n",
    "    linecount += 1   \n",
    "    # count the product\n",
    "    for item in products:\n",
    "        for item2 in products[products.index(item)+1:]:\n",
    "            print \"%s,%s\\t1\" % (item, item2)\n",
    "            print \"%s\\t1\" % (item)\n",
    "    \n",
    "print \"linecount\\t\"+str(linecount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting 3_4_combiner.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile 3_4_combiner.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "\n",
    "sys.stderr.write(\"reporter:counter:Calls,combiner_calls,1\\n\")\n",
    "current_key = None\n",
    "current_count = 0\n",
    "key = None\n",
    "\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    key, count = line.split(\"\\t\", 1)\n",
    "    count = int(count)\n",
    "\n",
    "    if current_key == key:\n",
    "        current_count += count\n",
    "    else:\n",
    "        if current_key:\n",
    "            # write result to STDOUT\n",
    "            print '%s\\t%s' % (current_key, current_count)\n",
    "        current_count = count\n",
    "        current_key = key\n",
    "\n",
    "# output the last word\n",
    "if current_key == key:\n",
    "    print '%s\\t%s' % (current_key, current_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting 3_4_reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile 3_4_reducer.py\n",
    "#!/usr/bin/python\n",
    "#HW 3.4\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.stderr.write(\"reporter:counter:Calls,reducer_calls,1\\n\")\n",
    "pairs = {}\n",
    "current_key = None\n",
    "current_count = 0\n",
    "key = None\n",
    "linecount = 0\n",
    "\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    key, count = line.split(\"\\t\", 1)\n",
    "    count = int(count)\n",
    "    \n",
    "    if current_key == key:\n",
    "        current_count += int(count)\n",
    "    else:\n",
    "        if current_key:\n",
    "            pairs[current_key] = current_count\n",
    "        current_count = count\n",
    "        current_key = key\n",
    "\n",
    "# output the last word\n",
    "if current_key == key:\n",
    "    pairs[current_key] = current_count\n",
    "linecount = pairs[\"linecount\"]\n",
    "pairs.pop(\"linecount\")\n",
    "\n",
    "for key,value in pairs.items():\n",
    "    if value >= 100:\n",
    "        items = key.split(\",\", 1)\n",
    "        if len(items) == 2:\n",
    "            print \"%s\\t%s\\t%s\\t%.4f\\t%.4f\" % \\\n",
    "            (items[0], items[1], str(value), value*1.0/linecount, value*1.0/pairs[items[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/03 02:36:58 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/03 02:36:59 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /hw3/3_4_out\n",
      "16/02/03 02:37:00 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/03 02:37:01 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/02/03 02:37:01 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/02/03 02:37:01 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/02/03 02:37:01 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/02/03 02:37:01 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/02/03 02:37:01 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local643593373_0001\n",
      "16/02/03 02:37:01 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/02/03 02:37:01 INFO mapreduce.Job: Running job: job_local643593373_0001\n",
      "16/02/03 02:37:01 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/02/03 02:37:01 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/02/03 02:37:01 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/02/03 02:37:01 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/02/03 02:37:01 INFO mapred.LocalJobRunner: Starting task: attempt_local643593373_0001_m_000000_0\n",
      "16/02/03 02:37:01 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/02/03 02:37:01 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "16/02/03 02:37:01 INFO mapred.MapTask: Processing split: hdfs://localhost:54310/hw3/ProductPurchaseData.txt:0+3458517\n",
      "16/02/03 02:37:01 INFO mapred.MapTask: numReduceTasks: 1\n",
      "16/02/03 02:37:01 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/02/03 02:37:01 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/02/03 02:37:01 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/02/03 02:37:01 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/02/03 02:37:01 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/02/03 02:37:01 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/02/03 02:37:01 INFO streaming.PipeMapRed: PipeMapRed exec [/home/ubuntu/HW3/./3_4_mapper.py]\n",
      "16/02/03 02:37:01 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/02/03 02:37:01 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/02/03 02:37:01 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/02/03 02:37:01 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/02/03 02:37:01 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/02/03 02:37:01 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/02/03 02:37:01 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/02/03 02:37:01 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/02/03 02:37:01 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/02/03 02:37:01 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/02/03 02:37:01 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/02/03 02:37:01 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/02/03 02:37:01 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/03 02:37:01 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/03 02:37:01 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/03 02:37:01 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/03 02:37:02 INFO streaming.PipeMapRed: Records R/W=1216/1\n",
      "16/02/03 02:37:02 INFO mapreduce.Job: Job job_local643593373_0001 running in uber mode : false\n",
      "16/02/03 02:37:02 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/02/03 02:37:02 INFO streaming.PipeMapRed: R/W/S=10000/1726133/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/03 02:37:03 INFO mapred.MapTask: Spilling map output\n",
      "16/02/03 02:37:03 INFO mapred.MapTask: bufstart = 0; bufend = 42588315; bufvoid = 104857600\n",
      "16/02/03 02:37:03 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 15889960(63559840); length = 10324437/6553600\n",
      "16/02/03 02:37:03 INFO mapred.MapTask: (EQUATOR) 53074059 kvi 13268508(53074032)\n",
      "16/02/03 02:37:05 INFO streaming.PipeMapRed: PipeMapRed exec [/home/ubuntu/HW3/./3_4_combiner.py]\n",
      "16/02/03 02:37:05 INFO Configuration.deprecation: mapred.skip.map.auto.incr.proc.count is deprecated. Instead, use mapreduce.map.skip.proc-count.auto-incr\n",
      "16/02/03 02:37:05 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/03 02:37:05 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/03 02:37:05 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/03 02:37:05 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/03 02:37:05 INFO streaming.PipeMapRed: Records R/W=7946/1\n",
      "16/02/03 02:37:06 INFO streaming.PipeMapRed: R/W/S=10000/1520/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/03 02:37:06 INFO streaming.PipeMapRed: R/W/S=100000/19921/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/03 02:37:06 INFO streaming.PipeMapRed: R/W/S=200000/40648/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/03 02:37:06 INFO streaming.PipeMapRed: R/W/S=300000/61788/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/03 02:37:06 INFO streaming.PipeMapRed: R/W/S=400000/81268/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/03 02:37:06 INFO streaming.PipeMapRed: R/W/S=500000/101799/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/03 02:37:06 INFO streaming.PipeMapRed: R/W/S=600000/107301/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/03 02:37:06 INFO streaming.PipeMapRed: R/W/S=700000/125993/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/03 02:37:06 INFO streaming.PipeMapRed: R/W/S=800000/141994/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/03 02:37:06 INFO streaming.PipeMapRed: R/W/S=900000/156318/0 in:900000=900000/1 [rec/s] out:156318=156318/1 [rec/s]\n",
      "16/02/03 02:37:07 INFO streaming.PipeMapRed: R/W/S=1000000/177257/0 in:1000000=1000000/1 [rec/s] out:177257=177257/1 [rec/s]\n",
      "16/02/03 02:37:07 INFO streaming.PipeMapRed: R/W/S=1100000/193649/0 in:1100000=1100000/1 [rec/s] out:193649=193649/1 [rec/s]\n",
      "16/02/03 02:37:07 INFO streaming.PipeMapRed: R/W/S=1200000/212319/0 in:1200000=1200000/1 [rec/s] out:212319=212319/1 [rec/s]\n",
      "16/02/03 02:37:07 INFO streaming.PipeMapRed: R/W/S=1300000/229150/0 in:1300000=1300000/1 [rec/s] out:229150=229150/1 [rec/s]\n",
      "16/02/03 02:37:07 INFO streaming.PipeMapRed: R/W/S=1400000/253213/0 in:1400000=1400000/1 [rec/s] out:253213=253213/1 [rec/s]\n",
      "16/02/03 02:37:07 INFO streaming.PipeMapRed: R/W/S=1500000/273126/0 in:1500000=1500000/1 [rec/s] out:273126=273126/1 [rec/s]\n",
      "16/02/03 02:37:07 INFO streaming.PipeMapRed: R/W/S=1600000/290767/0 in:1600000=1600000/1 [rec/s] out:290767=290767/1 [rec/s]\n",
      "16/02/03 02:37:07 INFO streaming.PipeMapRed: R/W/S=1700000/313197/0 in:1700000=1700000/1 [rec/s] out:313197=313197/1 [rec/s]\n",
      "16/02/03 02:37:07 INFO streaming.PipeMapRed: R/W/S=1800000/333583/0 in:1800000=1800000/1 [rec/s] out:333583=333583/1 [rec/s]\n",
      "16/02/03 02:37:07 INFO mapred.LocalJobRunner: Records R/W=7946/1 > map\n",
      "16/02/03 02:37:07 INFO streaming.PipeMapRed: R/W/S=1900000/353348/0 in:1900000=1900000/1 [rec/s] out:353348=353348/1 [rec/s]\n",
      "16/02/03 02:37:07 INFO streaming.PipeMapRed: R/W/S=2000000/372511/0 in:1000000=2000000/2 [rec/s] out:186255=372511/2 [rec/s]\n",
      "16/02/03 02:37:08 INFO streaming.PipeMapRed: R/W/S=2100000/392914/0 in:1050000=2100000/2 [rec/s] out:196457=392914/2 [rec/s]\n",
      "16/02/03 02:37:08 INFO streaming.PipeMapRed: R/W/S=2200000/412305/0 in:1100000=2200000/2 [rec/s] out:206152=412305/2 [rec/s]\n",
      "16/02/03 02:37:08 INFO streaming.PipeMapRed: R/W/S=2300000/431721/0 in:1150000=2300000/2 [rec/s] out:215860=431721/2 [rec/s]\n",
      "16/02/03 02:37:08 INFO streaming.PipeMapRed: R/W/S=2400000/450524/0 in:1200000=2400000/2 [rec/s] out:225262=450524/2 [rec/s]\n",
      "16/02/03 02:37:08 INFO streaming.PipeMapRed: R/W/S=2500000/468149/0 in:1250000=2500000/2 [rec/s] out:234074=468149/2 [rec/s]\n",
      "16/02/03 02:37:08 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/03 02:37:08 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/03 02:37:08 INFO mapred.MapTask: Finished spill 0\n",
      "16/02/03 02:37:08 INFO mapred.MapTask: (RESET) equator 53074059 kv 13268508(53074032) kvi 10726504(42906016)\n",
      "16/02/03 02:37:08 INFO mapreduce.Job:  map 45% reduce 0%\n",
      "16/02/03 02:37:09 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/03 02:37:09 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/03 02:37:09 INFO mapred.LocalJobRunner: Records R/W=7946/1 > map\n",
      "16/02/03 02:37:09 INFO mapred.MapTask: Starting flush of map output\n",
      "16/02/03 02:37:09 INFO mapred.MapTask: Spilling map output\n",
      "16/02/03 02:37:09 INFO mapred.MapTask: bufstart = 53074059; bufend = 94109741; bufvoid = 104857600\n",
      "16/02/03 02:37:09 INFO mapred.MapTask: kvstart = 13268508(53074032); kvend = 3320468(13281872); length = 9948041/6553600\n",
      "16/02/03 02:37:10 INFO mapred.LocalJobRunner: Records R/W=7946/1 > sort\n",
      "16/02/03 02:37:11 INFO mapreduce.Job:  map 67% reduce 0%\n",
      "16/02/03 02:37:11 INFO streaming.PipeMapRed: PipeMapRed exec [/home/ubuntu/HW3/./3_4_combiner.py]\n",
      "16/02/03 02:37:11 INFO Configuration.deprecation: mapred.skip.reduce.auto.incr.proc.count is deprecated. Instead, use mapreduce.reduce.skip.proc-count.auto-incr\n",
      "16/02/03 02:37:11 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/03 02:37:11 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/03 02:37:11 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/03 02:37:11 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/03 02:37:12 INFO streaming.PipeMapRed: Records R/W=7950/1\n",
      "16/02/03 02:37:12 INFO streaming.PipeMapRed: R/W/S=10000/1027/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/03 02:37:12 INFO streaming.PipeMapRed: R/W/S=100000/23206/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/03 02:37:12 INFO streaming.PipeMapRed: R/W/S=200000/47213/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/03 02:37:12 INFO streaming.PipeMapRed: R/W/S=300000/71012/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/03 02:37:12 INFO streaming.PipeMapRed: R/W/S=400000/91725/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/03 02:37:12 INFO streaming.PipeMapRed: R/W/S=500000/112636/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/03 02:37:12 INFO streaming.PipeMapRed: R/W/S=600000/125958/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/03 02:37:12 INFO streaming.PipeMapRed: R/W/S=700000/149148/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/03 02:37:12 INFO streaming.PipeMapRed: R/W/S=800000/167821/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/03 02:37:12 INFO streaming.PipeMapRed: R/W/S=900000/189143/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/03 02:37:12 INFO streaming.PipeMapRed: R/W/S=1000000/210483/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/03 02:37:13 INFO streaming.PipeMapRed: R/W/S=1100000/228116/0 in:1100000=1100000/1 [rec/s] out:228116=228116/1 [rec/s]\n",
      "16/02/03 02:37:13 INFO streaming.PipeMapRed: R/W/S=1200000/251122/0 in:1200000=1200000/1 [rec/s] out:251122=251122/1 [rec/s]\n",
      "16/02/03 02:37:13 INFO streaming.PipeMapRed: R/W/S=1300000/268951/0 in:1300000=1300000/1 [rec/s] out:268951=268951/1 [rec/s]\n",
      "16/02/03 02:37:13 INFO streaming.PipeMapRed: R/W/S=1400000/296310/0 in:1400000=1400000/1 [rec/s] out:296310=296310/1 [rec/s]\n",
      "16/02/03 02:37:13 INFO streaming.PipeMapRed: R/W/S=1500000/316422/0 in:1500000=1500000/1 [rec/s] out:316422=316422/1 [rec/s]\n",
      "16/02/03 02:37:13 INFO streaming.PipeMapRed: R/W/S=1600000/340682/0 in:1600000=1600000/1 [rec/s] out:340682=340682/1 [rec/s]\n",
      "16/02/03 02:37:13 INFO streaming.PipeMapRed: R/W/S=1700000/365368/0 in:1700000=1700000/1 [rec/s] out:365368=365368/1 [rec/s]\n",
      "16/02/03 02:37:13 INFO streaming.PipeMapRed: R/W/S=1800000/390087/0 in:1800000=1800000/1 [rec/s] out:390087=390087/1 [rec/s]\n",
      "16/02/03 02:37:13 INFO streaming.PipeMapRed: R/W/S=1900000/411075/0 in:1900000=1900000/1 [rec/s] out:411075=411075/1 [rec/s]\n",
      "16/02/03 02:37:13 INFO mapred.LocalJobRunner: Records R/W=7950/1 > sort\n",
      "16/02/03 02:37:13 INFO streaming.PipeMapRed: R/W/S=2000000/438302/0 in:2000000=2000000/1 [rec/s] out:438302=438302/1 [rec/s]\n",
      "16/02/03 02:37:14 INFO streaming.PipeMapRed: R/W/S=2100000/460142/0 in:1050000=2100000/2 [rec/s] out:230071=460142/2 [rec/s]\n",
      "16/02/03 02:37:14 INFO streaming.PipeMapRed: R/W/S=2200000/482216/0 in:1100000=2200000/2 [rec/s] out:241108=482216/2 [rec/s]\n",
      "16/02/03 02:37:14 INFO streaming.PipeMapRed: R/W/S=2300000/505570/0 in:1150000=2300000/2 [rec/s] out:252785=505570/2 [rec/s]\n",
      "16/02/03 02:37:14 INFO streaming.PipeMapRed: R/W/S=2400000/528579/0 in:1200000=2400000/2 [rec/s] out:264289=528579/2 [rec/s]\n",
      "16/02/03 02:37:14 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/03 02:37:14 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/03 02:37:14 INFO mapred.MapTask: Finished spill 1\n",
      "16/02/03 02:37:14 INFO mapred.Merger: Merging 2 sorted segments\n",
      "16/02/03 02:37:14 INFO mapred.Merger: Down to the last merge-pass, with 2 segments left of total size: 22805108 bytes\n",
      "16/02/03 02:37:15 INFO mapred.Task: Task:attempt_local643593373_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/02/03 02:37:15 INFO mapred.LocalJobRunner: Records R/W=7950/1 > sort\n",
      "16/02/03 02:37:15 INFO mapred.Task: Task 'attempt_local643593373_0001_m_000000_0' done.\n",
      "16/02/03 02:37:15 INFO mapred.LocalJobRunner: Finishing task: attempt_local643593373_0001_m_000000_0\n",
      "16/02/03 02:37:15 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/02/03 02:37:15 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/02/03 02:37:15 INFO mapred.LocalJobRunner: Starting task: attempt_local643593373_0001_r_000000_0\n",
      "16/02/03 02:37:15 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/02/03 02:37:15 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "16/02/03 02:37:15 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@51c3358f\n",
      "16/02/03 02:37:15 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=333971456, maxSingleShuffleLimit=83492864, mergeThreshold=220421168, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/03 02:37:15 INFO reduce.EventFetcher: attempt_local643593373_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/03 02:37:15 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local643593373_0001_m_000000_0 decomp: 22805120 len: 22805124 to MEMORY\n",
      "16/02/03 02:37:15 INFO reduce.InMemoryMapOutput: Read 22805120 bytes from map-output for attempt_local643593373_0001_m_000000_0\n",
      "16/02/03 02:37:15 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 22805120, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->22805120\n",
      "16/02/03 02:37:15 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/03 02:37:15 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/03 02:37:15 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/03 02:37:15 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/03 02:37:15 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 22805109 bytes\n",
      "16/02/03 02:37:15 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/02/03 02:37:15 INFO reduce.MergeManagerImpl: Merged 1 segments, 22805120 bytes to disk to satisfy reduce memory limit\n",
      "16/02/03 02:37:15 INFO reduce.MergeManagerImpl: Merging 1 files, 22805124 bytes from disk\n",
      "16/02/03 02:37:15 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/03 02:37:15 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/03 02:37:15 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 22805109 bytes\n",
      "16/02/03 02:37:15 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/03 02:37:15 INFO streaming.PipeMapRed: PipeMapRed exec [/home/ubuntu/HW3/./3_4_reducer.py]\n",
      "16/02/03 02:37:15 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/02/03 02:37:15 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/02/03 02:37:15 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/03 02:37:15 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/03 02:37:15 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/03 02:37:15 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/03 02:37:16 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/03 02:37:16 INFO streaming.PipeMapRed: R/W/S=100000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/03 02:37:16 INFO streaming.PipeMapRed: R/W/S=200000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/03 02:37:16 INFO streaming.PipeMapRed: R/W/S=300000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/03 02:37:16 INFO streaming.PipeMapRed: R/W/S=400000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/03 02:37:16 INFO streaming.PipeMapRed: R/W/S=500000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/03 02:37:16 INFO streaming.PipeMapRed: R/W/S=600000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/03 02:37:16 INFO streaming.PipeMapRed: R/W/S=700000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/03 02:37:16 INFO streaming.PipeMapRed: R/W/S=800000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/03 02:37:17 INFO streaming.PipeMapRed: R/W/S=900000/0/0 in:900000=900000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/02/03 02:37:17 INFO streaming.PipeMapRed: R/W/S=1000000/0/0 in:1000000=1000000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/02/03 02:37:17 INFO streaming.PipeMapRed: Records R/W=1041189/1\n",
      "16/02/03 02:37:17 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/03 02:37:17 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/03 02:37:17 INFO mapred.Task: Task:attempt_local643593373_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/02/03 02:37:17 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/03 02:37:17 INFO mapred.Task: Task attempt_local643593373_0001_r_000000_0 is allowed to commit now\n",
      "16/02/03 02:37:17 INFO output.FileOutputCommitter: Saved output of task 'attempt_local643593373_0001_r_000000_0' to hdfs://localhost:54310/hw3/3_4_out/_temporary/0/task_local643593373_0001_r_000000\n",
      "16/02/03 02:37:17 INFO mapred.LocalJobRunner: Records R/W=1041189/1 > reduce\n",
      "16/02/03 02:37:17 INFO mapred.Task: Task 'attempt_local643593373_0001_r_000000_0' done.\n",
      "16/02/03 02:37:17 INFO mapred.LocalJobRunner: Finishing task: attempt_local643593373_0001_r_000000_0\n",
      "16/02/03 02:37:17 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/02/03 02:37:18 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/02/03 02:37:18 INFO mapreduce.Job: Job job_local643593373_0001 completed successfully\n",
      "16/02/03 02:37:18 INFO mapreduce.Job: Counters: 38\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=91432674\n",
      "\t\tFILE: Number of bytes written=114796704\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=6917034\n",
      "\t\tHDFS: Number of bytes written=48031\n",
      "\t\tHDFS: Number of read operations=13\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=31101\n",
      "\t\tMap output records=5068121\n",
      "\t\tMap output bytes=83623997\n",
      "\t\tMap output materialized bytes=22805124\n",
      "\t\tInput split bytes=102\n",
      "\t\tCombine input records=5068121\n",
      "\t\tCombine output records=1041189\n",
      "\t\tReduce input groups=889110\n",
      "\t\tReduce shuffle bytes=22805124\n",
      "\t\tReduce input records=1041189\n",
      "\t\tReduce output records=1334\n",
      "\t\tSpilled Records=3123567\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=18\n",
      "\t\tTotal committed heap usage (bytes)=666894336\n",
      "\tCalls\n",
      "\t\tcombiner_calls=2\n",
      "\t\tmapper_calls=1\n",
      "\t\treducer_calls=1\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=3458517\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=48031\n",
      "16/02/03 02:37:18 INFO streaming.StreamJob: Output directory: /hw3/3_4_out\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r /hw3/3_4_out\n",
    "\n",
    "!hadoop jar /usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar \\\n",
    "-D stream.map.output.field.separator=\"\\t\" \\\n",
    "-mapper 3_4_mapper.py -combiner 3_4_combiner.py -reducer 3_4_reducer.py \\\n",
    "-input /hw3/ProductPurchaseData.txt -output /hw3/3_4_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/03 02:37:39 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "DAI63921\tFRO80039\t126\t0.0041\t0.0058\n",
      "DAI95741\tSNA90094\t120\t0.0039\t0.0124\n",
      "DAI55911\tELE26917\t113\t0.0036\t0.0093\n",
      "DAI63921\tDAI83733\t142\t0.0046\t0.0065\n",
      "DAI62779\tFRO80039\t550\t0.0177\t0.0071\n",
      "DAI42493\tELE32164\t266\t0.0086\t0.0203\n",
      "ELE17451\tFRO75586\t151\t0.0049\t0.0039\n",
      "SNA90094\tSNA96271\t104\t0.0033\t0.1908\n",
      "DAI63921\tSNA55952\t110\t0.0035\t0.0051\n",
      "ELE11111\tELE17451\t121\t0.0039\t0.0142\n",
      "cat: Unable to write to output stream.\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /hw3/3_4_out/part-00000 | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting 3_4_mapper_sort.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile 3_4_mapper_sort.py\n",
    "#!/usr/bin/python\n",
    "#HW 3.4\n",
    "\n",
    "import sys\n",
    "\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    print line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting 3_4_reducer_sort.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile 3_4_reducer_sort.py\n",
    "#!/usr/bin/python\n",
    "#HW 3.4\n",
    "\n",
    "import sys\n",
    "topN = 0\n",
    "\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    if topN < 50:\n",
    "        print line\n",
    "        topN += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/03 02:38:34 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/03 02:38:34 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /hw3/3_4_out_sorted\n",
      "16/02/03 02:38:36 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/03 02:38:36 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/02/03 02:38:36 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/02/03 02:38:36 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/02/03 02:38:36 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/02/03 02:38:36 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/02/03 02:38:36 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local452038128_0001\n",
      "16/02/03 02:38:37 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/02/03 02:38:37 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/02/03 02:38:37 INFO mapreduce.Job: Running job: job_local452038128_0001\n",
      "16/02/03 02:38:37 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/02/03 02:38:37 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/02/03 02:38:37 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/02/03 02:38:37 INFO mapred.LocalJobRunner: Starting task: attempt_local452038128_0001_m_000000_0\n",
      "16/02/03 02:38:37 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/02/03 02:38:37 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "16/02/03 02:38:37 INFO mapred.MapTask: Processing split: hdfs://localhost:54310/hw3/3_4_out/part-00000:0+48031\n",
      "16/02/03 02:38:37 INFO mapred.MapTask: numReduceTasks: 1\n",
      "16/02/03 02:38:37 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/02/03 02:38:37 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/02/03 02:38:37 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/02/03 02:38:37 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/02/03 02:38:37 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/02/03 02:38:37 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/02/03 02:38:37 INFO streaming.PipeMapRed: PipeMapRed exec [/home/ubuntu/HW3/./3_4_mapper_sort.py]\n",
      "16/02/03 02:38:37 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/02/03 02:38:37 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/02/03 02:38:37 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/02/03 02:38:37 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/02/03 02:38:37 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/02/03 02:38:37 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/02/03 02:38:37 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/02/03 02:38:37 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/02/03 02:38:37 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/02/03 02:38:37 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/02/03 02:38:37 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/02/03 02:38:37 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/02/03 02:38:37 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/03 02:38:37 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/03 02:38:37 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/03 02:38:37 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/03 02:38:37 INFO streaming.PipeMapRed: Records R/W=1334/1\n",
      "16/02/03 02:38:37 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/03 02:38:37 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/03 02:38:37 INFO mapred.LocalJobRunner: \n",
      "16/02/03 02:38:37 INFO mapred.MapTask: Starting flush of map output\n",
      "16/02/03 02:38:37 INFO mapred.MapTask: Spilling map output\n",
      "16/02/03 02:38:37 INFO mapred.MapTask: bufstart = 0; bufend = 52033; bufvoid = 104857600\n",
      "16/02/03 02:38:37 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26203728(104814912); length = 10669/6553600\n",
      "16/02/03 02:38:37 INFO mapred.MapTask: Finished spill 0\n",
      "16/02/03 02:38:37 INFO mapred.Task: Task:attempt_local452038128_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/02/03 02:38:37 INFO mapred.LocalJobRunner: Records R/W=1334/1\n",
      "16/02/03 02:38:37 INFO mapred.Task: Task 'attempt_local452038128_0001_m_000000_0' done.\n",
      "16/02/03 02:38:37 INFO mapred.LocalJobRunner: Finishing task: attempt_local452038128_0001_m_000000_0\n",
      "16/02/03 02:38:37 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/02/03 02:38:37 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/02/03 02:38:37 INFO mapred.LocalJobRunner: Starting task: attempt_local452038128_0001_r_000000_0\n",
      "16/02/03 02:38:37 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/02/03 02:38:37 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "16/02/03 02:38:37 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@2925e41e\n",
      "16/02/03 02:38:37 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=333971456, maxSingleShuffleLimit=83492864, mergeThreshold=220421168, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/03 02:38:37 INFO reduce.EventFetcher: attempt_local452038128_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/03 02:38:37 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local452038128_0001_m_000000_0 decomp: 57371 len: 57375 to MEMORY\n",
      "16/02/03 02:38:37 INFO reduce.InMemoryMapOutput: Read 57371 bytes from map-output for attempt_local452038128_0001_m_000000_0\n",
      "16/02/03 02:38:37 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 57371, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->57371\n",
      "16/02/03 02:38:37 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/03 02:38:37 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/03 02:38:37 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/03 02:38:37 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/03 02:38:37 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 57332 bytes\n",
      "16/02/03 02:38:37 INFO reduce.MergeManagerImpl: Merged 1 segments, 57371 bytes to disk to satisfy reduce memory limit\n",
      "16/02/03 02:38:37 INFO reduce.MergeManagerImpl: Merging 1 files, 57375 bytes from disk\n",
      "16/02/03 02:38:37 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/03 02:38:37 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/03 02:38:37 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 57332 bytes\n",
      "16/02/03 02:38:37 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/03 02:38:37 INFO streaming.PipeMapRed: PipeMapRed exec [/home/ubuntu/HW3/./3_4_reducer_sort.py]\n",
      "16/02/03 02:38:37 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/02/03 02:38:37 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/02/03 02:38:37 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/03 02:38:37 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/03 02:38:37 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/03 02:38:37 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/03 02:38:37 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/03 02:38:37 INFO streaming.PipeMapRed: Records R/W=2668/1\n",
      "16/02/03 02:38:37 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/03 02:38:37 INFO mapred.Task: Task:attempt_local452038128_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/02/03 02:38:37 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/03 02:38:37 INFO mapred.Task: Task attempt_local452038128_0001_r_000000_0 is allowed to commit now\n",
      "16/02/03 02:38:37 INFO output.FileOutputCommitter: Saved output of task 'attempt_local452038128_0001_r_000000_0' to hdfs://localhost:54310/hw3/3_4_out_sorted/_temporary/0/task_local452038128_0001_r_000000\n",
      "16/02/03 02:38:37 INFO mapred.LocalJobRunner: Records R/W=2668/1 > reduce\n",
      "16/02/03 02:38:37 INFO mapred.Task: Task 'attempt_local452038128_0001_r_000000_0' done.\n",
      "16/02/03 02:38:37 INFO mapred.LocalJobRunner: Finishing task: attempt_local452038128_0001_r_000000_0\n",
      "16/02/03 02:38:37 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/02/03 02:38:38 INFO mapreduce.Job: Job job_local452038128_0001 running in uber mode : false\n",
      "16/02/03 02:38:38 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/02/03 02:38:38 INFO mapreduce.Job: Job job_local452038128_0001 completed successfully\n",
      "16/02/03 02:38:38 INFO mapreduce.Job: Counters: 35\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=326904\n",
      "\t\tFILE: Number of bytes written=943393\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=96062\n",
      "\t\tHDFS: Number of bytes written=1957\n",
      "\t\tHDFS: Number of read operations=13\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=1334\n",
      "\t\tMap output records=2668\n",
      "\t\tMap output bytes=52033\n",
      "\t\tMap output materialized bytes=57375\n",
      "\t\tInput split bytes=97\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=1335\n",
      "\t\tReduce shuffle bytes=57375\n",
      "\t\tReduce input records=2668\n",
      "\t\tReduce output records=100\n",
      "\t\tSpilled Records=5336\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=9\n",
      "\t\tTotal committed heap usage (bytes)=597164032\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=48031\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=1957\n",
      "16/02/03 02:38:38 INFO streaming.StreamJob: Output directory: /hw3/3_4_out_sorted\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r /hw3/3_4_out_sorted\n",
    "\n",
    "!hadoop jar /usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar \\\n",
    "-D stream.map.output.field.separator=\"\\t\" \\\n",
    "-D mapreduce.job.output.key.comparator.class=\\\n",
    "org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "-D mapreduce.partition.keycomparator.options=\"-k3,3nr -k1,1 -k2,2\" \\\n",
    "-mapper 3_4_mapper_sort.py -reducer 3_4_reducer_sort.py \\\n",
    "-input /hw3/3_4_out -output /hw3/3_4_out_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/03 02:38:48 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "DAI62779\tELE17451\t1592\t0.0512\t0.0204\t\n",
      "\t\n",
      "FRO40251\tSNA80324\t1412\t0.0454\t0.0489\t\n",
      "\t\n",
      "DAI75645\tFRO40251\t1254\t0.0403\t0.0365\t\n",
      "\t\n",
      "FRO40251\tGRO85051\t1213\t0.0390\t0.0420\t\n",
      "\t\n",
      "DAI62779\tGRO73461\t1139\t0.0366\t0.0146\t\n",
      "\t\n",
      "DAI75645\tSNA80324\t1130\t0.0363\t0.0329\t\n",
      "\t\n",
      "DAI62779\tFRO40251\t1070\t0.0344\t0.0137\t\n",
      "\t\n",
      "DAI62779\tSNA80324\t923\t0.0297\t0.0119\t\n",
      "\t\n",
      "DAI62779\tDAI85309\t918\t0.0295\t0.0118\t\n",
      "\t\n",
      "ELE32164\tGRO59710\t911\t0.0293\t0.0344\t\n",
      "\t\n",
      "DAI62779\tDAI75645\t882\t0.0284\t0.0113\t\n",
      "\t\n",
      "FRO40251\tGRO73461\t882\t0.0284\t0.0305\t\n",
      "\t\n",
      "DAI62779\tELE92920\t877\t0.0282\t0.0113\t\n",
      "\t\n",
      "FRO40251\tFRO92469\t835\t0.0268\t0.0289\t\n",
      "\t\n",
      "DAI62779\tELE32164\t832\t0.0268\t0.0107\t\n",
      "\t\n",
      "DAI75645\tGRO73461\t712\t0.0229\t0.0207\t\n",
      "\t\n",
      "DAI43223\tELE32164\t711\t0.0229\t0.0423\t\n",
      "\t\n",
      "DAI62779\tGRO30386\t709\t0.0228\t0.0091\t\n",
      "\t\n",
      "ELE17451\tFRO40251\t697\t0.0224\t0.0180\t\n",
      "\t\n",
      "DAI85309\tELE99737\t659\t0.0212\t0.0246\t\n",
      "\t\n",
      "DAI62779\tELE26917\t650\t0.0209\t0.0083\t\n",
      "\t\n",
      "GRO21487\tGRO73461\t631\t0.0203\t0.0569\t\n",
      "\t\n",
      "DAI62779\tSNA45677\t604\t0.0194\t0.0078\t\n",
      "\t\n",
      "ELE17451\tSNA80324\t597\t0.0192\t0.0154\t\n",
      "\t\n",
      "DAI62779\tGRO71621\t595\t0.0191\t0.0076\t\n",
      "\t\n",
      "DAI62779\tSNA55762\t593\t0.0191\t0.0076\t\n",
      "\t\n",
      "DAI62779\tDAI83733\t586\t0.0188\t0.0075\t\n",
      "\t\n",
      "ELE17451\tGRO73461\t580\t0.0186\t0.0150\t\n",
      "\t\n",
      "GRO73461\tSNA80324\t562\t0.0181\t0.0470\t\n",
      "\t\n",
      "DAI62779\tGRO59710\t561\t0.0180\t0.0072\t\n",
      "\t\n",
      "DAI62779\tFRO80039\t550\t0.0177\t0.0071\t\n",
      "\t\n",
      "DAI75645\tELE17451\t547\t0.0176\t0.0159\t\n",
      "\t\n",
      "DAI62779\tSNA93860\t537\t0.0173\t0.0069\t\n",
      "\t\n",
      "DAI55148\tDAI62779\t526\t0.0169\t0.0452\t\n",
      "\t\n",
      "DAI43223\tGRO59710\t512\t0.0165\t0.0305\t\n",
      "\t\n",
      "ELE17451\tELE32164\t511\t0.0164\t0.0132\t\n",
      "\t\n",
      "DAI62779\tSNA18336\t506\t0.0163\t0.0065\t\n",
      "\t\n",
      "ELE32164\tGRO73461\t486\t0.0156\t0.0184\t\n",
      "\t\n",
      "DAI62779\tFRO78087\t482\t0.0155\t0.0062\t\n",
      "\t\n",
      "DAI85309\tELE17451\t482\t0.0155\t0.0180\t\n",
      "\t\n",
      "DAI62779\tGRO94758\t479\t0.0154\t0.0062\t\n",
      "\t\n",
      "DAI62779\tGRO21487\t471\t0.0151\t0.0061\t\n",
      "\t\n",
      "GRO85051\tSNA80324\t471\t0.0151\t0.1296\t\n",
      "\t\n",
      "ELE17451\tGRO30386\t468\t0.0150\t0.0121\t\n",
      "\t\n",
      "FRO85978\tSNA95666\t463\t0.0149\t0.0413\t\n",
      "\t\n",
      "DAI62779\tFRO19221\t462\t0.0149\t0.0059\t\n",
      "\t\n",
      "DAI62779\tGRO46854\t461\t0.0148\t0.0059\t\n",
      "\t\n",
      "DAI43223\tDAI62779\t459\t0.0148\t0.0273\t\n",
      "\t\n",
      "ELE92920\tSNA18336\t455\t0.0146\t0.0482\t\n",
      "\t\n",
      "DAI88079\tFRO40251\t446\t0.0143\t0.0839\t\n",
      "\t\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /hw3/3_4_out_sorted/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW3.5: Stripes ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting 3_5_mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile 3_5_mapper.py\n",
    "#!/usr/bin/python\n",
    "#HW 3.4\n",
    "\n",
    "import sys\n",
    "sys.stderr.write(\"reporter:counter:Calls,mapper_calls,1\\n\")\n",
    "linecount = 0\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    products = line.split(\" \")\n",
    "    products = sorted(products)\n",
    "    linecount += 1   \n",
    "    # emit the product\n",
    "    for item in products:\n",
    "        for item2 in products[products.index(item)+1:]:\n",
    "            print \"%s,%s\\t1\" % (item, item2)\n",
    "    \n",
    "print \"linecount\\t\"+str(linecount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting 3_5_reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile 3_5_reducer.py\n",
    "#!/usr/bin/python\n",
    "#HW 3.5\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.stderr.write(\"reporter:counter:Calls,reducer_calls,1\\n\")\n",
    "stripes = {}\n",
    "current_key = None\n",
    "current_count = 0\n",
    "key = None\n",
    "linecount = 0\n",
    "\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    key, count = line.split(\"\\t\", 1)\n",
    "    count = int(count)\n",
    "    \n",
    "    if current_key == key:\n",
    "        current_count += int(count)\n",
    "    else:\n",
    "        if current_key:\n",
    "            items = current_key.split(\",\", 1)\n",
    "            if len(items) == 2:\n",
    "                stripes.setdefault(items[0], {})\n",
    "                stripes[items[0]][items[1]]=current_count\n",
    "            elif items[0] == \"linecount\":\n",
    "                linecount = current_count\n",
    "        current_count = count\n",
    "        current_key = key\n",
    "\n",
    "# output the last word\n",
    "if current_key == key:\n",
    "    items = current_key.split(\",\", 1)\n",
    "    if len(items) == 2:\n",
    "        stripes.setdefault(items[0], {})\n",
    "        stripes[items[0]][items[1]]=current_count\n",
    "    elif items[0] == \"linecount\":\n",
    "        linecount = current_count\n",
    "        \n",
    "\n",
    "for key, stripe in stripes.items():\n",
    "    marg_count = sum(stripe.values())\n",
    "    for key2, count in stripe.items():\n",
    "        if count >= 100:\n",
    "            print \"%s\\t%s\\t%s\\t%.4f\\t%.4f\" % \\\n",
    "            (key, key2, str(count), count*1.0/linecount, count*1.0/marg_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/03 03:02:14 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/03 03:02:14 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /hw3/3_5_out\n",
      "16/02/03 03:02:16 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/03 03:02:16 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/02/03 03:02:16 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/02/03 03:02:16 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/02/03 03:02:16 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/02/03 03:02:16 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/02/03 03:02:17 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1800315953_0001\n",
      "16/02/03 03:02:17 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/02/03 03:02:17 INFO mapreduce.Job: Running job: job_local1800315953_0001\n",
      "16/02/03 03:02:17 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/02/03 03:02:17 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/02/03 03:02:17 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/02/03 03:02:17 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/02/03 03:02:17 INFO mapred.LocalJobRunner: Starting task: attempt_local1800315953_0001_m_000000_0\n",
      "16/02/03 03:02:17 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/02/03 03:02:17 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "16/02/03 03:02:17 INFO mapred.MapTask: Processing split: hdfs://localhost:54310/hw3/ProductPurchaseData.txt:0+3458517\n",
      "16/02/03 03:02:17 INFO mapred.MapTask: numReduceTasks: 1\n",
      "16/02/03 03:02:17 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/02/03 03:02:17 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/02/03 03:02:17 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/02/03 03:02:17 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/02/03 03:02:17 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/02/03 03:02:17 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/02/03 03:02:17 INFO streaming.PipeMapRed: PipeMapRed exec [/home/ubuntu/HW3/./3_5_mapper.py]\n",
      "16/02/03 03:02:17 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/02/03 03:02:17 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/02/03 03:02:17 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/02/03 03:02:17 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/02/03 03:02:17 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/02/03 03:02:17 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/02/03 03:02:17 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/02/03 03:02:17 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/02/03 03:02:17 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/02/03 03:02:17 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/02/03 03:02:17 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/02/03 03:02:17 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/02/03 03:02:17 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/03 03:02:17 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/03 03:02:17 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/03 03:02:17 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/03 03:02:17 INFO streaming.PipeMapRed: Records R/W=1216/1\n",
      "16/02/03 03:02:18 INFO mapreduce.Job: Job job_local1800315953_0001 running in uber mode : false\n",
      "16/02/03 03:02:18 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/02/03 03:02:18 INFO streaming.PipeMapRed: R/W/S=10000/860528/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/03 03:02:19 INFO mapred.MapTask: Spilling map output\n",
      "16/02/03 03:02:19 INFO mapred.MapTask: bufstart = 0; bufend = 47611011; bufvoid = 104857600\n",
      "16/02/03 03:02:19 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 17145636(68582544); length = 9068761/6553600\n",
      "16/02/03 03:02:19 INFO mapred.MapTask: (EQUATOR) 56679763 kvi 14169936(56679744)\n",
      "16/02/03 03:02:19 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/03 03:02:19 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/03 03:02:19 INFO mapred.LocalJobRunner: \n",
      "16/02/03 03:02:19 INFO mapred.MapTask: Starting flush of map output\n",
      "16/02/03 03:02:23 INFO mapred.MapTask: Finished spill 0\n",
      "16/02/03 03:02:23 INFO mapred.MapTask: (RESET) equator 56679763 kv 14169936(56679744) kvi 13102456(52409824)\n",
      "16/02/03 03:02:23 INFO mapred.MapTask: Spilling map output\n",
      "16/02/03 03:02:23 INFO mapred.MapTask: bufstart = 56679763; bufend = 62284029; bufvoid = 104857600\n",
      "16/02/03 03:02:23 INFO mapred.MapTask: kvstart = 14169936(56679744); kvend = 13102460(52409840); length = 1067477/6553600\n",
      "16/02/03 03:02:23 INFO mapred.LocalJobRunner: Records R/W=1216/1 > sort\n",
      "16/02/03 03:02:23 INFO mapred.MapTask: Finished spill 1\n",
      "16/02/03 03:02:23 INFO mapred.Merger: Merging 2 sorted segments\n",
      "16/02/03 03:02:23 INFO mapred.Merger: Down to the last merge-pass, with 2 segments left of total size: 58283367 bytes\n",
      "16/02/03 03:02:24 INFO mapreduce.Job:  map 67% reduce 0%\n",
      "16/02/03 03:02:25 INFO mapred.Task: Task:attempt_local1800315953_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/02/03 03:02:25 INFO mapred.LocalJobRunner: Records R/W=1216/1 > sort\n",
      "16/02/03 03:02:25 INFO mapred.Task: Task 'attempt_local1800315953_0001_m_000000_0' done.\n",
      "16/02/03 03:02:25 INFO mapred.LocalJobRunner: Finishing task: attempt_local1800315953_0001_m_000000_0\n",
      "16/02/03 03:02:25 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/02/03 03:02:25 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/02/03 03:02:25 INFO mapred.LocalJobRunner: Starting task: attempt_local1800315953_0001_r_000000_0\n",
      "16/02/03 03:02:25 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/02/03 03:02:25 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "16/02/03 03:02:25 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@6c77d00e\n",
      "16/02/03 03:02:25 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=333971456, maxSingleShuffleLimit=83492864, mergeThreshold=220421168, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/03 03:02:25 INFO reduce.EventFetcher: attempt_local1800315953_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/03 03:02:25 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1800315953_0001_m_000000_0 decomp: 58283401 len: 58283405 to MEMORY\n",
      "16/02/03 03:02:25 INFO reduce.InMemoryMapOutput: Read 58283401 bytes from map-output for attempt_local1800315953_0001_m_000000_0\n",
      "16/02/03 03:02:25 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 58283401, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->58283401\n",
      "16/02/03 03:02:25 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/03 03:02:25 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/03 03:02:25 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/03 03:02:25 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/03 03:02:25 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 58283379 bytes\n",
      "16/02/03 03:02:26 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/02/03 03:02:27 INFO reduce.MergeManagerImpl: Merged 1 segments, 58283401 bytes to disk to satisfy reduce memory limit\n",
      "16/02/03 03:02:27 INFO reduce.MergeManagerImpl: Merging 1 files, 58283405 bytes from disk\n",
      "16/02/03 03:02:27 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/03 03:02:27 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/03 03:02:27 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 58283379 bytes\n",
      "16/02/03 03:02:27 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/03 03:02:27 INFO streaming.PipeMapRed: PipeMapRed exec [/home/ubuntu/HW3/./3_5_reducer.py]\n",
      "16/02/03 03:02:27 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/02/03 03:02:27 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/02/03 03:02:27 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/03 03:02:27 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/03 03:02:27 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/03 03:02:27 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/03 03:02:27 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/03 03:02:27 INFO streaming.PipeMapRed: R/W/S=100000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/03 03:02:27 INFO streaming.PipeMapRed: R/W/S=200000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/03 03:02:27 INFO streaming.PipeMapRed: R/W/S=300000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/03 03:02:27 INFO streaming.PipeMapRed: R/W/S=400000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/03 03:02:27 INFO streaming.PipeMapRed: R/W/S=500000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/03 03:02:28 INFO streaming.PipeMapRed: R/W/S=600000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/03 03:02:28 INFO streaming.PipeMapRed: R/W/S=700000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/03 03:02:28 INFO streaming.PipeMapRed: R/W/S=800000/0/0 in:800000=800000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/02/03 03:02:28 INFO streaming.PipeMapRed: R/W/S=900000/0/0 in:900000=900000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/02/03 03:02:28 INFO streaming.PipeMapRed: R/W/S=1000000/0/0 in:1000000=1000000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/02/03 03:02:28 INFO streaming.PipeMapRed: R/W/S=1100000/0/0 in:1100000=1100000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/02/03 03:02:28 INFO streaming.PipeMapRed: R/W/S=1200000/0/0 in:1200000=1200000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/02/03 03:02:28 INFO streaming.PipeMapRed: R/W/S=1300000/0/0 in:1300000=1300000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/02/03 03:02:28 INFO streaming.PipeMapRed: R/W/S=1400000/0/0 in:1400000=1400000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/02/03 03:02:29 INFO streaming.PipeMapRed: R/W/S=1500000/0/0 in:1500000=1500000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/02/03 03:02:29 INFO streaming.PipeMapRed: R/W/S=1600000/0/0 in:1600000=1600000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/02/03 03:02:29 INFO streaming.PipeMapRed: R/W/S=1700000/0/0 in:850000=1700000/2 [rec/s] out:0=0/2 [rec/s]\n",
      "16/02/03 03:02:29 INFO streaming.PipeMapRed: R/W/S=1800000/0/0 in:900000=1800000/2 [rec/s] out:0=0/2 [rec/s]\n",
      "16/02/03 03:02:29 INFO streaming.PipeMapRed: R/W/S=1900000/0/0 in:950000=1900000/2 [rec/s] out:0=0/2 [rec/s]\n",
      "16/02/03 03:02:29 INFO streaming.PipeMapRed: R/W/S=2000000/0/0 in:1000000=2000000/2 [rec/s] out:0=0/2 [rec/s]\n",
      "16/02/03 03:02:29 INFO streaming.PipeMapRed: R/W/S=2100000/0/0 in:1050000=2100000/2 [rec/s] out:0=0/2 [rec/s]\n",
      "16/02/03 03:02:29 INFO streaming.PipeMapRed: R/W/S=2200000/0/0 in:1100000=2200000/2 [rec/s] out:0=0/2 [rec/s]\n",
      "16/02/03 03:02:30 INFO streaming.PipeMapRed: R/W/S=2300000/0/0 in:1150000=2300000/2 [rec/s] out:0=0/2 [rec/s]\n",
      "16/02/03 03:02:30 INFO streaming.PipeMapRed: R/W/S=2400000/0/0 in:1200000=2400000/2 [rec/s] out:0=0/2 [rec/s]\n",
      "16/02/03 03:02:30 INFO streaming.PipeMapRed: R/W/S=2500000/0/0 in:833333=2500000/3 [rec/s] out:0=0/3 [rec/s]\n",
      "16/02/03 03:02:30 INFO streaming.PipeMapRed: Records R/W=2534061/1\n",
      "16/02/03 03:02:30 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/03 03:02:30 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/03 03:02:30 INFO mapred.Task: Task:attempt_local1800315953_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/02/03 03:02:30 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/03 03:02:30 INFO mapred.Task: Task attempt_local1800315953_0001_r_000000_0 is allowed to commit now\n",
      "16/02/03 03:02:30 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1800315953_0001_r_000000_0' to hdfs://localhost:54310/hw3/3_5_out/_temporary/0/task_local1800315953_0001_r_000000\n",
      "16/02/03 03:02:30 INFO mapred.LocalJobRunner: Records R/W=2534061/1 > reduce\n",
      "16/02/03 03:02:30 INFO mapred.Task: Task 'attempt_local1800315953_0001_r_000000_0' done.\n",
      "16/02/03 03:02:30 INFO mapred.LocalJobRunner: Finishing task: attempt_local1800315953_0001_r_000000_0\n",
      "16/02/03 03:02:30 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/02/03 03:02:31 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/02/03 03:02:31 INFO mapreduce.Job: Job job_local1800315953_0001 completed successfully\n",
      "16/02/03 03:02:31 INFO mapreduce.Job: Counters: 37\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=233345798\n",
      "\t\tFILE: Number of bytes written=292189597\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=6917034\n",
      "\t\tHDFS: Number of bytes written=48031\n",
      "\t\tHDFS: Number of read operations=13\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=31101\n",
      "\t\tMap output records=2534061\n",
      "\t\tMap output bytes=53215277\n",
      "\t\tMap output materialized bytes=58283405\n",
      "\t\tInput split bytes=102\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=877099\n",
      "\t\tReduce shuffle bytes=58283405\n",
      "\t\tReduce input records=2534061\n",
      "\t\tReduce output records=1334\n",
      "\t\tSpilled Records=7602183\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=190\n",
      "\t\tTotal committed heap usage (bytes)=828375040\n",
      "\tCalls\n",
      "\t\tmapper_calls=1\n",
      "\t\treducer_calls=1\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=3458517\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=48031\n",
      "16/02/03 03:02:31 INFO streaming.StreamJob: Output directory: /hw3/3_5_out\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r /hw3/3_5_out\n",
    "\n",
    "!hadoop jar /usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar \\\n",
    "-D stream.map.output.field.separator=\"\\t\" \\\n",
    "-mapper 3_5_mapper.py -reducer 3_5_reducer.py \\\n",
    "-input /hw3/ProductPurchaseData.txt -output /hw3/3_5_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/03 03:02:44 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "ELE20847\tELE26917\t110\t0.0035\t0.0118\n",
      "ELE20847\tGRO73461\t187\t0.0060\t0.0200\n",
      "ELE20847\tFRO92469\t122\t0.0039\t0.0131\n",
      "ELE20847\tGRO85051\t139\t0.0045\t0.0149\n",
      "ELE20847\tSNA80324\t410\t0.0132\t0.0439\n",
      "ELE20847\tFRO75586\t118\t0.0038\t0.0126\n",
      "ELE20847\tSNA96271\t184\t0.0059\t0.0197\n",
      "ELE20847\tFRO40251\t434\t0.0140\t0.0465\n",
      "DAI22896\tGRO21487\t114\t0.0037\t0.0069\n",
      "DAI22896\tGRO38814\t223\t0.0072\t0.0135\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /hw3/3_5_out/part-00000 | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/03 03:03:27 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/03 03:03:28 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /hw3/3_5_out_sorted\n",
      "16/02/03 03:03:29 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/03 03:03:30 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/02/03 03:03:30 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/02/03 03:03:30 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/02/03 03:03:30 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/02/03 03:03:30 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/02/03 03:03:30 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local525946704_0001\n",
      "16/02/03 03:03:30 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/02/03 03:03:30 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/02/03 03:03:30 INFO mapreduce.Job: Running job: job_local525946704_0001\n",
      "16/02/03 03:03:30 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/02/03 03:03:30 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/02/03 03:03:31 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/02/03 03:03:31 INFO mapred.LocalJobRunner: Starting task: attempt_local525946704_0001_m_000000_0\n",
      "16/02/03 03:03:31 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/02/03 03:03:31 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "16/02/03 03:03:31 INFO mapred.MapTask: Processing split: hdfs://localhost:54310/hw3/3_5_out/part-00000:0+48031\n",
      "16/02/03 03:03:31 INFO mapred.MapTask: numReduceTasks: 1\n",
      "16/02/03 03:03:31 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/02/03 03:03:31 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/02/03 03:03:31 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/02/03 03:03:31 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/02/03 03:03:31 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/02/03 03:03:31 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/02/03 03:03:31 INFO streaming.PipeMapRed: PipeMapRed exec [/home/ubuntu/HW3/./3_4_mapper_sort.py]\n",
      "16/02/03 03:03:31 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/02/03 03:03:31 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/02/03 03:03:31 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/02/03 03:03:31 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/02/03 03:03:31 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/02/03 03:03:31 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/02/03 03:03:31 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/02/03 03:03:31 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/02/03 03:03:31 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/02/03 03:03:31 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/02/03 03:03:31 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/02/03 03:03:31 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/02/03 03:03:31 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/03 03:03:31 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/03 03:03:31 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/03 03:03:31 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/03 03:03:31 INFO streaming.PipeMapRed: Records R/W=1334/1\n",
      "16/02/03 03:03:31 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/03 03:03:31 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/03 03:03:31 INFO mapred.LocalJobRunner: \n",
      "16/02/03 03:03:31 INFO mapred.MapTask: Starting flush of map output\n",
      "16/02/03 03:03:31 INFO mapred.MapTask: Spilling map output\n",
      "16/02/03 03:03:31 INFO mapred.MapTask: bufstart = 0; bufend = 52033; bufvoid = 104857600\n",
      "16/02/03 03:03:31 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26203728(104814912); length = 10669/6553600\n",
      "16/02/03 03:03:31 INFO mapred.MapTask: Finished spill 0\n",
      "16/02/03 03:03:31 INFO mapred.Task: Task:attempt_local525946704_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/02/03 03:03:31 INFO mapred.LocalJobRunner: Records R/W=1334/1\n",
      "16/02/03 03:03:31 INFO mapred.Task: Task 'attempt_local525946704_0001_m_000000_0' done.\n",
      "16/02/03 03:03:31 INFO mapred.LocalJobRunner: Finishing task: attempt_local525946704_0001_m_000000_0\n",
      "16/02/03 03:03:31 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/02/03 03:03:31 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/02/03 03:03:31 INFO mapred.LocalJobRunner: Starting task: attempt_local525946704_0001_r_000000_0\n",
      "16/02/03 03:03:31 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/02/03 03:03:31 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "16/02/03 03:03:31 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@68d8270d\n",
      "16/02/03 03:03:31 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=333971456, maxSingleShuffleLimit=83492864, mergeThreshold=220421168, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/03 03:03:31 INFO reduce.EventFetcher: attempt_local525946704_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/03 03:03:31 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local525946704_0001_m_000000_0 decomp: 57371 len: 57375 to MEMORY\n",
      "16/02/03 03:03:31 INFO reduce.InMemoryMapOutput: Read 57371 bytes from map-output for attempt_local525946704_0001_m_000000_0\n",
      "16/02/03 03:03:31 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 57371, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->57371\n",
      "16/02/03 03:03:31 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/03 03:03:31 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/03 03:03:31 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/03 03:03:31 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/03 03:03:31 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 57332 bytes\n",
      "16/02/03 03:03:31 INFO reduce.MergeManagerImpl: Merged 1 segments, 57371 bytes to disk to satisfy reduce memory limit\n",
      "16/02/03 03:03:31 INFO reduce.MergeManagerImpl: Merging 1 files, 57375 bytes from disk\n",
      "16/02/03 03:03:31 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/03 03:03:31 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/03 03:03:31 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 57332 bytes\n",
      "16/02/03 03:03:31 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/03 03:03:31 INFO streaming.PipeMapRed: PipeMapRed exec [/home/ubuntu/HW3/./3_4_reducer_sort.py]\n",
      "16/02/03 03:03:31 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/02/03 03:03:31 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/02/03 03:03:31 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/03 03:03:31 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/03 03:03:31 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/03 03:03:31 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/03 03:03:31 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/03 03:03:31 INFO streaming.PipeMapRed: Records R/W=2668/1\n",
      "16/02/03 03:03:31 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/03 03:03:31 INFO mapred.Task: Task:attempt_local525946704_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/02/03 03:03:31 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/03 03:03:31 INFO mapred.Task: Task attempt_local525946704_0001_r_000000_0 is allowed to commit now\n",
      "16/02/03 03:03:31 INFO output.FileOutputCommitter: Saved output of task 'attempt_local525946704_0001_r_000000_0' to hdfs://localhost:54310/hw3/3_5_out_sorted/_temporary/0/task_local525946704_0001_r_000000\n",
      "16/02/03 03:03:31 INFO mapred.LocalJobRunner: Records R/W=2668/1 > reduce\n",
      "16/02/03 03:03:31 INFO mapred.Task: Task 'attempt_local525946704_0001_r_000000_0' done.\n",
      "16/02/03 03:03:31 INFO mapred.LocalJobRunner: Finishing task: attempt_local525946704_0001_r_000000_0\n",
      "16/02/03 03:03:31 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/02/03 03:03:31 INFO mapreduce.Job: Job job_local525946704_0001 running in uber mode : false\n",
      "16/02/03 03:03:31 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/02/03 03:03:31 INFO mapreduce.Job: Job job_local525946704_0001 completed successfully\n",
      "16/02/03 03:03:31 INFO mapreduce.Job: Counters: 35\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=326904\n",
      "\t\tFILE: Number of bytes written=943393\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=96062\n",
      "\t\tHDFS: Number of bytes written=1957\n",
      "\t\tHDFS: Number of read operations=13\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=1334\n",
      "\t\tMap output records=2668\n",
      "\t\tMap output bytes=52033\n",
      "\t\tMap output materialized bytes=57375\n",
      "\t\tInput split bytes=97\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=1335\n",
      "\t\tReduce shuffle bytes=57375\n",
      "\t\tReduce input records=2668\n",
      "\t\tReduce output records=100\n",
      "\t\tSpilled Records=5336\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=9\n",
      "\t\tTotal committed heap usage (bytes)=597164032\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=48031\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=1957\n",
      "16/02/03 03:03:31 INFO streaming.StreamJob: Output directory: /hw3/3_5_out_sorted\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r /hw3/3_5_out_sorted\n",
    "\n",
    "!hadoop jar /usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar \\\n",
    "-D stream.map.output.field.separator=\"\\t\" \\\n",
    "-D mapreduce.job.output.key.comparator.class=\\\n",
    "org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "-D mapreduce.partition.keycomparator.options=\"-k3,3nr -k1,1 -k2,2\" \\\n",
    "-mapper 3_4_mapper_sort.py -reducer 3_4_reducer_sort.py \\\n",
    "-input /hw3/3_5_out -output /hw3/3_5_out_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/02 05:53:01 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "DAI62779\tELE17451\t1592\t0.0512\t0.0204\t\n",
      "\t\n",
      "FRO40251\tSNA80324\t1412\t0.0454\t0.0489\t\n",
      "\t\n",
      "DAI75645\tFRO40251\t1254\t0.0403\t0.0365\t\n",
      "\t\n",
      "FRO40251\tGRO85051\t1213\t0.0390\t0.0420\t\n",
      "\t\n",
      "DAI62779\tGRO73461\t1139\t0.0366\t0.0146\t\n",
      "\t\n",
      "DAI75645\tSNA80324\t1130\t0.0363\t0.0329\t\n",
      "\t\n",
      "DAI62779\tFRO40251\t1070\t0.0344\t0.0137\t\n",
      "\t\n",
      "DAI62779\tSNA80324\t923\t0.0297\t0.0119\t\n",
      "\t\n",
      "DAI62779\tDAI85309\t918\t0.0295\t0.0118\t\n",
      "\t\n",
      "ELE32164\tGRO59710\t911\t0.0293\t0.0344\t\n",
      "\t\n",
      "DAI62779\tDAI75645\t882\t0.0284\t0.0113\t\n",
      "\t\n",
      "FRO40251\tGRO73461\t882\t0.0284\t0.0305\t\n",
      "\t\n",
      "DAI62779\tELE92920\t877\t0.0282\t0.0113\t\n",
      "\t\n",
      "FRO40251\tFRO92469\t835\t0.0268\t0.0289\t\n",
      "\t\n",
      "DAI62779\tELE32164\t832\t0.0268\t0.0107\t\n",
      "\t\n",
      "DAI75645\tGRO73461\t712\t0.0229\t0.0207\t\n",
      "\t\n",
      "DAI43223\tELE32164\t711\t0.0229\t0.0423\t\n",
      "\t\n",
      "DAI62779\tGRO30386\t709\t0.0228\t0.0091\t\n",
      "\t\n",
      "ELE17451\tFRO40251\t697\t0.0224\t0.0180\t\n",
      "\t\n",
      "DAI85309\tELE99737\t659\t0.0212\t0.0246\t\n",
      "\t\n",
      "DAI62779\tELE26917\t650\t0.0209\t0.0083\t\n",
      "\t\n",
      "GRO21487\tGRO73461\t631\t0.0203\t0.0569\t\n",
      "\t\n",
      "DAI62779\tSNA45677\t604\t0.0194\t0.0078\t\n",
      "\t\n",
      "ELE17451\tSNA80324\t597\t0.0192\t0.0154\t\n",
      "\t\n",
      "DAI62779\tGRO71621\t595\t0.0191\t0.0076\t\n",
      "\t\n",
      "DAI62779\tSNA55762\t593\t0.0191\t0.0076\t\n",
      "\t\n",
      "DAI62779\tDAI83733\t586\t0.0188\t0.0075\t\n",
      "\t\n",
      "ELE17451\tGRO73461\t580\t0.0186\t0.0150\t\n",
      "\t\n",
      "GRO73461\tSNA80324\t562\t0.0181\t0.0470\t\n",
      "\t\n",
      "DAI62779\tGRO59710\t561\t0.0180\t0.0072\t\n",
      "\t\n",
      "DAI62779\tFRO80039\t550\t0.0177\t0.0071\t\n",
      "\t\n",
      "DAI75645\tELE17451\t547\t0.0176\t0.0159\t\n",
      "\t\n",
      "DAI62779\tSNA93860\t537\t0.0173\t0.0069\t\n",
      "\t\n",
      "DAI55148\tDAI62779\t526\t0.0169\t0.0452\t\n",
      "\t\n",
      "DAI43223\tGRO59710\t512\t0.0165\t0.0305\t\n",
      "\t\n",
      "ELE17451\tELE32164\t511\t0.0164\t0.0132\t\n",
      "\t\n",
      "DAI62779\tSNA18336\t506\t0.0163\t0.0065\t\n",
      "\t\n",
      "ELE32164\tGRO73461\t486\t0.0156\t0.0184\t\n",
      "\t\n",
      "DAI62779\tFRO78087\t482\t0.0155\t0.0062\t\n",
      "\t\n",
      "DAI85309\tELE17451\t482\t0.0155\t0.0180\t\n",
      "\t\n",
      "DAI62779\tGRO94758\t479\t0.0154\t0.0062\t\n",
      "\t\n",
      "DAI62779\tGRO21487\t471\t0.0151\t0.0061\t\n",
      "\t\n",
      "GRO85051\tSNA80324\t471\t0.0151\t0.1296\t\n",
      "\t\n",
      "ELE17451\tGRO30386\t468\t0.0150\t0.0121\t\n",
      "\t\n",
      "FRO85978\tSNA95666\t463\t0.0149\t0.0413\t\n",
      "\t\n",
      "DAI62779\tFRO19221\t462\t0.0149\t0.0059\t\n",
      "\t\n",
      "DAI62779\tGRO46854\t461\t0.0148\t0.0059\t\n",
      "\t\n",
      "DAI43223\tDAI62779\t459\t0.0148\t0.0273\t\n",
      "\t\n",
      "ELE92920\tSNA18336\t455\t0.0146\t0.0482\t\n",
      "\t\n",
      "DAI88079\tFRO40251\t446\t0.0143\t0.0839\t\n",
      "\t\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /hw3/3_5_out_sorted/part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This script is Deprecated. Instead use stop-dfs.sh and stop-yarn.sh\n",
      "16/01/29 23:04:10 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Stopping namenodes on [localhost]\n",
      "localhost: no namenode to stop\n",
      "localhost: no datanode to stop\n",
      "Stopping secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: no secondarynamenode to stop\n",
      "16/01/29 23:04:14 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "stopping yarn daemons\n",
      "no resourcemanager to stop\n",
      "localhost: no nodemanager to stop\n",
      "no proxyserver to stop\n"
     ]
    }
   ],
   "source": [
    "# stop hadoop\n",
    "!/usr/local/hadoop/sbin/stop-all.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATASCI W261: Machine Learning at Scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Group:** 10 <br>\n",
    "**Names:** <br>\n",
    "Prabhakar Gundugola, <br>\n",
    "Yi Jin, <br>\n",
    "Jaime Villalpando <br><br>\n",
    "**Emails:** <br>\n",
    "prabhakar@berkeley.edu, <br>\n",
    "yjin@ischool.berkeley.edu, <br>\n",
    "jaimegvl@ischool.berkeley.edu <br><br>\n",
    "**Time of Initial Submission:** Feb 2, 2016 <br>\n",
    "**Week 3:** Homework 3 <br>\n",
    "**Date:** February 4, 2016 <br>\n",
    "**Time of Submission:** 00:10 AM PT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW3.0\n",
    "#### What is a merge sort? \n",
    "Merge sort is an efficient, general-purpose, comparison based sorting algorithm for rearranging lists into a specified order.\n",
    "\n",
    "![Mergesort algorithm](mergesort.PNG)\n",
    "\n",
    "Mergesort works as follows:\n",
    "- Divide the unsorted list into n sublists, each containing only 1 element.\n",
    "- Merge sublists repeatedly into sorted sublists until there is only 1 sublist remaining. \n",
    "\n",
    "**Where is it used in Hadoop?**\n",
    "Mergesort is used in sort and shuffle phase of hadoop between Map and Reduce phases.\n",
    "\n",
    "#### How is  a combiner function in the context of Hadoop?\n",
    "A combiner, also known as a semi-reducer, accepts the inputs from the Map procedure and thereafter passes the output of key,value pairs to the Reduce procedure.\n",
    "\n",
    "It is used in between Map and Reduce procedures to reduce the volume of data transfer between Map and Reduce when the output of Map phase is very large.\n",
    "\n",
    "![Combiner - Multiple reducers](combiner.PNG)\n",
    "\n",
    "#### Give an example where it can be used and justify why it should be used in the context of this problem.\n",
    "An example where a combiner is required is word count in large number of documents. A map emits a (key, value) pair with (word, 1) for each and every word in the document. The output of Map phase is very large and to reduce the volume of data transfer to reduce phase, we need a combiner that aggregates the values by key.\n",
    "\n",
    "#### What is the Hadoop shuffle?\n",
    "Hadoop shuffle is the process of transferring data from mappers to reducers based on a partitioning function. It sorts and combines all the data based on a partitioning key and ensures that all the (key, value) pairs of the same key are sent to the same reducer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 3.1. Use Counters to do EDA (exploratory data analysis and to monitor progress)\n",
    "Counters are lightweight objects in Hadoop that allow you to keep track of system progress in both the map and reduce stages of processing. By default, Hadoop defines a number of standard counters in \"groups\"; these show up in the jobtracker webapp, giving you information such as \"Map input records\", \"Map output records\", etc. \n",
    "\n",
    "While processing information/data using MapReduce job, it is a challenge to monitor the progress of parallel threads running across nodes of distributed clusters. Moreover, it is also complicated to distinguish between the data that has been processed and the data which is yet to be processed. The MapReduce Framework offers a provision of user-defined Counters, which can be effectively utilized to monitor the progress of data across nodes of distributed clusters.\n",
    "\n",
    "Use the Consumer Complaints  Dataset provide here to complete this question:\n",
    "\n",
    "     https://www.dropbox.com/s/vbalm3yva2rr86m/Consumer_Complaints.csv?dl=0\n",
    "\n",
    "The consumer complaints dataset consists of diverse consumer complaints, which have been reported across the United States regarding various types of loans. The dataset consists of records of the form:\n",
    "\n",
    "Complaint ID,Product,Sub-product,Issue,Sub-issue,State,ZIP code,Submitted via,Date received,Date sent to company,Company,Company response,Timely response?,Consumer disputed?\n",
    "\n",
    "Here’s is the first few lines of the  of the Consumer Complaints  Dataset:\n",
    "\n",
    "Complaint ID,Product,Sub-product,Issue,Sub-issue,State,ZIP code,Submitted via,Date received,Date sent to company,Company,Company response,Timely response?,Consumer disputed?\n",
    "1114245,Debt collection,Medical,Disclosure verification of debt,Not given enough info to verify debt,FL,32219,Web,11/13/2014,11/13/2014,\"Choice Recovery, Inc.\",Closed with explanation,Yes,\n",
    "1114488,Debt collection,Medical,Disclosure verification of debt,Right to dispute notice not received,TX,75006,Web,11/13/2014,11/13/2014,\"Expert Global Solutions, Inc.\",In progress,Yes,\n",
    "1114255,Bank account or service,Checking account,Deposits and withdrawals,,NY,11102,Web,11/13/2014,11/13/2014,\"FNIS (Fidelity National Information Services, Inc.)\",In progress,Yes,\n",
    "1115106,Debt collection,\"Other (phone, health club, etc.)\",Communication tactics,Frequent or repeated calls,GA,31721,Web,11/13/2014,11/13/2014,\"Expert Global Solutions, Inc.\",In progress,Yes,\n",
    "\n",
    "User-defined Counters\n",
    "\n",
    "Now, let’s use Hadoop Counters to identify the number of complaints pertaining to debt collection, mortgage and other categories (all other categories get lumped into this one) in the consumer complaints dataset. Basically produce the distribution of the Product column in this dataset using counters (limited to 3 counters here).\n",
    "\n",
    "Hadoop offers Job Tracker, an UI tool to determine the status and statistics of all jobs. Using the job tracker UI, developers can view the Counters that have been created. Screenshot your  job tracker UI as your job completes and include it here. Make sure that your user defined counters are visible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper31.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper31.py\n",
    "#!/usr/bin/python\n",
    "## mapper31.py\n",
    "## Author: Prabhakar Gundugola\n",
    "## Description: mapper code for HW3.1\n",
    "\n",
    "import sys\n",
    "\n",
    "for line in sys.stdin:\n",
    "    tokens = line.strip().split(\",\")\n",
    "    \n",
    "    # Skip the Header\n",
    "    if tokens[0] == 'Complaint ID':\n",
    "        continue\n",
    "    \n",
    "    product = 'none'\n",
    "    if 'Debt' in tokens[1]:\n",
    "        product = 'debt'\n",
    "    elif 'Mortgage' in tokens[1]:\n",
    "        product = 'mortgage'\n",
    "    else:\n",
    "        product = 'others'\n",
    "    \n",
    "    sys.stderr.write(\"reporter:counter:MapperTokens,\" + product + ',1\\n')\n",
    "    print product + '\\t' + str(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer31.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer31.py\n",
    "#!/usr/bin/python\n",
    "## reducer31.py\n",
    "## Author: Prabhakar Gundugola\n",
    "## Description: reducer code for HW3.1\n",
    "\n",
    "import sys\n",
    "\n",
    "prev_word = None\n",
    "counts = 0\n",
    "for line in sys.stdin:\n",
    "    word, value = line.strip().split('\\t')\n",
    "    \n",
    "    if prev_word != word:\n",
    "        if prev_word is not None:\n",
    "            print prev_word + '\\t' + str(counts)\n",
    "            sys.stderr.write('reporter:counter:ReducerTokens,' \n",
    "                             + prev_word + ',' + str(counts) + '\\n')\n",
    "        \n",
    "        prev_word = word\n",
    "        counts = 0\n",
    "    counts += 1\n",
    "\n",
    "print prev_word + '\\t' + str(counts)\n",
    "sys.stderr.write('reporter:counter:ReducerTokens,' + prev_word + ',' + str(counts) + '\\n')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x mapper31.py\n",
    "!chmod a+x reducer31.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/31 23:17:41 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/root/wk3/hw31\n"
     ]
    }
   ],
   "source": [
    "# Ensure hw31 folder doesn't exist\n",
    "!hdfs dfs -rm -r /user/root/wk3/hw31\n",
    "\n",
    "# Create HDFS input and src folder\n",
    "!hdfs dfs -mkdir -p /user/root/wk3/hw31/input\n",
    "\n",
    "# Copy the input file, mapper.py, reducer.py\n",
    "!hdfs dfs -put Consumer_Complaints.csv /user/root/wk3/hw31/input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run Hadoop Streaming job**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `/user/root/wk3/hw31/output': No such file or directory\n",
      "packageJobJar: [/tmp/hadoop-unjar5817919191444807337/] [] /tmp/streamjob5060557907352090611.jar tmpDir=null\n",
      "16/01/31 23:17:53 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/01/31 23:17:54 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/01/31 23:17:54 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/01/31 23:17:54 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/01/31 23:17:54 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1454301000890_0001\n",
      "16/01/31 23:17:55 INFO impl.YarnClientImpl: Submitted application application_1454301000890_0001\n",
      "16/01/31 23:17:55 INFO mapreduce.Job: The url to track the job: http://prabhakar:8088/proxy/application_1454301000890_0001/\n",
      "16/01/31 23:17:55 INFO mapreduce.Job: Running job: job_1454301000890_0001\n",
      "16/01/31 23:18:03 INFO mapreduce.Job: Job job_1454301000890_0001 running in uber mode : false\n",
      "16/01/31 23:18:03 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/01/31 23:18:11 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/01/31 23:18:18 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/01/31 23:18:18 INFO mapreduce.Job: Job job_1454301000890_0001 completed successfully\n",
      "16/01/31 23:18:18 INFO mapreduce.Job: Counters: 55\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=3604798\n",
      "\t\tFILE: Number of bytes written=7562077\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=50910129\n",
      "\t\tHDFS: Number of bytes written=41\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=11800\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=4541\n",
      "\t\tTotal time spent by all map tasks (ms)=11800\n",
      "\t\tTotal time spent by all reduce tasks (ms)=4541\n",
      "\t\tTotal vcore-seconds taken by all map tasks=11800\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=4541\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=12083200\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=4649984\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=312913\n",
      "\t\tMap output records=312912\n",
      "\t\tMap output bytes=2978968\n",
      "\t\tMap output materialized bytes=3604804\n",
      "\t\tInput split bytes=246\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=3\n",
      "\t\tReduce shuffle bytes=3604804\n",
      "\t\tReduce input records=312912\n",
      "\t\tReduce output records=3\n",
      "\t\tSpilled Records=625824\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=248\n",
      "\t\tCPU time spent (ms)=9170\n",
      "\t\tPhysical memory (bytes) snapshot=690049024\n",
      "\t\tVirtual memory (bytes) snapshot=2528423936\n",
      "\t\tTotal committed heap usage (bytes)=598212608\n",
      "\tMapperTokens\n",
      "\t\tdebt=44372\n",
      "\t\tmortgage=125752\n",
      "\t\tothers=142788\n",
      "\tReducerTokens\n",
      "\t\tdebt=44372\n",
      "\t\tmortgage=125752\n",
      "\t\tothers=142788\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=50909883\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=41\n",
      "16/01/31 23:18:18 INFO streaming.StreamJob: Output directory: /user/root/wk3/hw31/output\n"
     ]
    }
   ],
   "source": [
    "# Ensure output folder doesn't exist\n",
    "!hdfs dfs -rm -r /user/root/wk3/hw31/output\n",
    "\n",
    "# Run Hadoop Streaming job\n",
    "!hadoop jar hadoop-streaming-2.7.1.jar \\\n",
    "-mapper /root/hw3/mapper31.py \\\n",
    "-reducer /root/hw3/reducer31.py \\\n",
    "-input /user/root/wk3/hw31/input \\\n",
    "-output /user/root/wk3/hw31/output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**As shown in the output:**\n",
    "- debt=44372\n",
    "- mortgage=125752\n",
    "- others=142788"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# HW 3.2. Analyze the performance of your Mappers, Combiners and Reducers using Counters\n",
    "\n",
    "a) For this brief study the Input file will be one record (the next line only): \n",
    "foo foo quux labs foo bar quux\n",
    "\n",
    "Perform a word count analysis of this single record dataset using a Mapper and Reducer based WordCount (i.e., no combiners are used here) using user defined Counters to count up how many time the mapper and reducer are called. What is the value of your user defined Mapper Counter, and Reducer Counter after completing this word count job. The answer  should be 1 and 4 respectively. Please explain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!echo \"foo foo quux labs foo bar quux\" > input_data.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper32a.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper32a.py\n",
    "#!/usr/bin/python\n",
    "## mapper32a.py\n",
    "## Author: Prabhakar Gundugola\n",
    "## Description: mapper code for HW3.2\n",
    "import sys\n",
    "\n",
    "sys.stderr.write('reporter:counter:mapper,Mapper,1\\n')\n",
    "\n",
    "for line in sys.stdin:\n",
    "    words = line.strip().split()\n",
    "    \n",
    "    for word in words:\n",
    "        print word + '\\t' + str(1)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer32a.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer32a.py\n",
    "#!/usr/bin/python\n",
    "## reducer32a.py\n",
    "## Author: Prabhakar Gundugola\n",
    "## Description: reducer code for HW3.2\n",
    "import sys\n",
    "\n",
    "for line in sys.stdin:\n",
    "    print line\n",
    "    \n",
    "sys.stderr.write('reporter:counter:mapper,Reducer,1\\n')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x mapper32a.py\n",
    "!chmod a+x reducer32a.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/31 14:27:35 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\r\n",
      "Deleted /user/root/wk3/hw32a\r\n"
     ]
    }
   ],
   "source": [
    "# Ensure the input folder doesn't exist\n",
    "!hdfs dfs -rm -r /user/root/wk3/hw32a\n",
    "\n",
    "# Create HDFS directory for input folder\n",
    "!hdfs dfs -mkdir -p /user/root/wk3/hw32a/input\n",
    "\n",
    "# Copy input data \n",
    "!hdfs dfs -put input_data.txt /user/root/wk3/hw32a/input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `/user/root/wk3/hw32a/output': No such file or directory\n",
      "packageJobJar: [/tmp/hadoop-unjar2188181261195739559/] [] /tmp/streamjob9135154725253275562.jar tmpDir=null\n",
      "16/01/31 14:27:47 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/01/31 14:27:47 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/01/31 14:27:48 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/01/31 14:27:48 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/01/31 14:27:48 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "16/01/31 14:27:48 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/01/31 14:27:48 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1454270249092_0011\n",
      "16/01/31 14:27:48 INFO impl.YarnClientImpl: Submitted application application_1454270249092_0011\n",
      "16/01/31 14:27:48 INFO mapreduce.Job: The url to track the job: http://prabhakar:8088/proxy/application_1454270249092_0011/\n",
      "16/01/31 14:27:48 INFO mapreduce.Job: Running job: job_1454270249092_0011\n",
      "16/01/31 14:27:54 INFO mapreduce.Job: Job job_1454270249092_0011 running in uber mode : false\n",
      "16/01/31 14:27:54 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/01/31 14:28:00 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/01/31 14:28:07 INFO mapreduce.Job:  map 100% reduce 25%\n",
      "16/01/31 14:28:09 INFO mapreduce.Job:  map 100% reduce 50%\n",
      "16/01/31 14:28:10 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/01/31 14:28:10 INFO mapreduce.Job: Job job_1454270249092_0011 completed successfully\n",
      "16/01/31 14:28:10 INFO mapreduce.Job: Counters: 51\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=83\n",
      "\t\tFILE: Number of bytes written=587583\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=146\n",
      "\t\tHDFS: Number of bytes written=59\n",
      "\t\tHDFS: Number of read operations=15\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=8\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=1\n",
      "\t\tLaunched reduce tasks=4\n",
      "\t\tData-local map tasks=1\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=3296\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=20191\n",
      "\t\tTotal time spent by all map tasks (ms)=3296\n",
      "\t\tTotal time spent by all reduce tasks (ms)=20191\n",
      "\t\tTotal vcore-seconds taken by all map tasks=3296\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=20191\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=3375104\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=20675584\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=1\n",
      "\t\tMap output records=7\n",
      "\t\tMap output bytes=45\n",
      "\t\tMap output materialized bytes=83\n",
      "\t\tInput split bytes=115\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=4\n",
      "\t\tReduce shuffle bytes=83\n",
      "\t\tReduce input records=7\n",
      "\t\tReduce output records=14\n",
      "\t\tSpilled Records=14\n",
      "\t\tShuffled Maps =4\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=4\n",
      "\t\tGC time elapsed (ms)=402\n",
      "\t\tCPU time spent (ms)=5090\n",
      "\t\tPhysical memory (bytes) snapshot=938647552\n",
      "\t\tVirtual memory (bytes) snapshot=4204924928\n",
      "\t\tTotal committed heap usage (bytes)=1006632960\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tmapper\n",
      "\t\tMapper=1\n",
      "\t\tReducer=4\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=31\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=59\n",
      "16/01/31 14:28:10 INFO streaming.StreamJob: Output directory: /user/root/wk3/hw32a/output\n"
     ]
    }
   ],
   "source": [
    "# Ensure the output folder doesn't exist\n",
    "!hdfs dfs -rm -r /user/root/wk3/hw32a/output\n",
    "\n",
    "# Run Hadoop Streaming job\n",
    "!hadoop jar hadoop-streaming-2.7.1.jar \\\n",
    "-D mapred.map.tasks=1 \\\n",
    "-D mapred.reduce.tasks=4 \\\n",
    "-mapper /root/hw3/mapper32a.py \\\n",
    "-reducer /root/hw3/reducer32a.py \\\n",
    "-input /user/root/wk3/hw32a/input \\\n",
    "-output /user/root/wk3/hw32a/output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mapper Counter:** 1 <br>\n",
    "**Reducer Counter:** 4\n",
    "\n",
    "The default output counters for mapper and reducer are 2 and 1 when I didn't pass the properties for mapper and reducer in Hadoop Streaming command. I had to explicitly set the counters for mapper and reducer as 1 and 4 to produce the output counters 1 and 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b) Please use mulitple mappers and reducers for these jobs (at least 2 mappers and 2 reducers).** <br>\n",
    "Perform a word count analysis of the Issue column of the Consumer Complaints  Dataset using a Mapper and Reducer based WordCount (i.e., no combiners used anywhere)  using user defined Counters to count up how many time the mapper and reducer are called. What is the value of your user defined Mapper Counter, and Reducer Counter after completing your word count job. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper32b.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper32b.py\n",
    "#!/usr/bin/python\n",
    "## mapper32b.py\n",
    "## Author: Prabhakar Gundugola\n",
    "## Description: mapper code for HW3.2b\n",
    "import sys\n",
    "import string\n",
    "\n",
    "sys.stderr.write('reporter:counter:mapper32b,Mapper,1\\n')\n",
    "total_words = 0\n",
    "for line in sys.stdin:\n",
    "    tokens = line.strip().split(\",\")\n",
    "    if 'Complaint' in tokens[0]:\n",
    "        continue\n",
    "    \n",
    "    word_string = tokens[3].replace(',', ' ').replace('/', ' ').replace('\"', '')\n",
    "    for word in word_string.lower().split():\n",
    "        total_words += 1\n",
    "        print word + '\\t' + str(1)\n",
    "print '0000TOTALWORDS' + '\\t' + str(total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer32b.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer32b.py\n",
    "#!/usr/bin/python\n",
    "## reducer32b.py\n",
    "## Author: Prabhakar Gundugola\n",
    "## Description: reducer code for HW3.2b\n",
    "import sys\n",
    "\n",
    "sys.stderr.write('reporter:counter:reducer32b,Reducer,1\\n')\n",
    "prev_word = None\n",
    "counts = 0\n",
    "\n",
    "for line in sys.stdin:\n",
    "    word, value = line.strip().split('\\t')\n",
    "    \n",
    "    if prev_word != word:\n",
    "        if prev_word is not None:\n",
    "            print prev_word + '\\t' + str(counts)\n",
    "        prev_word = word\n",
    "        counts = 0\n",
    "    counts += eval(value)\n",
    "print prev_word + '\\t' + str(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x mapper32b.py\n",
    "!chmod a+x reducer32b.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/31 14:29:01 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\r\n",
      "Deleted /user/root/wk3/hw32b\r\n"
     ]
    }
   ],
   "source": [
    "# Ensure the input folder doesn't exist\n",
    "!hdfs dfs -rm -r /user/root/wk3/hw32b\n",
    "\n",
    "# Create Input folder\n",
    "!hdfs dfs -mkdir -p /user/root/wk3/hw32b/input\n",
    "\n",
    "# Copy the input file to input folder\n",
    "!hdfs dfs -put Consumer_Complaints.csv /user/root/wk3/hw32b/input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/31 15:07:28 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/root/wk3/hw32b/output\n",
      "packageJobJar: [/tmp/hadoop-unjar3847755633682334463/] [] /tmp/streamjob5197053891155922287.jar tmpDir=null\n",
      "16/01/31 15:07:31 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/01/31 15:07:31 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/01/31 15:07:32 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/01/31 15:07:32 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/01/31 15:07:32 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "16/01/31 15:07:32 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1454270249092_0022\n",
      "16/01/31 15:07:32 INFO impl.YarnClientImpl: Submitted application application_1454270249092_0022\n",
      "16/01/31 15:07:32 INFO mapreduce.Job: The url to track the job: http://prabhakar:8088/proxy/application_1454270249092_0022/\n",
      "16/01/31 15:07:32 INFO mapreduce.Job: Running job: job_1454270249092_0022\n",
      "16/01/31 15:07:38 INFO mapreduce.Job: Job job_1454270249092_0022 running in uber mode : false\n",
      "16/01/31 15:07:38 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/01/31 15:07:46 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/01/31 15:07:56 INFO mapreduce.Job:  map 100% reduce 25%\n",
      "16/01/31 15:07:59 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/01/31 15:07:59 INFO mapreduce.Job: Job job_1454270249092_0022 completed successfully\n",
      "16/01/31 15:07:59 INFO mapreduce.Job: Counters: 51\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=11233537\n",
      "\t\tFILE: Number of bytes written=23172100\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=50910131\n",
      "\t\tHDFS: Number of bytes written=2113\n",
      "\t\tHDFS: Number of read operations=18\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=8\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=4\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=11378\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=32035\n",
      "\t\tTotal time spent by all map tasks (ms)=11378\n",
      "\t\tTotal time spent by all reduce tasks (ms)=32035\n",
      "\t\tTotal vcore-seconds taken by all map tasks=11378\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=32035\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=11651072\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=32803840\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=312913\n",
      "\t\tMap output records=980484\n",
      "\t\tMap output bytes=9272545\n",
      "\t\tMap output materialized bytes=11233561\n",
      "\t\tInput split bytes=248\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=170\n",
      "\t\tReduce shuffle bytes=11233561\n",
      "\t\tReduce input records=980484\n",
      "\t\tReduce output records=170\n",
      "\t\tSpilled Records=1960968\n",
      "\t\tShuffled Maps =8\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=8\n",
      "\t\tGC time elapsed (ms)=527\n",
      "\t\tCPU time spent (ms)=14760\n",
      "\t\tPhysical memory (bytes) snapshot=1190985728\n",
      "\t\tVirtual memory (bytes) snapshot=5073149952\n",
      "\t\tTotal committed heap usage (bytes)=1204813824\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tmapper32b\n",
      "\t\tMapper=2\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=50909883\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=2113\n",
      "\treducer32b\n",
      "\t\tReducer=4\n",
      "16/01/31 15:07:59 INFO streaming.StreamJob: Output directory: /user/root/wk3/hw32b/output\n"
     ]
    }
   ],
   "source": [
    "# Ensure the output folder doesn't exist\n",
    "!hdfs dfs -rm -r /user/root/wk3/hw32b/output\n",
    "\n",
    "\n",
    "# Run Hadoop Streaming job\n",
    "!hadoop jar hadoop-streaming-2.7.1.jar \\\n",
    "-D mapred.reduce.tasks=4 \\\n",
    "-mapper /root/hw3/mapper32b.py \\\n",
    "-reducer /root/hw3/reducer32b.py \\\n",
    "-input /user/root/wk3/hw32b/input \\\n",
    "-output /user/root/wk3/hw32b/output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mapper Counter:** 2 <br>\n",
    "**Reducer Counter:** 4\n",
    "\n",
    "The default output counters for mapper and reducer are 2 and 1. I had to explicitly set the counter for reducer to 4 to produce the output counters 2 and 4 for mapper and reducer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c) Perform a word count analysis of the Issue column of the Consumer Complaints  Dataset using a Mapper, Reducer, and standalone combiner (i.e., not an in-memory combiner) based WordCount using user defined Counters to count up how many time the mapper, combiner, reducer are called.** <br> \n",
    "What is the value of your user defined Mapper Counter, and Reducer Counter after completing your word count job. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting combiner32c.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile combiner32c.py\n",
    "#!/usr/bin/python\n",
    "## combiner32c.py\n",
    "## Author: Prabhakar Gundugola\n",
    "## Description: combiner code for HW3.2c\n",
    "import sys\n",
    "\n",
    "sys.stderr.write('reporter:counter:combiner32c,Combiner,1\\n')\n",
    "prev_word = None\n",
    "counts = 0\n",
    "\n",
    "for line in sys.stdin:\n",
    "    word, value = line.strip().split('\\t')\n",
    "    \n",
    "    if prev_word != word:\n",
    "        if prev_word is not None:\n",
    "            print prev_word + '\\t' + str(counts)\n",
    "        prev_word = word\n",
    "        counts = 0\n",
    "    counts += eval(value)\n",
    "print prev_word + '\\t' + str(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x combiner32c.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/31 14:10:43 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\r\n",
      "Deleted /user/root/wk3/hw32c\r\n"
     ]
    }
   ],
   "source": [
    "# Ensure the input folder doesn't exist\n",
    "!hdfs dfs -rm -r /user/root/wk3/hw32c\n",
    "\n",
    "# Create Input folder\n",
    "!hdfs dfs -mkdir -p /user/root/wk3/hw32c/input\n",
    "\n",
    "# Copy the input file to input folder\n",
    "!hdfs dfs -put Consumer_Complaints.csv /user/root/wk3/hw32c/input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/31 14:31:51 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/root/wk3/hw32c/output\n",
      "packageJobJar: [/tmp/hadoop-unjar7939414701922345542/] [] /tmp/streamjob8041904735065720818.jar tmpDir=null\n",
      "16/01/31 14:31:53 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/01/31 14:31:54 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/01/31 14:31:54 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/01/31 14:31:54 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/01/31 14:31:54 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "16/01/31 14:31:54 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1454270249092_0014\n",
      "16/01/31 14:31:54 INFO impl.YarnClientImpl: Submitted application application_1454270249092_0014\n",
      "16/01/31 14:31:55 INFO mapreduce.Job: The url to track the job: http://prabhakar:8088/proxy/application_1454270249092_0014/\n",
      "16/01/31 14:31:55 INFO mapreduce.Job: Running job: job_1454270249092_0014\n",
      "16/01/31 14:32:01 INFO mapreduce.Job: Job job_1454270249092_0014 running in uber mode : false\n",
      "16/01/31 14:32:01 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/01/31 14:32:12 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/01/31 14:32:18 INFO mapreduce.Job:  map 100% reduce 25%\n",
      "16/01/31 14:32:20 INFO mapreduce.Job:  map 100% reduce 50%\n",
      "16/01/31 14:32:21 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/01/31 14:32:22 INFO mapreduce.Job: Job job_1454270249092_0014 completed successfully\n",
      "16/01/31 14:32:22 INFO mapreduce.Job: Counters: 52\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=4525\n",
      "\t\tFILE: Number of bytes written=716098\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=50910131\n",
      "\t\tHDFS: Number of bytes written=2128\n",
      "\t\tHDFS: Number of read operations=18\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=8\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=4\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=17664\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=19239\n",
      "\t\tTotal time spent by all map tasks (ms)=17664\n",
      "\t\tTotal time spent by all reduce tasks (ms)=19239\n",
      "\t\tTotal vcore-seconds taken by all map tasks=17664\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=19239\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=18087936\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=19700736\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=312913\n",
      "\t\tMap output records=966249\n",
      "\t\tMap output bytes=9210210\n",
      "\t\tMap output materialized bytes=4549\n",
      "\t\tInput split bytes=248\n",
      "\t\tCombine input records=966249\n",
      "\t\tCombine output records=311\n",
      "\t\tReduce input groups=168\n",
      "\t\tReduce shuffle bytes=4549\n",
      "\t\tReduce input records=311\n",
      "\t\tReduce output records=168\n",
      "\t\tSpilled Records=622\n",
      "\t\tShuffled Maps =8\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=8\n",
      "\t\tGC time elapsed (ms)=448\n",
      "\t\tCPU time spent (ms)=10780\n",
      "\t\tPhysical memory (bytes) snapshot=1188081664\n",
      "\t\tVirtual memory (bytes) snapshot=5059936256\n",
      "\t\tTotal committed heap usage (bytes)=1207959552\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tcombiner32c\n",
      "\t\tCombiner=8\n",
      "\tmapper32b\n",
      "\t\tMapper=2\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=50909883\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=2128\n",
      "\treducer32b\n",
      "\t\tReducer=4\n",
      "16/01/31 14:32:22 INFO streaming.StreamJob: Output directory: /user/root/wk3/hw32c/output\n"
     ]
    }
   ],
   "source": [
    "# Ensure the output folder doesn't exist\n",
    "!hdfs dfs -rm -r /user/root/wk3/hw32c/output\n",
    "\n",
    "\n",
    "# Run Hadoop Streaming job. \n",
    "!hadoop jar hadoop-streaming-2.7.1.jar \\\n",
    "-D mapred.reduce.tasks=4 \\\n",
    "-mapper /root/hw3/mapper32b.py \\\n",
    "-combiner /root/hw3/combiner32c.py \\\n",
    "-reducer /root/hw3/reducer32b.py \\\n",
    "-input /user/root/wk3/hw32c/input \\\n",
    "-output /user/root/wk3/hw32c/output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The counters produced by Hadoop Mapreduce job are:\n",
    "- Mapper - 2\n",
    "- Combiner - 8\n",
    "- Reducer - 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using a single reducer: What are the top 50 most frequent terms in your word count analysis?** <br> \n",
    "Present the top 50 terms and their frequency and their relative frequency. Present the top 50 terms and their frequency and their relative frequency. If there are ties please sort the tokens in alphanumeric/string order. Present bottom 10 tokens (least frequent items). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper32d.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper32d.py\n",
    "#!/usr/bin/python\n",
    "## mapper32d.py\n",
    "## Author: Prabhakar Gundugola\n",
    "## Description: mapper code for HW3.2d that takes the output of HW3.2c as input\n",
    "import sys\n",
    "\n",
    "sys.stderr.write('reporter:counter:mapper,Mapper32d,1\\n')\n",
    "\n",
    "for line in sys.stdin:\n",
    "    word, value = line.strip().split('\\t')\n",
    "    print value + '\\t' + word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer32d.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer32d.py\n",
    "#!/usr/bin/python\n",
    "## reducer32d.py\n",
    "## Author: Prabhakar Gundugola\n",
    "## Description: reducer code for HW3.2d\n",
    "import sys\n",
    "\n",
    "sys.stderr.write('reporter:counter:reducer,Reducer32d,1\\n')\n",
    "\n",
    "total = 0\n",
    "for line in sys.stdin:\n",
    "    value, word = line.strip().split('\\t')\n",
    "    # First word should be 0000TOTALWORDS\n",
    "    if word == '0000TOTALWORDS':\n",
    "        total = int(value)        \n",
    "    else:\n",
    "        term_freq = 100.0 * int(value)/total\n",
    "        print word.ljust(20) + '\\t' + value + '\\t' + str(round(term_freq,4)) + '%'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x mapper32d.py\n",
    "!chmod a+x reducer32d.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/31 14:59:47 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/root/wk3/hw32d\n"
     ]
    }
   ],
   "source": [
    "# Ensure the input folder doesn't exist\n",
    "!hdfs dfs -rm -r /user/root/wk3/hw32d\n",
    "\n",
    "# Create Input folder\n",
    "!hdfs dfs -mkdir -p /user/root/wk3/hw32d/input\n",
    "\n",
    "# Copy the input file to input folder\n",
    "!hdfs dfs -put Consumer_Complaints.csv /user/root/wk3/hw32d/input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/31 16:52:54 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/root/wk3/hw32d/output\n",
      "packageJobJar: [/tmp/hadoop-unjar6767512836284184782/] [] /tmp/streamjob1590576393051470913.jar tmpDir=null\n",
      "16/01/31 16:52:57 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/01/31 16:52:57 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/01/31 16:52:58 INFO mapred.FileInputFormat: Total input paths to process : 4\n",
      "16/01/31 16:52:58 INFO mapreduce.JobSubmitter: number of splits:4\n",
      "16/01/31 16:52:58 INFO Configuration.deprecation: mapred.output.key.comparator.class is deprecated. Instead, use mapreduce.job.output.key.comparator.class\n",
      "16/01/31 16:52:58 INFO Configuration.deprecation: mapred.text.key.comparator.options is deprecated. Instead, use mapreduce.partition.keycomparator.options\n",
      "16/01/31 16:52:58 INFO Configuration.deprecation: mapred.text.key.partitioner.options is deprecated. Instead, use mapreduce.partition.keypartitioner.options\n",
      "16/01/31 16:52:58 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1454270249092_0045\n",
      "16/01/31 16:52:58 INFO impl.YarnClientImpl: Submitted application application_1454270249092_0045\n",
      "16/01/31 16:52:58 INFO mapreduce.Job: The url to track the job: http://prabhakar:8088/proxy/application_1454270249092_0045/\n",
      "16/01/31 16:52:58 INFO mapreduce.Job: Running job: job_1454270249092_0045\n",
      "16/01/31 16:53:04 INFO mapreduce.Job: Job job_1454270249092_0045 running in uber mode : false\n",
      "16/01/31 16:53:04 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/01/31 16:53:12 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/01/31 16:53:19 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/01/31 16:53:19 INFO mapreduce.Job: Job job_1454270249092_0045 completed successfully\n",
      "16/01/31 16:53:20 INFO mapreduce.Job: Counters: 51\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=2629\n",
      "\t\tFILE: Number of bytes written=595976\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=2561\n",
      "\t\tHDFS: Number of bytes written=5694\n",
      "\t\tHDFS: Number of read operations=15\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=4\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=4\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=24043\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=3360\n",
      "\t\tTotal time spent by all map tasks (ms)=24043\n",
      "\t\tTotal time spent by all reduce tasks (ms)=3360\n",
      "\t\tTotal vcore-seconds taken by all map tasks=24043\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=3360\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=24620032\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=3440640\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=170\n",
      "\t\tMap output records=170\n",
      "\t\tMap output bytes=2283\n",
      "\t\tMap output materialized bytes=2647\n",
      "\t\tInput split bytes=448\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=170\n",
      "\t\tReduce shuffle bytes=2647\n",
      "\t\tReduce input records=170\n",
      "\t\tReduce output records=169\n",
      "\t\tSpilled Records=340\n",
      "\t\tShuffled Maps =4\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=4\n",
      "\t\tGC time elapsed (ms)=314\n",
      "\t\tCPU time spent (ms)=3940\n",
      "\t\tPhysical memory (bytes) snapshot=1191534592\n",
      "\t\tVirtual memory (bytes) snapshot=4193255424\n",
      "\t\tTotal committed heap usage (bytes)=1006632960\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tmapper\n",
      "\t\tMapper32d=4\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=2113\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=5694\n",
      "\treducer\n",
      "\t\tReducer32d=1\n",
      "16/01/31 16:53:20 INFO streaming.StreamJob: Output directory: /user/root/wk3/hw32d/output\n"
     ]
    }
   ],
   "source": [
    "# Ensure the output folder doesn't exist\n",
    "!hdfs dfs -rm -r /user/root/wk3/hw32d/output\n",
    "\n",
    "\n",
    "# Run Hadoop Streaming job. \n",
    "!hadoop jar hadoop-streaming-2.7.1.jar \\\n",
    "-D mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "-D mapred.text.key.partitioner.options=-k1,1 \\\n",
    "-D stream.num.map.output.key.fields=2 \\\n",
    "-D mapred.text.key.comparator.options='-k1,1nr -k2,2n' \\\n",
    "-mapper /root/hw3/mapper32d.py \\\n",
    "-reducer /root/hw3/reducer32d.py \\\n",
    "-input /user/root/wk3/hw32b/output/part* \\\n",
    "-output /user/root/wk3/hw32d/output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apply               \t118\t0.012%\n",
      "amount              \t98\t0.01%\n",
      "credited            \t92\t0.0094%\n",
      "payment             \t92\t0.0094%\n",
      "convenience         \t75\t0.0076%\n",
      "checks              \t75\t0.0076%\n",
      "amt                 \t71\t0.0072%\n",
      "day                 \t71\t0.0072%\n",
      "disclosures         \t64\t0.0065%\n",
      "missing             \t64\t0.0065%\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -tail /user/root/wk3/hw32d/output/part-00000 |tail -10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loan                \t119630\t12.2011%\r\n",
      "modification        \t70487\t7.189%\r\n",
      "credit              \t55251\t5.6351%\r\n",
      "servicing           \t36767\t3.7499%\r\n",
      "report              \t34903\t3.5598%\r\n",
      "incorrect           \t29133\t2.9713%\r\n",
      "information         \t29069\t2.9648%\r\n",
      "on                  \t29069\t2.9648%\r\n",
      "or                  \t22533\t2.2982%\r\n",
      "account             \t20681\t2.1093%\r\n",
      "debt                \t19309\t1.9693%\r\n",
      "and                 \t16448\t1.6775%\r\n",
      "opening             \t16205\t1.6528%\r\n",
      "club                \t12545\t1.2795%\r\n",
      "health              \t12545\t1.2795%\r\n",
      "not                 \t12353\t1.2599%\r\n",
      "attempts            \t11848\t1.2084%\r\n",
      "collect             \t11848\t1.2084%\r\n",
      "cont'd              \t11848\t1.2084%\r\n",
      "owed                \t11848\t1.2084%\r\n",
      "of                  \t10885\t1.1102%\r\n",
      "my                  \t10731\t1.0945%\r\n",
      "deposits            \t10555\t1.0765%\r\n",
      "withdrawals         \t10555\t1.0765%\r\n",
      "problems            \t9484\t0.9673%\r\n",
      "application         \t8868\t0.9045%\r\n",
      "to                  \t8401\t0.8568%\r\n",
      "unable              \t8178\t0.8341%\r\n",
      "billing             \t8158\t0.832%\r\n",
      "other               \t7886\t0.8043%\r\n",
      "disputes            \t6938\t0.7076%\r\n",
      "communication       \t6920\t0.7058%\r\n",
      "tactics             \t6920\t0.7058%\r\n",
      "reporting           \t6559\t0.669%\r\n",
      "lease               \t6337\t0.6463%\r\n",
      "the                 \t6248\t0.6372%\r\n",
      "by                  \t5663\t0.5776%\r\n",
      "being               \t5663\t0.5776%\r\n",
      "caused              \t5663\t0.5776%\r\n",
      "funds               \t5663\t0.5776%\r\n",
      "low                 \t5663\t0.5776%\r\n",
      "process             \t5505\t0.5615%\r\n",
      "disclosure          \t5214\t0.5318%\r\n",
      "verification        \t5214\t0.5318%\r\n",
      "managing            \t5006\t0.5106%\r\n",
      "company's           \t4858\t0.4955%\r\n",
      "investigation       \t4858\t0.4955%\r\n",
      "identity            \t4729\t0.4823%\r\n",
      "card                \t4405\t0.4493%\r\n",
      "get                 \t4357\t0.4444%\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /user/root/wk3/hw32d/output/part-00000 |head -50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    169     507    5694\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /user/root/wk3/hw32d/output/part-00000|wc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW3.3. Shopping Cart Analysis\n",
    "Product Recommendations: The action or practice of selling additional products or services \n",
    "to existing customers is called cross-selling. Giving product recommendation is \n",
    "one of the examples of cross-selling that are frequently used by online retailers. \n",
    "One simple method to give product recommendations is to recommend products that are frequently\n",
    "browsed together by the customers.\n",
    "\n",
    "For this homework use the online browsing behavior dataset located at: \n",
    "\n",
    "       https://www.dropbox.com/s/zlfyiwa70poqg74/ProductPurchaseData.txt?dl=0\n",
    "\n",
    "Each line in this dataset represents a browsing session of a customer. \n",
    "On each line, each string of 8 characters represents the id of an item browsed during that session. \n",
    "The items are separated by spaces.\n",
    "\n",
    "Here are the first few lines of the ProductPurchaseData \n",
    "FRO11987 ELE17451 ELE89019 SNA90258 GRO99222 \n",
    "GRO99222 GRO12298 FRO12685 ELE91550 SNA11465 ELE26917 ELE52966 FRO90334 SNA30755 ELE17451 FRO84225 SNA80192 \n",
    "ELE17451 GRO73461 DAI22896 SNA99873 FRO86643 \n",
    "ELE17451 ELE37798 FRO86643 GRO56989 ELE23393 SNA11465 \n",
    "ELE17451 SNA69641 FRO86643 FRO78087 SNA11465 GRO39357 ELE28573 ELE11375 DAI54444 \n",
    "\n",
    "\n",
    "Do some exploratory data analysis of this dataset. \n",
    "\n",
    "How many unique items are available from this supplier?\n",
    "\n",
    "Using a single reducer: Report your findings such as number of unique products; largest basket; report the top 50 most frequently purchased items,  their frequency,  and their relative frequency (break ties by sorting the products alphabetical order) etc. using Hadoop Map-Reduce. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper33a.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper33a.py\n",
    "#!/usr/bin/python\n",
    "## mapper33a.py\n",
    "## Author: Prabhakar Gundugola\n",
    "## Description: mapper code for HW3.3\n",
    "import sys\n",
    "\n",
    "sys.stderr.write('reporter:counter:mapper,mapper331,1\\n')\n",
    "total_products = 0\n",
    "basket = 0\n",
    "largest_basket = 0\n",
    "for line in sys.stdin:\n",
    "    products = line.strip().split()\n",
    "    for product in products:\n",
    "        total_products += 1\n",
    "        basket += 1\n",
    "        print product + '\\t' + str(1)\n",
    "    if basket > largest_basket:\n",
    "        largest_basket = basket\n",
    "    basket = 0\n",
    "print '0000TOTALPRODUCTS' + '\\t' + str(total_products)\n",
    "print '0000LARGESTBASKET' + '\\t' + str(largest_basket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer33a.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer33a.py\n",
    "#!/usr/bin/python\n",
    "## reducer33.py\n",
    "## Author: Prabhakar Gundugola\n",
    "## Description: reducer code for HW3.3\n",
    "import sys\n",
    "\n",
    "sys.stderr.write('reporter:counter:reducer,reducer33,1\\n')\n",
    "prev_product = None\n",
    "counts = 0\n",
    "total = 0\n",
    "unique_count = 0\n",
    "largest_basket = 0\n",
    "for line in sys.stdin:\n",
    "    product, value = line.strip().split('\\t')\n",
    "    if prev_product != product:\n",
    "        if prev_product is not None:\n",
    "            if prev_product != '0000LARGESTBASKET':\n",
    "                print prev_product + '\\t' + str(counts)\n",
    "                if prev_product != '0000TOTALWORDS':\n",
    "                    unique_count += 1\n",
    "            else:\n",
    "                print prev_product + '\\t' + str(largest_basket)\n",
    "                       \n",
    "        prev_product = product\n",
    "        counts = 0\n",
    "    if product == '0000LARGESTBASKET':\n",
    "        if int(value) > largest_basket:\n",
    "            largest_basket = int(value)\n",
    "    else:\n",
    "        counts += int(value)\n",
    "unique_count += 1\n",
    "print prev_product + '\\t' + str(counts)\n",
    "print '0000UNIQUECOUNT' + '\\t' + str(unique_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x mapper33a.py\n",
    "!chmod a+x reducer33a.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `/user/root/wk3/hw33a': No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "# Ensure the input folder doesn't exist\n",
    "!hdfs dfs -rm -r /user/root/wk3/hw33a\n",
    "\n",
    "# Create Input folder\n",
    "!hdfs dfs -mkdir -p /user/root/wk3/hw33a/input\n",
    "\n",
    "# Copy the input file to input folder\n",
    "!hdfs dfs -put ProductPurchaseData.txt /user/root/wk3/hw33a/input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/31 17:16:35 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/root/wk3/hw33a/output\n",
      "packageJobJar: [/tmp/hadoop-unjar5488039784815785888/] [] /tmp/streamjob1187851644835129363.jar tmpDir=null\n",
      "16/01/31 17:16:38 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/01/31 17:16:38 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/01/31 17:16:39 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/01/31 17:16:39 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/01/31 17:16:39 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1454270249092_0051\n",
      "16/01/31 17:16:39 INFO impl.YarnClientImpl: Submitted application application_1454270249092_0051\n",
      "16/01/31 17:16:39 INFO mapreduce.Job: The url to track the job: http://prabhakar:8088/proxy/application_1454270249092_0051/\n",
      "16/01/31 17:16:39 INFO mapreduce.Job: Running job: job_1454270249092_0051\n",
      "16/01/31 17:16:45 INFO mapreduce.Job: Job job_1454270249092_0051 running in uber mode : false\n",
      "16/01/31 17:16:45 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/01/31 17:16:52 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/01/31 17:16:59 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/01/31 17:16:59 INFO mapreduce.Job: Job job_1454270249092_0051 completed successfully\n",
      "16/01/31 17:17:00 INFO mapreduce.Job: Counters: 51\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=4950818\n",
      "\t\tFILE: Number of bytes written=10254129\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=3462115\n",
      "\t\tHDFS: Number of bytes written=142726\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=9288\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=4539\n",
      "\t\tTotal time spent by all map tasks (ms)=9288\n",
      "\t\tTotal time spent by all reduce tasks (ms)=4539\n",
      "\t\tTotal vcore-seconds taken by all map tasks=9288\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=4539\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=9510912\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=4647936\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=31101\n",
      "\t\tMap output records=380828\n",
      "\t\tMap output bytes=4189156\n",
      "\t\tMap output materialized bytes=4950824\n",
      "\t\tInput split bytes=248\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=12594\n",
      "\t\tReduce shuffle bytes=4950824\n",
      "\t\tReduce input records=380828\n",
      "\t\tReduce output records=12595\n",
      "\t\tSpilled Records=761656\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=106\n",
      "\t\tCPU time spent (ms)=6730\n",
      "\t\tPhysical memory (bytes) snapshot=699924480\n",
      "\t\tVirtual memory (bytes) snapshot=2515263488\n",
      "\t\tTotal committed heap usage (bytes)=601882624\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tmapper\n",
      "\t\tmapper331=2\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=3461867\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=142726\n",
      "\treducer\n",
      "\t\treducer33=1\n",
      "16/01/31 17:17:00 INFO streaming.StreamJob: Output directory: /user/root/wk3/hw33a/output\n"
     ]
    }
   ],
   "source": [
    "# Ensure output folder doesn't exist\n",
    "!hdfs dfs -rm -r /user/root/wk3/hw33a/output\n",
    "\n",
    "# Run Hadoop Streaming job\n",
    "!hadoop jar hadoop-streaming-2.7.1.jar \\\n",
    "-mapper /root/hw3/mapper33a.py \\\n",
    "-reducer /root/hw3/reducer33a.py \\\n",
    "-input /user/root/wk3/hw33a/input \\\n",
    "-output /user/root/wk3/hw33a/output\n",
    "\n",
    "#-D mapred.text.key.comparator.options='-k1,1n -k2,2nr' \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer33b.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer33b.py\n",
    "#!/usr/bin/python\n",
    "## reducer33b.py\n",
    "## Author: Prabhakar Gundugola\n",
    "## Description: reducer code for HW3.3\n",
    "import sys\n",
    "\n",
    "sys.stderr.write('reporter:counter:reducer,Reducer32d,1\\n')\n",
    "\n",
    "total = 0\n",
    "for line in sys.stdin:\n",
    "    value, word = line.strip().split('\\t')\n",
    "    # First word should be 0000TOTALWORDS\n",
    "    if word == '0000TOTALPRODUCTS':\n",
    "        total = int(value)\n",
    "    elif word == '0000UNIQUECOUNT':\n",
    "        print word.ljust(20) + '\\t' + value\n",
    "    elif word == '0000LARGESTBASKET':\n",
    "        print word.ljust(20) + '\\t' + value\n",
    "    else:\n",
    "        term_freq = round(100.0 * int(value)/total, 3)\n",
    "        print word.ljust(20) + '\\t' + value + '\\t' + str(term_freq) + '%'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!chmod a+x reducer33b.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/31 17:36:54 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/root/wk3/hw33b/output\n",
      "packageJobJar: [/tmp/hadoop-unjar2487017460346512839/] [] /tmp/streamjob7490952225334260520.jar tmpDir=null\n",
      "16/01/31 17:36:57 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/01/31 17:36:58 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/01/31 17:36:58 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/01/31 17:36:58 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/01/31 17:36:58 INFO Configuration.deprecation: mapred.output.key.comparator.class is deprecated. Instead, use mapreduce.job.output.key.comparator.class\n",
      "16/01/31 17:36:58 INFO Configuration.deprecation: mapred.text.key.comparator.options is deprecated. Instead, use mapreduce.partition.keycomparator.options\n",
      "16/01/31 17:36:58 INFO Configuration.deprecation: mapred.text.key.partitioner.options is deprecated. Instead, use mapreduce.partition.keypartitioner.options\n",
      "16/01/31 17:36:58 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1454270249092_0055\n",
      "16/01/31 17:36:58 INFO impl.YarnClientImpl: Submitted application application_1454270249092_0055\n",
      "16/01/31 17:36:58 INFO mapreduce.Job: The url to track the job: http://prabhakar:8088/proxy/application_1454270249092_0055/\n",
      "16/01/31 17:36:58 INFO mapreduce.Job: Running job: job_1454270249092_0055\n",
      "16/01/31 17:37:05 INFO mapreduce.Job: Job job_1454270249092_0055 running in uber mode : false\n",
      "16/01/31 17:37:05 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/01/31 17:37:11 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/01/31 17:37:18 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/01/31 17:37:18 INFO mapreduce.Job: Job job_1454270249092_0055 completed successfully\n",
      "16/01/31 17:37:18 INFO mapreduce.Job: Counters: 51\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=180517\n",
      "\t\tFILE: Number of bytes written=715438\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=145315\n",
      "\t\tHDFS: Number of bytes written=373700\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=8106\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=3782\n",
      "\t\tTotal time spent by all map tasks (ms)=8106\n",
      "\t\tTotal time spent by all reduce tasks (ms)=3782\n",
      "\t\tTotal vcore-seconds taken by all map tasks=8106\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=3782\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=8300544\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=3872768\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=12595\n",
      "\t\tMap output records=12595\n",
      "\t\tMap output bytes=155321\n",
      "\t\tMap output materialized bytes=180523\n",
      "\t\tInput split bytes=224\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=12595\n",
      "\t\tReduce shuffle bytes=180523\n",
      "\t\tReduce input records=12595\n",
      "\t\tReduce output records=12594\n",
      "\t\tSpilled Records=25190\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=104\n",
      "\t\tCPU time spent (ms)=4750\n",
      "\t\tPhysical memory (bytes) snapshot=692658176\n",
      "\t\tVirtual memory (bytes) snapshot=2528735232\n",
      "\t\tTotal committed heap usage (bytes)=603979776\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tmapper\n",
      "\t\tMapper32d=2\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=145091\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=373700\n",
      "\treducer\n",
      "\t\tReducer32d=1\n",
      "16/01/31 17:37:18 INFO streaming.StreamJob: Output directory: /user/root/wk3/hw33b/output\n"
     ]
    }
   ],
   "source": [
    "# Ensure the output folder doesn't exist\n",
    "!hdfs dfs -rm -r /user/root/wk3/hw33b/output\n",
    "\n",
    "\n",
    "# Run Hadoop Streaming job. \n",
    "!hadoop jar hadoop-streaming-2.7.1.jar \\\n",
    "-D mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "-D mapred.text.key.partitioner.options=-k1,1 \\\n",
    "-D stream.num.map.output.key.fields=2 \\\n",
    "-D mapred.text.key.comparator.options='-k1,1nr -k2,2n' \\\n",
    "-mapper /root/hw3/mapper32d.py \\\n",
    "-reducer /root/hw3/reducer33b.py \\\n",
    "-input /user/root/wk3/hw33a/output/part* \\\n",
    "-output /user/root/wk3/hw33b/output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove 'output33a.txt': No such file or directory\n",
      "16/01/31 17:40:14 WARN hdfs.DFSClient: DFSInputStream has been closed already\n",
      "16/01/31 17:40:17 WARN hdfs.DFSClient: DFSInputStream has been closed already\n"
     ]
    }
   ],
   "source": [
    "!rm output33a.txt\n",
    "!rm output33b.txt\n",
    "!hdfs dfs -copyToLocal /user/root/wk3/hw33a/output/part-00000 output33a.txt\n",
    "!hdfs dfs -copyToLocal /user/root/wk3/hw33b/output/part-00000 output33b.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DAI62779            \t6667\t1.751%\r\n",
      "FRO40251            \t3881\t1.019%\r\n",
      "ELE17451            \t3875\t1.018%\r\n",
      "GRO73461            \t3602\t0.946%\r\n",
      "SNA80324            \t3044\t0.799%\r\n",
      "ELE32164            \t2851\t0.749%\r\n",
      "DAI75645            \t2736\t0.718%\r\n",
      "SNA45677            \t2455\t0.645%\r\n",
      "FRO31317            \t2330\t0.612%\r\n",
      "DAI85309            \t2293\t0.602%\r\n",
      "ELE26917            \t2292\t0.602%\r\n",
      "FRO80039            \t2233\t0.586%\r\n",
      "GRO21487            \t2115\t0.555%\r\n",
      "SNA99873            \t2083\t0.547%\r\n",
      "GRO59710            \t2004\t0.526%\r\n",
      "GRO71621            \t1920\t0.504%\r\n",
      "FRO85978            \t1918\t0.504%\r\n",
      "GRO30386            \t1840\t0.483%\r\n",
      "ELE74009            \t1816\t0.477%\r\n",
      "GRO56726            \t1784\t0.468%\r\n",
      "DAI63921            \t1773\t0.466%\r\n",
      "GRO46854            \t1756\t0.461%\r\n",
      "ELE66600            \t1713\t0.45%\r\n",
      "DAI83733            \t1712\t0.45%\r\n",
      "FRO32293            \t1702\t0.447%\r\n",
      "ELE66810            \t1697\t0.446%\r\n",
      "SNA55762            \t1646\t0.432%\r\n",
      "DAI22177            \t1627\t0.427%\r\n",
      "FRO78087            \t1531\t0.402%\r\n",
      "ELE99737            \t1516\t0.398%\r\n",
      "ELE34057            \t1489\t0.391%\r\n",
      "GRO94758            \t1489\t0.391%\r\n",
      "FRO35904            \t1436\t0.377%\r\n",
      "FRO53271            \t1420\t0.373%\r\n",
      "SNA93860            \t1407\t0.369%\r\n",
      "SNA90094            \t1390\t0.365%\r\n",
      "GRO38814            \t1352\t0.355%\r\n",
      "ELE56788            \t1345\t0.353%\r\n",
      "GRO61133            \t1321\t0.347%\r\n",
      "DAI88807            \t1316\t0.346%\r\n",
      "ELE74482            \t1316\t0.346%\r\n",
      "ELE59935            \t1311\t0.344%\r\n",
      "SNA96271            \t1295\t0.34%\r\n",
      "DAI43223            \t1290\t0.339%\r\n",
      "ELE91337            \t1289\t0.338%\r\n",
      "GRO15017            \t1275\t0.335%\r\n",
      "DAI31081            \t1261\t0.331%\r\n",
      "GRO81087            \t1220\t0.32%\r\n",
      "DAI22896            \t1219\t0.32%\r\n",
      "GRO85051            \t1214\t0.319%\r\n"
     ]
    }
   ],
   "source": [
    "!head -51 output33b.txt|tail -50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Product count:  12593\n",
      "Largest Basket:  37\n"
     ]
    }
   ],
   "source": [
    "!echo \"Unique Product count: \" `head -1 output33b.txt|cut -f 2`\n",
    "!echo \"Largest Basket: \" `head -1 output33a.txt|cut -f 2`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#HW 3.4. (Computationally prohibitive but then again Hadoop can handle this) Pairs\n",
    "Suppose we want to recommend new products to the customer based on the products they\n",
    "have already browsed on the online website. Write a map-reduce program \n",
    "to find products which are frequently browsed together. Fix the support count (cooccurence count) to s = 100 \n",
    "(i.e. product pairs need to occur together at least 100 times to be considered frequent) \n",
    "and find pairs of items (sometimes referred to itemsets of size 2 in association rule mining) that have a support count of 100 or more.\n",
    "\n",
    "List the top 50 product pairs with corresponding support count (aka frequency), and relative frequency or support (number of records where they coccur, the number of records where they coccur/the number of baskets in the dataset)  in decreasing order of support  for frequent (100>count) itemsets of size 2. \n",
    "\n",
    "Use the Pairs pattern (lecture 3)  to  extract these frequent itemsets of size 2. Free free to use combiners if they bring value. Instrument your code with counters for count the number of times your mapper, combiner and reducers are called.  \n",
    "\n",
    "Please output records of the following form for the top 50 pairs (itemsets of size 2): \n",
    "\n",
    "      item1, item2, support count, support\n",
    "\n",
    "\n",
    "\n",
    "Fix the ordering of the pairs lexicographically (left to right), \n",
    "and break ties in support (between pairs, if any exist) \n",
    "by taking the first ones in lexicographically increasing order. \n",
    "\n",
    "Report  the compute time for the Pairs job. Describe the computational setup used (E.g., single computer; dual core; linux, number of mappers, number of reducers)\n",
    "Instrument your mapper, combiner, and reducer to count how many times each is called using Counters and report these counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper34a.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper34a.py\n",
    "#!/usr/bin/python\n",
    "## mapper34a.py\n",
    "## Author: Prabhakar Gundugola\n",
    "## Description: mapper code for HW3.4\n",
    "import sys\n",
    "import itertools\n",
    "\n",
    "sys.stderr.write('reporter:counter:mapper,mapper34a,1\\n')\n",
    "\n",
    "record_count = 0\n",
    "\n",
    "for line in sys.stdin:\n",
    "    products = line.strip().split()\n",
    "    \n",
    "    product_pairs = list(itertools.combinations(set(products),2))\n",
    "    \n",
    "    for product_pair in product_pairs:\n",
    "        pair = sorted(product_pair)\n",
    "        print pair[0] + ', ' + pair[1] + '\\t' + str(1)\n",
    "    \n",
    "    record_count += 1\n",
    "\n",
    "print '0000RECORDCOUNT' +'\\t' + str(record_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer34a.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer34a.py\n",
    "#!/usr/bin/python\n",
    "## reducer34a.py\n",
    "## Author: Prabhakar Gundugola\n",
    "## Description: reducer code for HW3.4\n",
    "import sys\n",
    "\n",
    "sys.stderr.write('reporter:counter:reducer,reducer34a,1\\n')\n",
    "\n",
    "prev_pair = None\n",
    "counts = 0\n",
    "for line in sys.stdin:\n",
    "    pair, value = line.strip().split('\\t')\n",
    "    \n",
    "    if prev_pair != pair:\n",
    "        if prev_pair is not None:\n",
    "            print prev_pair + '\\t' + str(counts)\n",
    "        counts = 0\n",
    "        prev_pair = pair\n",
    "    counts += eval(value)\n",
    "print prev_pair + '\\t' + str(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x mapper34a.py\n",
    "!chmod a+x reducer34a.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/31 20:00:09 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\r\n",
      "Deleted /user/root/wk3/hw34a\r\n"
     ]
    }
   ],
   "source": [
    "# Ensure the input folder doesn't exist\n",
    "!hdfs dfs -rm -r /user/root/wk3/hw34a\n",
    "\n",
    "# Create the input folder\n",
    "!hdfs dfs -mkdir -p /user/root/wk3/hw34a/input\n",
    "\n",
    "# Copy the input data file to HDFS input folder\n",
    "!hdfs dfs -put ProductPurchaseData.txt /user/root/wk3/hw34a/input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/04 01:38:43 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/root/wk3/hw34a/output\n",
      "packageJobJar: [/tmp/hadoop-unjar5547206564294289197/] [] /tmp/streamjob5742543864462449860.jar tmpDir=null\n",
      "16/02/04 01:38:46 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/02/04 01:38:46 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/02/04 01:38:46 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/02/04 01:38:46 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/02/04 01:38:47 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1454525374165_0020\n",
      "16/02/04 01:38:47 INFO impl.YarnClientImpl: Submitted application application_1454525374165_0020\n",
      "16/02/04 01:38:47 INFO mapreduce.Job: The url to track the job: http://prabhakar:8088/proxy/application_1454525374165_0020/\n",
      "16/02/04 01:38:47 INFO mapreduce.Job: Running job: job_1454525374165_0020\n",
      "16/02/04 01:38:53 INFO mapreduce.Job: Job job_1454525374165_0020 running in uber mode : false\n",
      "16/02/04 01:38:53 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/02/04 01:39:03 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "16/02/04 01:39:04 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/02/04 01:39:14 INFO mapreduce.Job:  map 100% reduce 72%\n",
      "16/02/04 01:39:18 INFO mapreduce.Job:  map 100% reduce 77%\n",
      "16/02/04 01:39:21 INFO mapreduce.Job:  map 100% reduce 83%\n",
      "16/02/04 01:39:24 INFO mapreduce.Job:  map 100% reduce 88%\n",
      "16/02/04 01:39:27 INFO mapreduce.Job:  map 100% reduce 93%\n",
      "16/02/04 01:39:30 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/02/04 01:39:31 INFO mapreduce.Job: Job job_1454525374165_0020 completed successfully\n",
      "16/02/04 01:39:31 INFO mapreduce.Job: Counters: 51\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=58282376\n",
      "\t\tFILE: Number of bytes written=116917245\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=3462115\n",
      "\t\tHDFS: Number of bytes written=18458752\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=17779\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=24959\n",
      "\t\tTotal time spent by all map tasks (ms)=17779\n",
      "\t\tTotal time spent by all reduce tasks (ms)=24959\n",
      "\t\tTotal vcore-seconds taken by all map tasks=17779\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=24959\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=18205696\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=25558016\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=31101\n",
      "\t\tMap output records=2534016\n",
      "\t\tMap output bytes=53214338\n",
      "\t\tMap output materialized bytes=58282382\n",
      "\t\tInput split bytes=248\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=877096\n",
      "\t\tReduce shuffle bytes=58282382\n",
      "\t\tReduce input records=2534016\n",
      "\t\tReduce output records=877096\n",
      "\t\tSpilled Records=5068032\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=181\n",
      "\t\tCPU time spent (ms)=38050\n",
      "\t\tPhysical memory (bytes) snapshot=728227840\n",
      "\t\tVirtual memory (bytes) snapshot=2531340288\n",
      "\t\tTotal committed heap usage (bytes)=591921152\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tmapper\n",
      "\t\tmapper34a=2\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=3461867\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=18458752\n",
      "\treducer\n",
      "\t\treducer34a=1\n",
      "16/02/04 01:39:31 INFO streaming.StreamJob: Output directory: /user/root/wk3/hw34a/output\n"
     ]
    }
   ],
   "source": [
    "# Ensure output folder doesn't exist\n",
    "!hdfs dfs -rm -r /user/root/wk3/hw34a/output\n",
    "\n",
    "# Run Hadoop Streaming job\n",
    "!hadoop jar hadoop-streaming-2.7.1.jar \\\n",
    "-mapper /root/hw3/mapper34a.py \\\n",
    "-reducer /root/hw3/reducer34a.py \\\n",
    "-input /user/root/wk3/hw34a/input \\\n",
    "-output /user/root/wk3/hw34a/output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing reducer34b.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer34b.py\n",
    "#!/usr/bin/python\n",
    "## reducer34b.py\n",
    "## Author: Prabhakar Gundugola\n",
    "## Description: reducer code for HW3.4\n",
    "import sys\n",
    "\n",
    "sys.stderr.write('reporter:counter:reducer,Reducer34b,1\\n')\n",
    "\n",
    "total = 0\n",
    "for line in sys.stdin:\n",
    "    value, pair = line.strip().split('\\t')\n",
    "    # First word should be 0000TOTALWORDS\n",
    "    if pair == '0000RECORDCOUNT':\n",
    "        total = int(value)\n",
    "    else:\n",
    "        term_freq = round(100.0 * int(value)/total, 3)\n",
    "        print pair.ljust(20) + '\\t' + value + '\\t' + str(term_freq) + '%'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x reducer34b.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `/user/root/wk3/hw34b/output': No such file or directory\n",
      "packageJobJar: [/tmp/hadoop-unjar3536631726763484975/] [] /tmp/streamjob8537272369429628869.jar tmpDir=null\n",
      "16/01/31 20:15:27 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/01/31 20:15:27 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/01/31 20:15:27 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/01/31 20:15:27 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/01/31 20:15:27 INFO Configuration.deprecation: mapred.output.key.comparator.class is deprecated. Instead, use mapreduce.job.output.key.comparator.class\n",
      "16/01/31 20:15:27 INFO Configuration.deprecation: mapred.text.key.comparator.options is deprecated. Instead, use mapreduce.partition.keycomparator.options\n",
      "16/01/31 20:15:27 INFO Configuration.deprecation: mapred.text.key.partitioner.options is deprecated. Instead, use mapreduce.partition.keypartitioner.options\n",
      "16/01/31 20:15:28 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1454270249092_0061\n",
      "16/01/31 20:15:28 INFO impl.YarnClientImpl: Submitted application application_1454270249092_0061\n",
      "16/01/31 20:15:28 INFO mapreduce.Job: The url to track the job: http://prabhakar:8088/proxy/application_1454270249092_0061/\n",
      "16/01/31 20:15:28 INFO mapreduce.Job: Running job: job_1454270249092_0061\n",
      "16/01/31 20:15:35 INFO mapreduce.Job: Job job_1454270249092_0061 running in uber mode : false\n",
      "16/01/31 20:15:35 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/01/31 20:15:42 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/01/31 20:15:53 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/01/31 20:15:53 INFO mapreduce.Job: Job job_1454270249092_0061 completed successfully\n",
      "16/01/31 20:15:53 INFO mapreduce.Job: Counters: 51\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=20212951\n",
      "\t\tFILE: Number of bytes written=40780306\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=17585165\n",
      "\t\tHDFS: Number of bytes written=26295178\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=11243\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=8731\n",
      "\t\tTotal time spent by all map tasks (ms)=11243\n",
      "\t\tTotal time spent by all reduce tasks (ms)=8731\n",
      "\t\tTotal vcore-seconds taken by all map tasks=11243\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=8731\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=11512832\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=8940544\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=877096\n",
      "\t\tMap output records=877096\n",
      "\t\tMap output bytes=18458753\n",
      "\t\tMap output materialized bytes=20212957\n",
      "\t\tInput split bytes=224\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=877096\n",
      "\t\tReduce shuffle bytes=20212957\n",
      "\t\tReduce input records=877096\n",
      "\t\tReduce output records=877095\n",
      "\t\tSpilled Records=1754192\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=260\n",
      "\t\tCPU time spent (ms)=12900\n",
      "\t\tPhysical memory (bytes) snapshot=723660800\n",
      "\t\tVirtual memory (bytes) snapshot=2530881536\n",
      "\t\tTotal committed heap usage (bytes)=568852480\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tmapper\n",
      "\t\tMapper32d=2\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=17584941\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=26295178\n",
      "\treducer\n",
      "\t\tReducer34b=1\n",
      "16/01/31 20:15:53 INFO streaming.StreamJob: Output directory: /user/root/wk3/hw34b/output\n"
     ]
    }
   ],
   "source": [
    "# Ensure the output folder doesn't exist\n",
    "!hdfs dfs -rm -r /user/root/wk3/hw34b/output\n",
    "\n",
    "# Run Hadoop Streaming job. \n",
    "!hadoop jar hadoop-streaming-2.7.1.jar \\\n",
    "-D mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "-D mapred.text.key.partitioner.options=-k1,1 \\\n",
    "-D stream.num.map.output.key.fields=2 \\\n",
    "-D mapred.text.key.comparator.options='-k1,1nr -k2,2n' \\\n",
    "-mapper /root/hw3/mapper32d.py \\\n",
    "-reducer /root/hw3/reducer34b.py \\\n",
    "-input /user/root/wk3/hw34a/output/part* \\\n",
    "-output /user/root/wk3/hw34b/output"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove 'output34b.txt': No such file or directory\n",
      "16/01/31 20:17:04 WARN hdfs.DFSClient: DFSInputStream has been closed already\n"
     ]
    }
   ],
   "source": [
    "!rm output34b.txt\n",
    "!hdfs dfs -copyToLocal /user/root/wk3/hw34b/output/part-00000 output34b.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DAI62779,ELE17451   \t1592\t5.119%\r\n",
      "FRO40251,SNA80324   \t1412\t4.54%\r\n",
      "DAI75645,FRO40251   \t1254\t4.032%\r\n",
      "FRO40251,GRO85051   \t1213\t3.9%\r\n",
      "DAI62779,GRO73461   \t1139\t3.662%\r\n",
      "DAI75645,SNA80324   \t1130\t3.633%\r\n",
      "DAI62779,FRO40251   \t1070\t3.44%\r\n",
      "DAI62779,SNA80324   \t923\t2.968%\r\n",
      "DAI62779,DAI85309   \t918\t2.952%\r\n",
      "ELE32164,GRO59710   \t911\t2.929%\r\n",
      "DAI62779,DAI75645   \t882\t2.836%\r\n",
      "FRO40251,GRO73461   \t882\t2.836%\r\n",
      "DAI62779,ELE92920   \t877\t2.82%\r\n",
      "FRO40251,FRO92469   \t835\t2.685%\r\n",
      "DAI62779,ELE32164   \t832\t2.675%\r\n",
      "DAI75645,GRO73461   \t712\t2.289%\r\n",
      "DAI43223,ELE32164   \t711\t2.286%\r\n",
      "DAI62779,GRO30386   \t709\t2.28%\r\n",
      "ELE17451,FRO40251   \t697\t2.241%\r\n",
      "DAI85309,ELE99737   \t659\t2.119%\r\n",
      "DAI62779,ELE26917   \t650\t2.09%\r\n",
      "GRO21487,GRO73461   \t631\t2.029%\r\n",
      "DAI62779,SNA45677   \t604\t1.942%\r\n",
      "ELE17451,SNA80324   \t597\t1.92%\r\n",
      "DAI62779,GRO71621   \t595\t1.913%\r\n",
      "DAI62779,SNA55762   \t593\t1.907%\r\n",
      "DAI62779,DAI83733   \t586\t1.884%\r\n",
      "ELE17451,GRO73461   \t580\t1.865%\r\n",
      "GRO73461,SNA80324   \t562\t1.807%\r\n",
      "DAI62779,GRO59710   \t561\t1.804%\r\n",
      "DAI62779,FRO80039   \t550\t1.768%\r\n",
      "DAI75645,ELE17451   \t547\t1.759%\r\n",
      "DAI62779,SNA93860   \t537\t1.727%\r\n",
      "DAI55148,DAI62779   \t526\t1.691%\r\n",
      "DAI43223,GRO59710   \t512\t1.646%\r\n",
      "ELE17451,ELE32164   \t511\t1.643%\r\n",
      "DAI62779,SNA18336   \t506\t1.627%\r\n",
      "ELE32164,GRO73461   \t486\t1.563%\r\n",
      "DAI85309,ELE17451   \t482\t1.55%\r\n",
      "DAI62779,FRO78087   \t482\t1.55%\r\n",
      "DAI62779,GRO94758   \t479\t1.54%\r\n",
      "DAI62779,GRO21487   \t471\t1.514%\r\n",
      "GRO85051,SNA80324   \t471\t1.514%\r\n",
      "ELE17451,GRO30386   \t468\t1.505%\r\n",
      "FRO85978,SNA95666   \t463\t1.489%\r\n",
      "DAI62779,FRO19221   \t462\t1.485%\r\n",
      "DAI62779,GRO46854   \t461\t1.482%\r\n",
      "DAI43223,DAI62779   \t459\t1.476%\r\n",
      "ELE92920,SNA18336   \t455\t1.463%\r\n",
      "DAI88079,FRO40251   \t446\t1.434%\r\n"
     ]
    }
   ],
   "source": [
    "!head -50 output34b.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Report the compute time - Pairs job\n",
    "**1st Map Reduce program:** <br>\n",
    "All map tasks: 17779 ms <br>\n",
    "All reduce tasks: 24959 ms\n",
    "\n",
    "**2nd Map Reduce program:** <br>\n",
    "All map tasks: 11243 ms <br>\n",
    "All reduce tasks: 8731 ms\n",
    "\n",
    "#### Computational Setup\n",
    "SoftLayer VM, 4 Core, 32 GB RAM, 2 mappers, 1 reducer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#HW3.5. Stripes\n",
    "Repeat 3.4 using the stripes design pattern for finding cooccuring pairs.\n",
    "\n",
    "Report  the compute times for stripes job versus the Pairs job. Describe the computational setup used (E.g., single computer; dual core; linux, number of mappers, number of reducers)\n",
    "\n",
    "Instrument your mapper, combiner, and reducer to count how many times each is called using Counters and report these counts. Discuss the differences in these counts between the Pairs and Stripes jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper35a.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper35a.py\n",
    "#!/usr/bin/python\n",
    "#HW 3.5\n",
    "\n",
    "import sys\n",
    "sys.stderr.write(\"reporter:counter:Calls,mapper_calls,1\\n\")\n",
    "linecount = 0\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    products = line.split(\" \")\n",
    "    products = sorted(products)\n",
    "    linecount += 1   \n",
    "    # emit the product\n",
    "    for item in products:\n",
    "        for item2 in products[products.index(item)+1:]:\n",
    "            print \"%s,%s\\t1\" % (item, item2)\n",
    "    \n",
    "print \"linecount\\t\"+str(linecount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer35a.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer35a.py\n",
    "#!/usr/bin/python\n",
    "#HW 3.5\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.stderr.write(\"reporter:counter:Calls,reducer_calls,1\\n\")\n",
    "stripes = {}\n",
    "current_key = None\n",
    "current_count = 0\n",
    "key = None\n",
    "linecount = 0\n",
    "\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    key, count = line.split(\"\\t\", 1)\n",
    "    count = int(count)\n",
    "    \n",
    "    if current_key == key:\n",
    "        current_count += int(count)\n",
    "    else:\n",
    "        if current_key:\n",
    "            items = current_key.split(\",\", 1)\n",
    "            if len(items) == 2:\n",
    "                stripes.setdefault(items[0], {})\n",
    "                stripes[items[0]][items[1]]=current_count\n",
    "            elif items[0] == \"linecount\":\n",
    "                linecount = current_count\n",
    "        current_count = count\n",
    "        current_key = key\n",
    "\n",
    "# output the last word\n",
    "if current_key == key:\n",
    "    items = current_key.split(\",\", 1)\n",
    "    if len(items) == 2:\n",
    "        stripes.setdefault(items[0], {})\n",
    "        stripes[items[0]][items[1]]=current_count\n",
    "    elif items[0] == \"linecount\":\n",
    "        linecount = current_count\n",
    "        \n",
    "\n",
    "for key, stripe in stripes.items():\n",
    "    marg_count = sum(stripe.values())\n",
    "    for key2, count in stripe.items():\n",
    "        if count >= 100:\n",
    "            line_freq = round(100.0*count/marg_count, 4)\n",
    "            #print \"%s\\t%s\\t%s\\t%.4f\\t%.4f\" % \\\n",
    "            #(key, key2, str(count), count*1.0/linecount, count*1.0/marg_count)\n",
    "            print key + ', ' + key2 + '\\t' + str(count) +'\\t' + str(line_freq) + '%'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x mapper35a.py\n",
    "!chmod a+x reducer35a.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/04 00:42:02 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\r\n",
      "Deleted /user/root/wk3/hw35a\r\n"
     ]
    }
   ],
   "source": [
    "# Ensure the input folder doesn't exist\n",
    "!hdfs dfs -rm -r /user/root/wk3/hw35a\n",
    "\n",
    "# Create the input folder\n",
    "!hdfs dfs -mkdir -p /user/root/wk3/hw35a/input\n",
    "\n",
    "# Copy the input data file to HDFS input folder\n",
    "!hdfs dfs -put ProductPurchaseData.txt /user/root/wk3/hw35a/input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/04 00:45:27 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/root/wk3/hw35a/output\n",
      "packageJobJar: [/tmp/hadoop-unjar4941323668525768229/] [] /tmp/streamjob947624389084947282.jar tmpDir=null\n",
      "16/02/04 00:45:30 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/02/04 00:45:30 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/02/04 00:45:31 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/02/04 00:45:31 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/02/04 00:45:31 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1454525374165_0015\n",
      "16/02/04 00:45:31 INFO impl.YarnClientImpl: Submitted application application_1454525374165_0015\n",
      "16/02/04 00:45:31 INFO mapreduce.Job: The url to track the job: http://prabhakar:8088/proxy/application_1454525374165_0015/\n",
      "16/02/04 00:45:31 INFO mapreduce.Job: Running job: job_1454525374165_0015\n",
      "16/02/04 00:45:37 INFO mapreduce.Job: Job job_1454525374165_0015 running in uber mode : false\n",
      "16/02/04 00:45:37 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/02/04 00:45:48 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/02/04 00:46:00 INFO mapreduce.Job:  map 100% reduce 88%\n",
      "16/02/04 00:46:02 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/02/04 00:46:03 INFO mapreduce.Job: Job job_1454525374165_0015 completed successfully\n",
      "16/02/04 00:46:03 INFO mapreduce.Job: Counters: 51\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=58283424\n",
      "\t\tFILE: Number of bytes written=116919839\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=3462115\n",
      "\t\tHDFS: Number of bytes written=41230\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=16982\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=11127\n",
      "\t\tTotal time spent by all map tasks (ms)=16982\n",
      "\t\tTotal time spent by all reduce tasks (ms)=11127\n",
      "\t\tTotal vcore-seconds taken by all map tasks=16982\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=11127\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=17389568\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=11394048\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=31101\n",
      "\t\tMap output records=2534062\n",
      "\t\tMap output bytes=53215294\n",
      "\t\tMap output materialized bytes=58283430\n",
      "\t\tInput split bytes=248\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=877100\n",
      "\t\tReduce shuffle bytes=58283430\n",
      "\t\tReduce input records=2534062\n",
      "\t\tReduce output records=1334\n",
      "\t\tSpilled Records=5068124\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=261\n",
      "\t\tCPU time spent (ms)=20000\n",
      "\t\tPhysical memory (bytes) snapshot=713805824\n",
      "\t\tVirtual memory (bytes) snapshot=2528686080\n",
      "\t\tTotal committed heap usage (bytes)=590348288\n",
      "\tCalls\n",
      "\t\tmapper_calls=2\n",
      "\t\treducer_calls=1\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=3461867\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=41230\n",
      "16/02/04 00:46:03 INFO streaming.StreamJob: Output directory: /user/root/wk3/hw35a/output\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r /user/root/wk3/hw35a/output\n",
    "\n",
    "!hadoop jar /usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar \\\n",
    "-D stream.map.output.field.separator=\"\\t\" \\\n",
    "-mapper /root/hw3/mapper35a.py \\\n",
    "-reducer /root/hw3/reducer35a.py \\\n",
    "-input /user/root/wk3/hw35a/input/ProductPurchaseData.txt \\\n",
    "-output /user/root/wk3/hw35a/output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ELE20847, ELE26917\t110\t1.1777%\r\n",
      "ELE20847, GRO73461\t187\t2.0021%\r\n",
      "ELE20847, FRO92469\t122\t1.3062%\r\n",
      "ELE20847, GRO85051\t139\t1.4882%\r\n",
      "ELE20847, SNA80324\t410\t4.3897%\r\n",
      "ELE20847, FRO75586\t118\t1.2634%\r\n",
      "ELE20847, SNA96271\t184\t1.97%\r\n",
      "ELE20847, FRO40251\t434\t4.6467%\r\n",
      "DAI22896, GRO21487\t114\t0.6891%\r\n",
      "DAI22896, GRO38814\t223\t1.3479%\r\n",
      "DAI22896, ELE74009\t165\t0.9973%\r\n",
      "DAI22896, DAI62779\t297\t1.7952%\r\n",
      "DAI22896, GRO73461\t304\t1.8375%\r\n",
      "DAI22896, DAI75645\t215\t1.2996%\r\n",
      "DAI22896, GRO30386\t102\t0.6165%\r\n",
      "DAI22896, SNA80324\t195\t1.1787%\r\n",
      "DAI22896, ELE32164\t107\t0.6468%\r\n",
      "DAI22896, GRO46854\t114\t0.6891%\r\n",
      "DAI22896, FRO53271\t123\t0.7435%\r\n",
      "DAI22896, SNA72163\t227\t1.3721%\r\n",
      "cat: Unable to write to output stream.\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /user/root/wk3/hw35a/output/part-00000|head -20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper35b.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper35b.py\n",
    "#!/usr/bin/python\n",
    "#HW 3.5\n",
    "\n",
    "import sys\n",
    "\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    print line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!chmod a+x mapper35b.py\n",
    "!chmod a+x reducer35b.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `/user/root/wk3/hw35b': No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "# Ensure the input folder doesn't exist\n",
    "!hdfs dfs -rm -r /user/root/wk3/hw35b\n",
    "# Create the input folder\n",
    "!hdfs dfs -mkdir -p /user/root/wk3/hw35b/input\n",
    "\n",
    "# Copy the input data file to HDFS input folder\n",
    "!hdfs dfs -put ProductPurchaseData.txt /user/root/wk3/hw35b/input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/04 01:15:27 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/root/wk3/hw35b/output\n",
      "packageJobJar: [/tmp/hadoop-unjar6372001845285142446/] [] /tmp/streamjob7974217546918567144.jar tmpDir=null\n",
      "16/02/04 01:15:30 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/02/04 01:15:30 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/02/04 01:15:31 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/02/04 01:15:31 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/02/04 01:15:31 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1454525374165_0019\n",
      "16/02/04 01:15:31 INFO impl.YarnClientImpl: Submitted application application_1454525374165_0019\n",
      "16/02/04 01:15:31 INFO mapreduce.Job: The url to track the job: http://prabhakar:8088/proxy/application_1454525374165_0019/\n",
      "16/02/04 01:15:31 INFO mapreduce.Job: Running job: job_1454525374165_0019\n",
      "16/02/04 01:15:37 INFO mapreduce.Job: Job job_1454525374165_0019 running in uber mode : false\n",
      "16/02/04 01:15:37 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/02/04 01:15:43 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/02/04 01:15:49 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/02/04 01:15:50 INFO mapreduce.Job: Job job_1454525374165_0019 completed successfully\n",
      "16/02/04 01:15:51 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=45238\n",
      "\t\tFILE: Number of bytes written=444550\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=45415\n",
      "\t\tHDFS: Number of bytes written=41230\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=7822\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=3483\n",
      "\t\tTotal time spent by all map tasks (ms)=7822\n",
      "\t\tTotal time spent by all reduce tasks (ms)=3483\n",
      "\t\tTotal vcore-seconds taken by all map tasks=7822\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=3483\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=8009728\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=3566592\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=1334\n",
      "\t\tMap output records=1334\n",
      "\t\tMap output bytes=42564\n",
      "\t\tMap output materialized bytes=45244\n",
      "\t\tInput split bytes=224\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=1334\n",
      "\t\tReduce shuffle bytes=45244\n",
      "\t\tReduce input records=1334\n",
      "\t\tReduce output records=1334\n",
      "\t\tSpilled Records=2668\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=98\n",
      "\t\tCPU time spent (ms)=2830\n",
      "\t\tPhysical memory (bytes) snapshot=706846720\n",
      "\t\tVirtual memory (bytes) snapshot=2527629312\n",
      "\t\tTotal committed heap usage (bytes)=603979776\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=45191\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=41230\n",
      "16/02/04 01:15:51 INFO streaming.StreamJob: Output directory: /user/root/wk3/hw35b/output\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r /user/root/wk3/hw35b/output\n",
    "\n",
    "!hadoop jar /usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar \\\n",
    "-D stream.map.output.field.separator=\"\\t\" \\\n",
    "-D mapreduce.job.output.key.comparator.class=\\\n",
    "org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "-D mapreduce.partition.keycomparator.options=\"-k2,2nr -k1,1 -k2,2\" \\\n",
    "-mapper /root/hw3/mapper35b.py \\\n",
    "-reducer /root/hw3/mapper35b.py \\\n",
    "-input /user/root/wk3/hw35a/output/part-00000 \\\n",
    "-output /user/root/wk3/hw35b/output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DAI62779, ELE17451\t1592\t2.045%\r\n",
      "FRO40251, SNA80324\t1412\t4.888%\r\n",
      "DAI75645, FRO40251\t1254\t3.6456%\r\n",
      "FRO40251, GRO85051\t1213\t4.1991%\r\n",
      "DAI62779, GRO73461\t1139\t1.4631%\r\n",
      "DAI75645, SNA80324\t1130\t3.2851%\r\n",
      "DAI62779, FRO40251\t1070\t1.3744%\r\n",
      "DAI62779, SNA80324\t923\t1.1856%\r\n",
      "DAI62779, DAI85309\t918\t1.1792%\r\n",
      "ELE32164, GRO59710\t911\t3.4435%\r\n",
      "DAI62779, DAI75645\t882\t1.1329%\r\n",
      "FRO40251, GRO73461\t882\t3.0533%\r\n",
      "DAI62779, ELE92920\t877\t1.1265%\r\n",
      "FRO40251, FRO92469\t835\t2.8906%\r\n",
      "DAI62779, ELE32164\t832\t1.0687%\r\n",
      "DAI75645, GRO73461\t712\t2.0699%\r\n",
      "DAI43223, ELE32164\t711\t4.2296%\r\n",
      "DAI62779, GRO30386\t709\t0.9107%\r\n",
      "ELE17451, FRO40251\t697\t1.8024%\r\n",
      "DAI85309, ELE99737\t659\t2.4564%\r\n",
      "DAI62779, ELE26917\t650\t0.8349%\r\n",
      "GRO21487, GRO73461\t631\t5.6908%\r\n",
      "DAI62779, SNA45677\t604\t0.7759%\r\n",
      "ELE17451, SNA80324\t597\t1.5438%\r\n",
      "DAI62779, GRO71621\t595\t0.7643%\r\n",
      "DAI62779, SNA55762\t593\t0.7617%\r\n",
      "DAI62779, DAI83733\t586\t0.7527%\r\n",
      "ELE17451, GRO73461\t580\t1.4998%\r\n",
      "GRO73461, SNA80324\t562\t4.7014%\r\n",
      "DAI62779, GRO59710\t561\t0.7206%\r\n",
      "DAI62779, FRO80039\t550\t0.7065%\r\n",
      "DAI75645, ELE17451\t547\t1.5902%\r\n",
      "DAI62779, SNA93860\t537\t0.6898%\r\n",
      "DAI55148, DAI62779\t526\t4.5166%\r\n",
      "DAI43223, GRO59710\t512\t3.0458%\r\n",
      "ELE17451, ELE32164\t511\t1.3214%\r\n",
      "DAI62779, SNA18336\t506\t0.65%\r\n",
      "ELE32164, GRO73461\t486\t1.837%\r\n",
      "DAI62779, FRO78087\t482\t0.6191%\r\n",
      "DAI85309, ELE17451\t482\t1.7966%\r\n",
      "DAI62779, GRO94758\t479\t0.6153%\r\n",
      "DAI62779, GRO21487\t471\t0.605%\r\n",
      "GRO85051, SNA80324\t471\t12.9645%\r\n",
      "ELE17451, GRO30386\t468\t1.2102%\r\n",
      "FRO85978, SNA95666\t463\t4.1299%\r\n",
      "DAI62779, FRO19221\t462\t0.5934%\r\n",
      "DAI62779, GRO46854\t461\t0.5922%\r\n",
      "DAI43223, DAI62779\t459\t2.7305%\r\n",
      "ELE92920, SNA18336\t455\t4.8194%\r\n",
      "DAI88079, FRO40251\t446\t8.3898%\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /user/root/wk3/hw35b/output/part-00000 |head -50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Report the compute time - Stripes job\n",
    "**1st Map Reduce program:** <br>\n",
    "All map tasks: 16982 ms <br>\n",
    "All reduce tasks: 11127 ms\n",
    "\n",
    "**2nd Map Reduce program:** <br>\n",
    "All map tasks: 7822 ms <br>\n",
    "All reduce tasks: 3483 ms\n",
    "\n",
    "#### Computational Setup\n",
    "SoftLayer VM, 4 Core, 32 GB RAM, 2 mappers, 1 reducer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
